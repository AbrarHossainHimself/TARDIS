{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5108/84532322.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(file_path, parse_dates=time_columns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '/home/abrar/Desktop/Code/Temporal HPC/normalized_data.csv'\n",
    "\n",
    "# Read CSV and ensure datetime columns are parsed correctly\n",
    "time_columns = ['submit_time', 'eligible_time', 'start_time', 'end_time', 'wait_time']\n",
    "df = pd.read_csv(file_path, parse_dates=time_columns)\n",
    "# Now 'df' contains the data from the second sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['run_time'].plot.hist(bins=50)\n",
    "plt.title('Distribution of Run Time')\n",
    "plt.xlabel('Run Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour of day distribution:\n",
      "hour_of_day\n",
      "0      4680\n",
      "1      3880\n",
      "2      3645\n",
      "3      3653\n",
      "4      3834\n",
      "5      4312\n",
      "6      6420\n",
      "7     11364\n",
      "8     12706\n",
      "9     11611\n",
      "10    10179\n",
      "11    11690\n",
      "12     8799\n",
      "13    13013\n",
      "14    14091\n",
      "15     9886\n",
      "16     8403\n",
      "17     8840\n",
      "18     4967\n",
      "19     7375\n",
      "20     6316\n",
      "21     6719\n",
      "22     7563\n",
      "23     7369\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Day of week distribution:\n",
      "day_of_week\n",
      "Tuesday      34127\n",
      "Wednesday    33466\n",
      "Friday       32642\n",
      "Thursday     31801\n",
      "Monday       24452\n",
      "Saturday     21010\n",
      "Sunday       13817\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Monthly distribution:\n",
      "month\n",
      "5     19376\n",
      "6     33923\n",
      "7     33178\n",
      "8     29692\n",
      "9     33887\n",
      "10    41259\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Temporal distribution analysis\n",
    "# Convert submit_time to hour of day and day of week\n",
    "df['hour_of_day'] = df['submit_time'].dt.hour\n",
    "df['day_of_week'] = df['submit_time'].dt.day_name()\n",
    "df['month'] = df['submit_time'].dt.month\n",
    "\n",
    "# Get distributions\n",
    "print(\"Hour of day distribution:\")\n",
    "print(df['hour_of_day'].value_counts().sort_index())\n",
    "print(\"\\nDay of week distribution:\")\n",
    "print(df['day_of_week'].value_counts())\n",
    "print(\"\\nMonthly distribution:\")\n",
    "print(df['month'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job size and duration statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job size statistics:\n",
      "       num_cores_alloc  num_nodes_alloc  num_gpus_alloc      mem_alloc  \\\n",
      "count    191315.000000    191315.000000   191315.000000  191315.000000   \n",
      "mean        107.315872         1.173761        4.127314     213.540993   \n",
      "std         106.953631         0.732607        3.212431     190.284860   \n",
      "min           4.000000         1.000000        0.000000       0.000000   \n",
      "25%          32.000000         1.000000        4.000000     118.000000   \n",
      "50%         128.000000         1.000000        4.000000     237.000000   \n",
      "75%         128.000000         1.000000        4.000000     237.000000   \n",
      "max       20736.000000       162.000000      648.000000   38475.000000   \n",
      "\n",
      "            run_time  \n",
      "count  191315.000000  \n",
      "mean     5767.922732  \n",
      "std      3778.020112  \n",
      "min         1.000000  \n",
      "25%      2858.890321  \n",
      "50%      5581.545522  \n",
      "75%      8422.589206  \n",
      "max     23330.168557  \n",
      "\n",
      "Power usage statistics:\n",
      "       mean_cpu_power  mean_mem_power  mean_node_power\n",
      "count   191315.000000   191315.000000    191315.000000\n",
      "mean       142.347877       44.559368       840.158333\n",
      "std        118.132777       25.982101       183.097552\n",
      "min         22.666667       28.000000        44.778073\n",
      "25%         78.000000       36.000000       708.600413\n",
      "50%        102.780488       37.157895       840.644989\n",
      "75%        174.011173       40.428571       971.320238\n",
      "max      21683.333333     5848.000000      1635.693883\n"
     ]
    }
   ],
   "source": [
    "# Job size and duration statistics\n",
    "print(\"\\nJob size statistics:\")\n",
    "print(df[['num_cores_alloc', 'num_nodes_alloc', 'num_gpus_alloc', 'mem_alloc', 'run_time']].describe())\n",
    "\n",
    "# Power usage statistics\n",
    "print(\"\\nPower usage statistics:\")\n",
    "print(df[['mean_cpu_power', 'mean_mem_power', 'mean_node_power']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlations between numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation matrix:\n",
      "                 cores_per_task  num_tasks  num_cores_alloc  num_nodes_alloc  \\\n",
      "cores_per_task         1.000000  -0.259773         0.268369         0.048834   \n",
      "num_tasks             -0.259773   1.000000         0.147112         0.238280   \n",
      "num_cores_alloc        0.268369   0.147112         1.000000         0.838456   \n",
      "num_nodes_alloc        0.048834   0.238280         0.838456         1.000000   \n",
      "num_gpus_alloc         0.072371   0.128644         0.818108         0.883935   \n",
      "mem_alloc              0.189469   0.145238         0.903711         0.862834   \n",
      "mean_cpu_power         0.055080   0.346748         0.639818         0.672793   \n",
      "mean_mem_power         0.049872   0.281220         0.781548         0.923168   \n",
      "mean_node_power        0.657201   0.011314         0.669077         0.402031   \n",
      "run_time               0.610215   0.028178         0.670590         0.436848   \n",
      "\n",
      "                 num_gpus_alloc  mem_alloc  mean_cpu_power  mean_mem_power  \\\n",
      "cores_per_task         0.072371   0.189469        0.055080        0.049872   \n",
      "num_tasks              0.128644   0.145238        0.346748        0.281220   \n",
      "num_cores_alloc        0.818108   0.903711        0.639818        0.781548   \n",
      "num_nodes_alloc        0.883935   0.862834        0.672793        0.923168   \n",
      "num_gpus_alloc         1.000000   0.819673        0.580790        0.809477   \n",
      "mem_alloc              0.819673   1.000000        0.600435        0.795476   \n",
      "mean_cpu_power         0.580790   0.600435        1.000000        0.755235   \n",
      "mean_mem_power         0.809477   0.795476        0.755235        1.000000   \n",
      "mean_node_power        0.503112   0.579767        0.394570        0.431393   \n",
      "run_time               0.534207   0.619414        0.410196        0.465482   \n",
      "\n",
      "                 mean_node_power  run_time  \n",
      "cores_per_task          0.657201  0.610215  \n",
      "num_tasks               0.011314  0.028178  \n",
      "num_cores_alloc         0.669077  0.670590  \n",
      "num_nodes_alloc         0.402031  0.436848  \n",
      "num_gpus_alloc          0.503112  0.534207  \n",
      "mem_alloc               0.579767  0.619414  \n",
      "mean_cpu_power          0.394570  0.410196  \n",
      "mean_mem_power          0.431393  0.465482  \n",
      "mean_node_power         1.000000  0.961768  \n",
      "run_time                0.961768  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations between numerical columns\n",
    "corr_cols = ['cores_per_task', 'num_tasks', 'num_cores_alloc', 'num_nodes_alloc', \n",
    "             'num_gpus_alloc', 'mem_alloc', 'mean_cpu_power', 'mean_mem_power', \n",
    "             'mean_node_power', 'run_time']\n",
    "correlation_matrix = df[corr_cols].corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "appropriate groups based on the actual distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strata: 90\n",
      "\n",
      "Sample sizes per stratum:\n",
      "Min: 1\n",
      "Max: 14242\n",
      "Mean: 2125.722222222222\n",
      "Median: 325.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For hour groups\n",
    "df['hour_group'] = pd.cut(df['hour_of_day'], \n",
    "                         bins=[0, 6, 12, 18, 24], \n",
    "                         labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "# For core allocation, use custom bins based on the distribution we saw\n",
    "df['size_group'] = pd.cut(df['num_cores_alloc'], \n",
    "                         bins=[0, 32, 64, 128, 256, float('inf')],\n",
    "                         labels=['very_small', 'small', 'medium', 'large', 'very_large'])\n",
    "\n",
    "# For runtime, use custom bins based on the distribution\n",
    "df['runtime_group'] = pd.cut(df['run_time'],\n",
    "                            bins=[0, 2000, 4000, 6000, 8000, float('inf')],\n",
    "                            labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n",
    "\n",
    "# Create combined stratification feature\n",
    "df['strata'] = df['hour_group'].astype(str) + '_' + \\\n",
    "               df['size_group'].astype(str) + '_' + \\\n",
    "               df['runtime_group'].astype(str)\n",
    "\n",
    "# Check strata distribution\n",
    "strata_counts = df['strata'].value_counts()\n",
    "print(\"Number of strata:\", len(strata_counts))\n",
    "print(\"\\nSample sizes per stratum:\")\n",
    "print(\"Min:\", strata_counts.min())\n",
    "print(\"Max:\", strata_counts.max())\n",
    "print(\"Mean:\", strata_counts.mean())\n",
    "print(\"Median:\", strata_counts.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 191315\n",
      "Filtered samples: 191207\n",
      "Percentage retained: 99.94354859786216 %\n",
      "\n",
      "New strata statistics:\n",
      "Number of strata: 78\n",
      "Min samples per stratum: 20\n",
      "Max samples per stratum: 14242\n"
     ]
    }
   ],
   "source": [
    "# First, let's remove tiny strata that could cause issues\n",
    "# Keep only strata with at least 20 samples to ensure meaningful splits\n",
    "valid_strata = strata_counts[strata_counts >= 20].index\n",
    "df_filtered = df[df['strata'].isin(valid_strata)].copy()\n",
    "\n",
    "# Check how many samples we retained\n",
    "print(\"Original samples:\", len(df))\n",
    "print(\"Filtered samples:\", len(df_filtered))\n",
    "print(\"Percentage retained:\", (len(df_filtered)/len(df))*100, \"%\")\n",
    "\n",
    "# Check new strata distribution\n",
    "new_strata_counts = df_filtered['strata'].value_counts()\n",
    "print(\"\\nNew strata statistics:\")\n",
    "print(\"Number of strata:\", len(new_strata_counts))\n",
    "print(\"Min samples per stratum:\", new_strata_counts.min())\n",
    "print(\"Max samples per stratum:\", new_strata_counts.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Split for GNN Training (Power Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Training set size: 152984\n",
      "GNN Validation set size: 19102\n",
      "GNN Test set size: 19121\n",
      "\n",
      "Train set statistics:\n",
      "Mean CPU power: 142.37\n",
      "Mean node power: 840.20\n",
      "Average cores: 107.41\n",
      "\n",
      "Val set statistics:\n",
      "Mean CPU power: 141.78\n",
      "Mean node power: 840.45\n",
      "Average cores: 107.05\n",
      "\n",
      "Test set statistics:\n",
      "Mean CPU power: 142.78\n",
      "Mean node power: 839.84\n",
      "Average cores: 106.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate out the final test set (10%)\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    df_filtered.index,\n",
    "    test_size=0.1,\n",
    "    stratify=df_filtered['strata'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Then split remaining data into train (80% of total) and validation (10% of total)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx,\n",
    "    test_size=0.111,  # 0.111 of 90% is 10% of total\n",
    "    stratify=df_filtered.loc[train_val_idx, 'strata'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create the datasets\n",
    "gnn_train_data = df_filtered.loc[train_idx]\n",
    "gnn_val_data = df_filtered.loc[val_idx]\n",
    "gnn_test_data = df_filtered.loc[test_idx]\n",
    "\n",
    "# Print split sizes\n",
    "print(\"GNN Training set size:\", len(gnn_train_data))\n",
    "print(\"GNN Validation set size:\", len(gnn_val_data))\n",
    "print(\"GNN Test set size:\", len(gnn_test_data))\n",
    "\n",
    "# Verify distribution preservation\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    print(f\"\\n{split_name} set statistics:\")\n",
    "    print(f\"Mean CPU power: {split_data['mean_cpu_power'].mean():.2f}\")\n",
    "    print(f\"Mean node power: {split_data['mean_node_power'].mean():.2f}\")\n",
    "    print(f\"Average cores: {split_data['num_cores_alloc'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL training sequences. For RL, we need to maintain temporal ordering while ensuring we have enough episodes for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set:\n",
      "Time span: 160 days\n",
      "Average jobs per day: 954.19\n",
      "\n",
      "Val set:\n",
      "Time span: 159 days\n",
      "Average jobs per day: 119.65\n",
      "\n",
      "Test set:\n",
      "Time span: 159 days\n",
      "Average jobs per day: 119.78\n"
     ]
    }
   ],
   "source": [
    "# Sort data by submit_time within each split\n",
    "gnn_train_data = gnn_train_data.sort_values('submit_time')\n",
    "gnn_val_data = gnn_val_data.sort_values('submit_time')\n",
    "gnn_test_data = gnn_test_data.sort_values('submit_time')\n",
    "\n",
    "# Create temporal sequences for RL\n",
    "# Let's first analyze the time spans\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    time_span = split_data['submit_time'].max() - split_data['submit_time'].min()\n",
    "    jobs_per_day = len(split_data) / (time_span.total_seconds() / (24*3600))\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"Time span: {time_span.days} days\")\n",
    "    print(f\"Average jobs per day: {jobs_per_day:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the RL episodes. For HPC scheduling, we want episodes that:\n",
    "\n",
    "- Are long enough to capture meaningful scheduling decisions\n",
    "- Short enough to provide many training episodes\n",
    "- Account for daily power price variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train episodes:\n",
      "Number of episodes: 155\n",
      "Average jobs per episode: 986.99\n",
      "Min jobs per episode: 9\n",
      "Max jobs per episode: 7888\n",
      "\n",
      "Val episodes:\n",
      "Number of episodes: 154\n",
      "Average jobs per episode: 124.04\n",
      "Min jobs per episode: 1\n",
      "Max jobs per episode: 1119\n",
      "\n",
      "Test episodes:\n",
      "Number of episodes: 154\n",
      "Average jobs per episode: 124.16\n",
      "Min jobs per episode: 1\n",
      "Max jobs per episode: 1115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create episodes of 24 hours each (to capture full day cycles)\n",
    "def create_episodes(df, episode_duration_hours=24):\n",
    "    start_time = df['submit_time'].min()\n",
    "    end_time = df['submit_time'].max()\n",
    "    \n",
    "    episodes = []\n",
    "    current_time = start_time\n",
    "    \n",
    "    while current_time < end_time:\n",
    "        episode_end = current_time + pd.Timedelta(hours=episode_duration_hours)\n",
    "        \n",
    "        # Get jobs submitted during this episode\n",
    "        episode_jobs = df[\n",
    "            (df['submit_time'] >= current_time) & \n",
    "            (df['submit_time'] < episode_end)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(episode_jobs) > 0:\n",
    "            episodes.append(episode_jobs)\n",
    "            \n",
    "        current_time = episode_end\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Create episodes for each split\n",
    "train_episodes = create_episodes(gnn_train_data)\n",
    "val_episodes = create_episodes(gnn_val_data)\n",
    "test_episodes = create_episodes(gnn_test_data)\n",
    "\n",
    "# Print episode statistics\n",
    "for split_name, split_episodes in [(\"Train\", train_episodes), \n",
    "                                 (\"Val\", val_episodes), \n",
    "                                 (\"Test\", test_episodes)]:\n",
    "    num_episodes = len(split_episodes)\n",
    "    avg_jobs = np.mean([len(ep) for ep in split_episodes])\n",
    "    min_jobs = np.min([len(ep) for ep in split_episodes])\n",
    "    max_jobs = np.max([len(ep) for ep in split_episodes])\n",
    "    \n",
    "    print(f\"\\n{split_name} episodes:\")\n",
    "    print(f\"Number of episodes: {num_episodes}\")\n",
    "    print(f\"Average jobs per episode: {avg_jobs:.2f}\")\n",
    "    print(f\"Min jobs per episode: {min_jobs}\")\n",
    "    print(f\"Max jobs per episode: {max_jobs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN training structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing GNN data splits:\n",
      "\n",
      "Train set:\n",
      "Number of jobs: 152984\n",
      "Mean node power: 840.20\n",
      "Std node power: 182.99\n",
      "\n",
      "Val set:\n",
      "Number of jobs: 19102\n",
      "Mean node power: 840.45\n",
      "Std node power: 183.30\n",
      "\n",
      "Test set:\n",
      "Number of jobs: 19121\n",
      "Mean node power: 839.84\n",
      "Std node power: 184.18\n"
     ]
    }
   ],
   "source": [
    "# Features for GNN (selected based on correlations with mean_node_power)\n",
    "gnn_features = [\n",
    "    'cores_per_task',     # Strong correlation (0.657)\n",
    "    'num_cores_alloc',    # Strong correlation (0.669)\n",
    "    'num_nodes_alloc',    # Important for node-level prediction\n",
    "    'num_gpus_alloc',     # Relevant for power consumption\n",
    "    'mem_alloc',          # Moderate correlation (0.580)\n",
    "    'run_time'           # Strongest correlation (0.962)\n",
    "]\n",
    "\n",
    "# Single target variable\n",
    "power_target = ['mean_node_power']\n",
    "\n",
    "# Prepare the data structure for GNN\n",
    "def prepare_gnn_data(df, features=gnn_features, target=power_target):\n",
    "    \"\"\"\n",
    "    Prepare data for GNN training with KNN structure\n",
    "    \"\"\"\n",
    "    # Normalize features (important for KNN)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[features])\n",
    "    y = df[target].values\n",
    "    \n",
    "    # For each job, we'll find k similar jobs\n",
    "    k = 5  # Can be tuned\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 because it includes the point itself\n",
    "    knn.fit(X)\n",
    "    \n",
    "    # Get indices of k nearest neighbors for each job\n",
    "    distances, indices = knn.kneighbors(X)\n",
    "    \n",
    "    return X, y, indices[:, 1:], scaler  # Exclude self from neighbors\n",
    "\n",
    "print(\"Preparing GNN data splits:\")\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"Number of jobs: {len(split_data)}\")\n",
    "    print(f\"Mean node power: {split_data['mean_node_power'].mean():.2f}\")\n",
    "    print(f\"Std node power: {split_data['mean_node_power'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs in training: 152984\n",
      "Total number of jobs in validation: 19102\n",
      "Total number of jobs in test: 19121\n",
      "\n",
      "Preparing data...\n",
      "Calculating neighbors for training set...\n",
      "Calculating neighbors for validation set...\n",
      "Calculating neighbors for test set...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_graph_batch() missing 1 required positional argument: 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 104\u001b[0m\n\u001b[1;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m PowerPredictionGNN(input_dim\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPreparing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m train_graphs, val_graphs, test_graphs, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Print graph statistics\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGraph statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 39\u001b[0m, in \u001b[0;36mprepare_training_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m test_target \u001b[38;5;241m=\u001b[39m gnn_test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_node_power\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create graph batches\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train_graphs \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_graph_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbor_indices_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m val_graphs \u001b[38;5;241m=\u001b[39m create_graph_batch(val_features, neighbor_indices_val)\n\u001b[1;32m     41\u001b[0m test_graphs \u001b[38;5;241m=\u001b[39m create_graph_batch(test_features, neighbor_indices_test)\n",
      "\u001b[0;31mTypeError\u001b[0m: create_graph_batch() missing 1 required positional argument: 'targets'"
     ]
    }
   ],
   "source": [
    "class PowerPredictionGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super(PowerPredictionGNN, self).__init__()\n",
    "        \n",
    "        # Wider network\n",
    "        self.node_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Deeper graph convolutions\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # More robust prediction head\n",
    "        self.power_pred = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial embeddings\n",
    "        x = self.node_embed(x)\n",
    "        \n",
    "        # Graph convolutions with residual connections\n",
    "        x1 = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1, edge_index))) + x1\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2, edge_index))) + x2\n",
    "        \n",
    "        # Power prediction\n",
    "        return self.power_pred(x3)\n",
    "    \n",
    "    \n",
    "def create_graph_batch(features, neighbor_indices, targets, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Create graph batches ensuring consistent shapes between features and targets\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    num_samples = len(features)\n",
    "    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, num_samples)\n",
    "        batch_features = features[i:batch_end]\n",
    "        batch_neighbors = neighbor_indices[i:batch_end]\n",
    "        batch_targets = targets[i:batch_end]\n",
    "        \n",
    "        # Create edges based on KNN within this batch\n",
    "        edge_index = []\n",
    "        for job_idx, neighbors in enumerate(batch_neighbors):\n",
    "            # Only add edges for neighbors that are within this batch\n",
    "            valid_neighbors = [n - i for n in neighbors if i <= n < batch_end]\n",
    "            for neighbor in valid_neighbors:\n",
    "                edge_index.append([job_idx, neighbor])\n",
    "                edge_index.append([neighbor, job_idx])\n",
    "        \n",
    "        if not edge_index:  # If no valid edges in this batch\n",
    "            continue\n",
    "            \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        \n",
    "        # Create graph with matching features and targets\n",
    "        graph = Data(\n",
    "            x=torch.FloatTensor(batch_features),\n",
    "            edge_index=edge_index,\n",
    "            y=torch.FloatTensor(batch_targets).unsqueeze(-1)  # Add dimension for consistency\n",
    "        )\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "# Modified training configuration\n",
    "config = {\n",
    "    'hidden_dim': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 100,\n",
    "    'early_stopping_patience': 15\n",
    "}\n",
    "\n",
    "# Print some statistics about the data\n",
    "print(f\"Total number of jobs in training: {len(gnn_train_data)}\")\n",
    "print(f\"Total number of jobs in validation: {len(gnn_val_data)}\")\n",
    "print(f\"Total number of jobs in test: {len(gnn_test_data)}\")\n",
    "\n",
    "# Initialize model and start training\n",
    "input_dim = len(gnn_features)\n",
    "model = PowerPredictionGNN(input_dim=input_dim)\n",
    "\n",
    "\n",
    "print(\"\\nPreparing data...\")\n",
    "train_graphs, val_graphs, test_graphs, scaler = prepare_training_data()\n",
    "\n",
    "# Print graph statistics\n",
    "print(\"\\nGraph statistics:\")\n",
    "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
    "print(f\"Number of validation graphs: {len(val_graphs)}\")\n",
    "print(f\"Number of test graphs: {len(test_graphs)}\")\n",
    "\n",
    "print(\"\\nSample graph details:\")\n",
    "if train_graphs:\n",
    "    g = train_graphs[0]\n",
    "    print(f\"First training graph - Nodes: {g.x.size(0)}, Edges: {g.edge_index.size(1)//2}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, val_losses = train_gnn(model, train_graphs, val_graphs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_prediction_loss(pred_power, true_power):\n",
    "    \"\"\"\n",
    "    Custom loss function ensuring shape consistency\n",
    "    \"\"\"\n",
    "    # Ensure shapes match\n",
    "    pred_power = pred_power.squeeze()\n",
    "    true_power = true_power.squeeze()\n",
    "    \n",
    "    # Check shapes are identical\n",
    "    assert pred_power.shape == true_power.shape, f\"Shape mismatch: pred={pred_power.shape}, true={true_power.shape}\"\n",
    "    \n",
    "    # Compute normalized MSE\n",
    "    pred_norm = (pred_power - pred_power.mean()) / (pred_power.std() + 1e-8)\n",
    "    true_norm = (true_power - true_power.mean()) / (true_power.std() + 1e-8)\n",
    "    \n",
    "    mse_loss = F.mse_loss(pred_norm, true_norm)\n",
    "    \n",
    "    # Compute relative error\n",
    "    relative_error = torch.abs(pred_power - true_power) / (torch.abs(true_power) + 1e-8)\n",
    "    relative_loss = torch.mean(relative_error)\n",
    "    \n",
    "    return mse_loss + 0.1 * relative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rl_state(episode_df, gnn_model, scaler):\n",
    "    \"\"\"\n",
    "    Prepare RL state with GNN power predictions\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    features_normalized = scaler.transform(episode_df[gnn_features])\n",
    "    \n",
    "    # Get power predictions from GNN\n",
    "    with torch.no_grad():\n",
    "        power_pred = gnn_model(features_normalized)\n",
    "    \n",
    "    # Create state dictionary\n",
    "    state = {\n",
    "        'job_features': episode_df[gnn_features].values,\n",
    "        'predicted_power': power_pred.numpy(),\n",
    "        'submit_time': episode_df['submit_time'].values,\n",
    "        'time_of_day': episode_df['submit_time'].dt.hour.values\n",
    "    }\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_gnn(model, train_graphs, val_graphs, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    early_stopping = EarlyStopping(patience=config['early_stopping_patience'])\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for graph in train_graphs:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(graph.x, graph.edge_index)\n",
    "            loss = power_prediction_loss(out, graph.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for graph in val_graphs:\n",
    "                out = model(graph.x, graph.edge_index)\n",
    "                val_loss += power_prediction_loss(out, graph.y).item()\n",
    "        \n",
    "        train_loss /= len(train_graphs)\n",
    "        val_loss /= len(val_graphs)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_predictions(model, test_graphs, power_scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for graph in test_graphs:\n",
    "            pred = model(graph.x, graph.edge_index)\n",
    "            # Ensure 2D shape before inverse transform\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            true = graph.y.squeeze().numpy().reshape(-1, 1)\n",
    "            \n",
    "            # Inverse transform\n",
    "            pred = power_scaler.inverse_transform(pred)\n",
    "            true = power_scaler.inverse_transform(true)\n",
    "            \n",
    "            predictions.extend(pred.flatten())\n",
    "            true_values.extend(true.flatten())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    \n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mean_relative_error': np.mean(np.abs(predictions - true_values) / (true_values + 1e-8)),\n",
    "        'mean_pred': np.mean(predictions),\n",
    "        'mean_true': np.mean(true_values),\n",
    "        'std_pred': np.std(predictions),\n",
    "        'std_true': np.std(true_values)\n",
    "    }\n",
    "    \n",
    "    # Add some percentile errors\n",
    "    for p in [25, 50, 75, 90]:\n",
    "        error_percentile = np.percentile(np.abs(predictions - true_values), p)\n",
    "        results[f'error_p{p}'] = error_percentile\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "PowerPredictionGNN(\n",
      "  (node_embed): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (power_pred): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Preparing data...\n",
      "Calculating neighbors for training set...\n",
      "Calculating neighbors for validation set...\n",
      "Calculating neighbors for test set...\n",
      "\n",
      "Model architecture:\n",
      "PowerPredictionGNN(\n",
      "  (node_embed): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (power_pred): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Starting training...\n",
      "Epoch 001, Train Loss: 0.4389, Val Loss: 0.3778\n",
      "Epoch 002, Train Loss: 0.2657, Val Loss: 0.3650\n",
      "Epoch 003, Train Loss: 0.2261, Val Loss: 0.2974\n",
      "Epoch 004, Train Loss: 0.1972, Val Loss: 0.2678\n",
      "Epoch 005, Train Loss: 0.1858, Val Loss: 0.2271\n",
      "Epoch 006, Train Loss: 0.1663, Val Loss: 0.1935\n",
      "Epoch 007, Train Loss: 0.1608, Val Loss: 0.1873\n",
      "Epoch 008, Train Loss: 0.1560, Val Loss: 0.1695\n",
      "Epoch 009, Train Loss: 0.1577, Val Loss: 0.1676\n",
      "Epoch 010, Train Loss: 0.1472, Val Loss: 0.1891\n",
      "Epoch 011, Train Loss: 0.1479, Val Loss: 0.1451\n",
      "Epoch 012, Train Loss: 0.1384, Val Loss: 0.1528\n",
      "Epoch 013, Train Loss: 0.1328, Val Loss: 0.1626\n",
      "Epoch 014, Train Loss: 0.1295, Val Loss: 0.1493\n",
      "Epoch 015, Train Loss: 0.1322, Val Loss: 0.1238\n",
      "Epoch 016, Train Loss: 0.1267, Val Loss: 0.1444\n",
      "Epoch 017, Train Loss: 0.1225, Val Loss: 0.1580\n",
      "Epoch 018, Train Loss: 0.1219, Val Loss: 0.1335\n",
      "Epoch 019, Train Loss: 0.1244, Val Loss: 0.1088\n",
      "Epoch 020, Train Loss: 0.1203, Val Loss: 0.1131\n",
      "Epoch 021, Train Loss: 0.1263, Val Loss: 0.1248\n",
      "Epoch 022, Train Loss: 0.1213, Val Loss: 0.1949\n",
      "Epoch 023, Train Loss: 0.1216, Val Loss: 0.1276\n",
      "Epoch 024, Train Loss: 0.1190, Val Loss: 0.1034\n",
      "Epoch 025, Train Loss: 0.1154, Val Loss: 0.1322\n",
      "Epoch 026, Train Loss: 0.1107, Val Loss: 0.1195\n",
      "Epoch 027, Train Loss: 0.1137, Val Loss: 0.0930\n",
      "Epoch 028, Train Loss: 0.1118, Val Loss: 0.1024\n",
      "Epoch 029, Train Loss: 0.1121, Val Loss: 0.1069\n",
      "Epoch 030, Train Loss: 0.1066, Val Loss: 0.0941\n",
      "Epoch 031, Train Loss: 0.1038, Val Loss: 0.0841\n",
      "Epoch 032, Train Loss: 0.1046, Val Loss: 0.0812\n",
      "Epoch 033, Train Loss: 0.1030, Val Loss: 0.1156\n",
      "Epoch 034, Train Loss: 0.1006, Val Loss: 0.1000\n",
      "Epoch 035, Train Loss: 0.0994, Val Loss: 0.1079\n",
      "Epoch 036, Train Loss: 0.0960, Val Loss: 0.1420\n",
      "Epoch 037, Train Loss: 0.0995, Val Loss: 0.0840\n",
      "Epoch 038, Train Loss: 0.0941, Val Loss: 0.0676\n",
      "Epoch 039, Train Loss: 0.0952, Val Loss: 0.0664\n",
      "Epoch 040, Train Loss: 0.0947, Val Loss: 0.0777\n",
      "Epoch 041, Train Loss: 0.0920, Val Loss: 0.0949\n",
      "Epoch 042, Train Loss: 0.0903, Val Loss: 0.0838\n",
      "Epoch 043, Train Loss: 0.0907, Val Loss: 0.0711\n",
      "Epoch 044, Train Loss: 0.0879, Val Loss: 0.0765\n",
      "Epoch 045, Train Loss: 0.0896, Val Loss: 0.0719\n",
      "Epoch 046, Train Loss: 0.0870, Val Loss: 0.0902\n",
      "Epoch 047, Train Loss: 0.0845, Val Loss: 0.0704\n",
      "Epoch 048, Train Loss: 0.0913, Val Loss: 0.0663\n",
      "Epoch 049, Train Loss: 0.0832, Val Loss: 0.0713\n",
      "Epoch 050, Train Loss: 0.0885, Val Loss: 0.0895\n",
      "Epoch 051, Train Loss: 0.0898, Val Loss: 0.0685\n",
      "Epoch 052, Train Loss: 0.0808, Val Loss: 0.0740\n",
      "Epoch 053, Train Loss: 0.0817, Val Loss: 0.0688\n",
      "Epoch 054, Train Loss: 0.0767, Val Loss: 0.0679\n",
      "Epoch 055, Train Loss: 0.0789, Val Loss: 0.0808\n",
      "Epoch 056, Train Loss: 0.0800, Val Loss: 0.0667\n",
      "Epoch 057, Train Loss: 0.0758, Val Loss: 0.0617\n",
      "Epoch 058, Train Loss: 0.0795, Val Loss: 0.1066\n",
      "Epoch 059, Train Loss: 0.0771, Val Loss: 0.0611\n",
      "Epoch 060, Train Loss: 0.0757, Val Loss: 0.0750\n",
      "Epoch 061, Train Loss: 0.0737, Val Loss: 0.0603\n",
      "Epoch 062, Train Loss: 0.0712, Val Loss: 0.0722\n",
      "Epoch 063, Train Loss: 0.0727, Val Loss: 0.0769\n",
      "Epoch 064, Train Loss: 0.0715, Val Loss: 0.0753\n",
      "Epoch 065, Train Loss: 0.0718, Val Loss: 0.0710\n",
      "Epoch 066, Train Loss: 0.0779, Val Loss: 0.0675\n",
      "Epoch 067, Train Loss: 0.0716, Val Loss: 0.0683\n",
      "Epoch 068, Train Loss: 0.0711, Val Loss: 0.0688\n",
      "Epoch 069, Train Loss: 0.0699, Val Loss: 0.0648\n",
      "Epoch 070, Train Loss: 0.0678, Val Loss: 0.0787\n",
      "Epoch 071, Train Loss: 0.0683, Val Loss: 0.0692\n",
      "Epoch 072, Train Loss: 0.0734, Val Loss: 0.0611\n",
      "Epoch 073, Train Loss: 0.0713, Val Loss: 0.0674\n",
      "Epoch 074, Train Loss: 0.0669, Val Loss: 0.0631\n",
      "Epoch 075, Train Loss: 0.0660, Val Loss: 0.0727\n",
      "Epoch 076, Train Loss: 0.0659, Val Loss: 0.0728\n",
      "Early stopping triggered\n",
      "\n",
      "Evaluating model...\n",
      "mse: 1328.0101\n",
      "rmse: 36.4419\n",
      "mae: 26.9636\n",
      "r2: 0.9608\n",
      "mean_relative_error: 0.0370\n",
      "mean_pred: 841.9838\n",
      "mean_true: 839.8434\n",
      "std_pred: 154.4159\n",
      "std_true: 184.1739\n",
      "error_p25: 11.8698\n",
      "error_p50: 17.6879\n",
      "error_p75: 36.9659\n",
      "error_p90: 57.0168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_neighbors(features, k=5):\n",
    "    \"\"\"Calculate k-nearest neighbors for each job\"\"\"\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 because it includes the point itself\n",
    "    knn.fit(features)\n",
    "    distances, indices = knn.kneighbors(features)\n",
    "    return indices[:, 1:]  # Exclude self from neighbors\n",
    "\n",
    "def prepare_training_data():\n",
    "    \"\"\"\n",
    "    Prepare training data with proper scaling\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    feature_scaler = StandardScaler()\n",
    "    train_features = feature_scaler.fit_transform(gnn_train_data[gnn_features])\n",
    "    val_features = feature_scaler.transform(gnn_val_data[gnn_features])\n",
    "    test_features = feature_scaler.transform(gnn_test_data[gnn_features])\n",
    "    \n",
    "    # Scale power values\n",
    "    power_scaler = StandardScaler()\n",
    "    train_target = power_scaler.fit_transform(gnn_train_data[['mean_node_power']])\n",
    "    val_target = power_scaler.transform(gnn_val_data[['mean_node_power']])\n",
    "    test_target = power_scaler.transform(gnn_test_data[['mean_node_power']])\n",
    "    \n",
    "    # Calculate neighbor indices\n",
    "    print(\"Calculating neighbors for training set...\")\n",
    "    train_neighbors = calculate_neighbors(train_features)\n",
    "    print(\"Calculating neighbors for validation set...\")\n",
    "    val_neighbors = calculate_neighbors(val_features)\n",
    "    print(\"Calculating neighbors for test set...\")\n",
    "    test_neighbors = calculate_neighbors(test_features)\n",
    "    \n",
    "    # Create graph batches\n",
    "    train_graphs = create_graph_batch(train_features, train_neighbors, train_target)\n",
    "    val_graphs = create_graph_batch(val_features, val_neighbors, val_target)\n",
    "    test_graphs = create_graph_batch(test_features, test_neighbors, test_target)\n",
    "    \n",
    "    return train_graphs, val_graphs, test_graphs, feature_scaler, power_scaler\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = len(gnn_features)  # Number of features\n",
    "model = PowerPredictionGNN(input_dim=input_dim)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Initialize and train\n",
    "print(\"\\nPreparing data...\")\n",
    "# Fix the unpacking to match the 5 return values\n",
    "train_graphs, val_graphs, test_graphs, feature_scaler, power_scaler = prepare_training_data()\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "# Initialize model with the new architecture\n",
    "model = PowerPredictionGNN(input_dim=len(gnn_features), hidden_dim=config['hidden_dim'])\n",
    "print(model)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, val_losses = train_gnn(model, train_graphs, val_graphs, config)\n",
    "\n",
    "print(\"\\nEvaluating model...\")\n",
    "# Pass both scalers to evaluation\n",
    "test_metrics = evaluate_predictions(model, test_graphs, power_scaler)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot predictions vs actual values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mpredictions\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(true_values[:\u001b[38;5;241m1000\u001b[39m], predictions[:\u001b[38;5;241m1000\u001b[39m], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot predictions vs actual values\n",
    "if len(predictions) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(true_values[:1000], predictions[:1000], alpha=0.5)\n",
    "    plt.plot([true_values.min(), true_values.max()], [true_values.min(), true_values.max()], 'r--')\n",
    "    plt.xlabel('True Power (W)')\n",
    "    plt.ylabel('Predicted Power (W)')\n",
    "    plt.title('Predicted vs Actual Power Usage')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerAwareScheduler:\n",
    "    def __init__(self, gnn_model, feature_scaler, power_scaler, max_nodes=100):\n",
    "        self.gnn_model = gnn_model\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.power_scaler = power_scaler\n",
    "        self.max_nodes = max_nodes\n",
    "        \n",
    "    def predict_power(self, job_features):\n",
    "        \"\"\"Predict power consumption for a job\"\"\"\n",
    "        # Scale features\n",
    "        scaled_features = self.feature_scaler.transform(job_features[gnn_features])\n",
    "        \n",
    "        # Create temporary graph for single job\n",
    "        x = torch.FloatTensor(scaled_features)\n",
    "        # Simple self-connection for single job\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        \n",
    "        # Get prediction\n",
    "        self.gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self.gnn_model(x, edge_index)\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            pred = self.power_scaler.inverse_transform(pred)\n",
    "        \n",
    "        return pred.item()\n",
    "    \n",
    "    def get_state(self, current_jobs, node_status, current_time):\n",
    "        \"\"\"\n",
    "        Create state representation for RL\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            'available_nodes': sum(node_status),\n",
    "            'time_of_day': current_time.hour,\n",
    "            'waiting_jobs': len(current_jobs),\n",
    "            'predicted_powers': [self.predict_power(job) for job in current_jobs],\n",
    "            'job_sizes': [job['num_nodes_alloc'] for job in current_jobs],\n",
    "            'node_status': node_status\n",
    "        }\n",
    "        return state\n",
    "    \n",
    "    def get_valid_actions(self, state, job):\n",
    "        \"\"\"\n",
    "        Get valid actions for current state and job\n",
    "        \"\"\"\n",
    "        required_nodes = job['num_nodes_alloc']\n",
    "        available_nodes = state['available_nodes']\n",
    "        \n",
    "        actions = {\n",
    "            'schedule_now': available_nodes >= required_nodes,\n",
    "            'wait': True,  # Can always choose to wait\n",
    "            'valid_node_sets': self._get_valid_node_sets(state, required_nodes)\n",
    "        }\n",
    "        return actions\n",
    "    \n",
    "    def _get_valid_node_sets(self, state, required_nodes):\n",
    "        \"\"\"\n",
    "        Find valid node combinations for job placement\n",
    "        \"\"\"\n",
    "        node_status = state['node_status']\n",
    "        valid_sets = []\n",
    "        \n",
    "        # Simple contiguous allocation strategy\n",
    "        current_set = []\n",
    "        for i, status in enumerate(node_status):\n",
    "            if status:  # Node is available\n",
    "                current_set.append(i)\n",
    "                if len(current_set) == required_nodes:\n",
    "                    valid_sets.append(current_set.copy())\n",
    "                    current_set = current_set[1:]  # Slide window\n",
    "            else:\n",
    "                current_set = []\n",
    "        \n",
    "        return valid_sets\n",
    "\n",
    "def calculate_reward(action, job, predicted_power, actual_power, waiting_time, power_price):\n",
    "    \"\"\"\n",
    "    Calculate reward for scheduling decision\n",
    "    \"\"\"\n",
    "    # Base reward for job completion\n",
    "    completion_reward = 1.0\n",
    "    \n",
    "    # Power prediction accuracy penalty\n",
    "    power_error = abs(predicted_power - actual_power) / actual_power\n",
    "    prediction_penalty = -0.2 * power_error\n",
    "    \n",
    "    # Power cost component\n",
    "    power_cost = -0.1 * power_price * actual_power\n",
    "    \n",
    "    # Waiting time penalty\n",
    "    waiting_penalty = -0.05 * waiting_time\n",
    "    \n",
    "    # Total reward\n",
    "    reward = completion_reward + prediction_penalty + power_cost + waiting_penalty\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL agent using PPO (Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "class PPOSchedulerNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PPOSchedulerNet, self).__init__()\n",
    "        \n",
    "        # Shared feature layers\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Policy head (actor)\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Value head (critic)\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.feature_net(state)\n",
    "        action_probs = F.softmax(self.policy_net(features), dim=-1)\n",
    "        value = self.value_net(features)\n",
    "        return action_probs, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = PPOSchedulerNet(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        \n",
    "    def get_action(self, state, valid_actions=None):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action_probs, _ = self.policy(state)\n",
    "        \n",
    "        # Mask invalid actions\n",
    "        if valid_actions is not None:\n",
    "            mask = torch.zeros_like(action_probs)\n",
    "            mask[valid_actions] = 1\n",
    "            action_probs = action_probs * mask\n",
    "            action_probs = action_probs / action_probs.sum()\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item()\n",
    "    \n",
    "    def update(self, states, actions, old_log_probs, rewards, advantages):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        \n",
    "        # PPO update\n",
    "        for _ in range(5):  # Multiple epochs of training\n",
    "            action_probs, values = self.policy(states)\n",
    "            dist = Categorical(action_probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            \n",
    "            # Policy loss\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(values.squeeze(), rewards)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "            \n",
    "            # Update\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "def create_state_vector(scheduler_state):\n",
    "    \"\"\"Convert scheduler state dict to vector\"\"\"\n",
    "    # Normalize features\n",
    "    state_vector = [\n",
    "        scheduler_state['available_nodes'] / scheduler_state['max_nodes'],\n",
    "        scheduler_state['time_of_day'] / 24.0,\n",
    "        scheduler_state['waiting_jobs'] / 100.0,  # Normalize by assumed max queue\n",
    "        np.mean(scheduler_state['predicted_powers']) / 1000.0,  # Normalize power\n",
    "        np.mean(scheduler_state['job_sizes']) / scheduler_state['max_nodes'],\n",
    "        sum(scheduler_state['node_status']) / len(scheduler_state['node_status'])\n",
    "    ]\n",
    "    return np.array(state_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode generation and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(scheduler, agent, env_config, max_steps=1000):\n",
    "    \"\"\"Generate a training episode\"\"\"\n",
    "    episode_data = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'log_probs': [],\n",
    "        'rewards': [],\n",
    "        'values': []\n",
    "    }\n",
    "    \n",
    "    # Initialize environment\n",
    "    current_time = env_config['start_time']\n",
    "    node_status = [True] * env_config['num_nodes']\n",
    "    waiting_jobs = env_config['initial_jobs'].copy()\n",
    "    \n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        if len(waiting_jobs) == 0:\n",
    "            break\n",
    "            \n",
    "        # Get current state\n",
    "        state = scheduler.get_state(waiting_jobs, node_status, current_time)\n",
    "        state_vector = create_state_vector(state)\n",
    "        \n",
    "        # Get valid actions\n",
    "        valid_actions = scheduler.get_valid_actions(state, waiting_jobs[0])\n",
    "        \n",
    "        # Get action from agent\n",
    "        action, log_prob = agent.get_action(state_vector, valid_actions['valid_indices'])\n",
    "        \n",
    "        # Execute action\n",
    "        current_job = waiting_jobs[0]\n",
    "        power_price = get_power_price(current_time.hour)\n",
    "        \n",
    "        if action == 0 and valid_actions['schedule_now']:  # Schedule now\n",
    "            predicted_power = scheduler.predict_power(current_job)\n",
    "            actual_power = current_job['mean_node_power']\n",
    "            \n",
    "            # Schedule job and update cost tracker\n",
    "            cost = scheduler.schedule_job(current_job, current_time, node_status)\n",
    "            waiting_jobs.pop(0)\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = calculate_energy_focused_reward(\n",
    "                action, current_job, predicted_power, actual_power,\n",
    "                0, power_price\n",
    "            )\n",
    "        else:  # Wait\n",
    "            reward = -0.1  # Small penalty for waiting\n",
    "            \n",
    "        # Store transition\n",
    "        episode_data['states'].append(state_vector)\n",
    "        episode_data['actions'].append(action)\n",
    "        episode_data['log_probs'].append(log_prob)\n",
    "        episode_data['rewards'].append(reward)\n",
    "        \n",
    "        # Update time\n",
    "        current_time += pd.Timedelta(minutes=5)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update node status based on job completions (simplified)\n",
    "        for i, status in enumerate(node_status):\n",
    "            if not status and np.random.random() < 0.1:  # 10% chance of job completion\n",
    "                node_status[i] = True\n",
    "    \n",
    "    episode_data['total_reward'] = total_reward\n",
    "    return episode_data\n",
    "\n",
    "def train_rl_scheduler(scheduler, agent, env_config, num_episodes=1000):\n",
    "    \"\"\"Train the RL scheduler\"\"\"\n",
    "    all_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate episode\n",
    "        episode_data = generate_episode(scheduler, agent, env_config)\n",
    "        \n",
    "        # Calculate returns and advantages\n",
    "        returns = calculate_returns(episode_data['rewards'], agent.gamma)\n",
    "        advantages = calculate_advantages(returns, episode_data['values'])\n",
    "        \n",
    "        # Update agent\n",
    "        agent.update(\n",
    "            episode_data['states'],\n",
    "            episode_data['actions'],\n",
    "            episode_data['log_probs'],\n",
    "            returns,\n",
    "            advantages\n",
    "        )\n",
    "        \n",
    "        # Log progress\n",
    "        episode_reward = sum(episode_data['rewards'])\n",
    "        all_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Average Reward: {np.mean(all_rewards[-10:]):.2f}\")\n",
    "    \n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment stepping function and power price variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power_price(time_of_day):\n",
    "    \"\"\"Get power price based on time of day\"\"\"\n",
    "    # Simple day/night pricing\n",
    "    if 8 <= time_of_day < 20:  # Peak hours\n",
    "        return 1.5\n",
    "    else:  # Off-peak hours\n",
    "        return 1.0\n",
    "\n",
    "def env_step(state, action, scheduler, waiting_jobs, node_status, current_time):\n",
    "    \"\"\"Execute one environment step\"\"\"\n",
    "    if len(waiting_jobs) == 0:\n",
    "        return state, 0, True, {}\n",
    "    \n",
    "    current_job = waiting_jobs[0]\n",
    "    power_price = get_power_price(current_time.hour)\n",
    "    \n",
    "    if action == 0:  # Schedule now\n",
    "        if scheduler.can_schedule(current_job, node_status):\n",
    "            # Predict power\n",
    "            predicted_power = scheduler.predict_power(current_job)\n",
    "            \n",
    "            # Schedule job\n",
    "            node_set = scheduler.allocate_nodes(current_job, node_status)\n",
    "            waiting_jobs.pop(0)\n",
    "            \n",
    "            # Get actual power (simulated)\n",
    "            actual_power = current_job['mean_node_power']\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = calculate_reward(\n",
    "                action, current_job, predicted_power, actual_power,\n",
    "                0, power_price\n",
    "            )\n",
    "        else:\n",
    "            reward = -1  # Penalty for invalid scheduling\n",
    "    else:  # Wait\n",
    "        reward = -0.1  # Small penalty for waiting\n",
    "    \n",
    "    # Update time\n",
    "    next_time = current_time + pd.Timedelta(minutes=5)\n",
    "    \n",
    "    # Update node status based on job completions\n",
    "    node_status = update_node_status(node_status, next_time)\n",
    "    \n",
    "    done = len(waiting_jobs) == 0\n",
    "    info = {\n",
    "        'next_time': next_time,\n",
    "        'node_status': node_status,\n",
    "        'waiting_jobs': waiting_jobs\n",
    "    }\n",
    "    \n",
    "    next_state = scheduler.get_state(waiting_jobs, node_status, next_time)\n",
    "    \n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerCostTracker:\n",
    "    def __init__(self):\n",
    "        self.total_energy_cost = 0\n",
    "        self.total_energy_used = 0\n",
    "        self.hourly_costs = []\n",
    "        self.hourly_usage = []\n",
    "        \n",
    "    def update(self, power_usage, duration_hours, power_price):\n",
    "        \"\"\"Track energy usage and cost\"\"\"\n",
    "        energy_used = power_usage * duration_hours  # kWh\n",
    "        cost = energy_used * power_price\n",
    "        \n",
    "        self.total_energy_cost += cost\n",
    "        self.total_energy_used += energy_used\n",
    "        self.hourly_costs.append(cost)\n",
    "        self.hourly_usage.append(energy_used)\n",
    "        \n",
    "    def get_metrics(self):\n",
    "        return {\n",
    "            'total_cost': self.total_energy_cost,\n",
    "            'total_energy': self.total_energy_used,\n",
    "            'avg_cost_per_kwh': self.total_energy_cost / (self.total_energy_used + 1e-8),\n",
    "            'hourly_costs': self.hourly_costs,\n",
    "            'hourly_usage': self.hourly_usage\n",
    "        }\n",
    "\n",
    "def calculate_energy_focused_reward(action, job, predicted_power, actual_power, waiting_time, power_price):\n",
    "    \"\"\"\n",
    "    Calculate reward focusing on energy cost optimization\n",
    "    \"\"\"\n",
    "    # Energy cost component (primary focus)\n",
    "    energy_usage = actual_power * (job['run_time'] / 3600)  # Convert runtime to hours\n",
    "    energy_cost = power_price * energy_usage\n",
    "    cost_penalty = -0.5 * energy_cost / 1000  # Normalize cost impact\n",
    "    \n",
    "    # Prediction accuracy reward (helps in making better scheduling decisions)\n",
    "    power_prediction_error = abs(predicted_power - actual_power) / actual_power\n",
    "    prediction_reward = 0.2 * (1 - power_prediction_error)  # More accurate predictions get higher reward\n",
    "    \n",
    "    # Waiting time penalty (secondary consideration)\n",
    "    wait_penalty = -0.1 * (waiting_time / 3600)  # Small penalty for making jobs wait\n",
    "    \n",
    "    # Combine rewards with emphasis on energy cost\n",
    "    total_reward = cost_penalty + prediction_reward + wait_penalty\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "class EnergyAwareScheduler(PowerAwareScheduler):\n",
    "    def __init__(self, gnn_model, feature_scaler, power_scaler, max_nodes=100):\n",
    "        super().__init__(gnn_model, feature_scaler, power_scaler, max_nodes)\n",
    "        self.cost_tracker = PowerCostTracker()\n",
    "        \n",
    "    def get_state(self, waiting_jobs, node_status, current_time):\n",
    "        \"\"\"Get current state of the system\"\"\"\n",
    "        if len(waiting_jobs) == 0:\n",
    "            return {\n",
    "                'available_nodes': sum(node_status),\n",
    "                'max_nodes': self.max_nodes,\n",
    "                'time_of_day': current_time.hour,\n",
    "                'waiting_jobs': 0,\n",
    "                'predicted_powers': [0],\n",
    "                'job_sizes': [0],\n",
    "                'node_status': node_status\n",
    "            }\n",
    "            \n",
    "        # Get predictions for waiting jobs\n",
    "        predicted_powers = [self.predict_power(job) for job in waiting_jobs[:10]]  # Limit to first 10 jobs\n",
    "        job_sizes = [job['num_nodes_alloc'] for job in waiting_jobs[:10]]\n",
    "        \n",
    "        return {\n",
    "            'available_nodes': sum(node_status),\n",
    "            'max_nodes': self.max_nodes,\n",
    "            'time_of_day': current_time.hour,\n",
    "            'waiting_jobs': len(waiting_jobs),\n",
    "            'predicted_powers': predicted_powers,\n",
    "            'job_sizes': job_sizes,\n",
    "            'node_status': node_status\n",
    "        }\n",
    "    \n",
    "    def get_valid_actions(self, state, job):\n",
    "        \"\"\"Get valid actions for current state and job\"\"\"\n",
    "        required_nodes = job['num_nodes_alloc']\n",
    "        available_nodes = state['available_nodes']\n",
    "        \n",
    "        valid_actions = []\n",
    "        # Action 0: Schedule now\n",
    "        if available_nodes >= required_nodes:\n",
    "            valid_actions.append(0)\n",
    "        # Action 1: Wait\n",
    "        valid_actions.append(1)\n",
    "        \n",
    "        return {\n",
    "            'valid_indices': valid_actions,\n",
    "            'schedule_now': available_nodes >= required_nodes,\n",
    "            'wait': True\n",
    "        }\n",
    "\n",
    "def train_energy_aware_scheduler(scheduler, agent, env_config, num_episodes=100):\n",
    "    \"\"\"Train the scheduler with focus on energy cost optimization\"\"\"\n",
    "    episode_metrics = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        episode_data = generate_episode(scheduler, agent, env_config)\n",
    "        \n",
    "        # Calculate energy-focused metrics\n",
    "        metrics = {\n",
    "            'episode': episode,\n",
    "            'total_cost': scheduler.cost_tracker.total_energy_cost,\n",
    "            'total_energy': scheduler.cost_tracker.total_energy_used,\n",
    "            'total_reward': episode_data['total_reward']\n",
    "        }\n",
    "        episode_metrics.append(metrics)\n",
    "        \n",
    "        # Reset cost tracker for next episode\n",
    "        scheduler.cost_tracker = PowerCostTracker()\n",
    "        \n",
    "        # Log progress\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(f\"Total Energy Cost: ${metrics['total_cost']:.2f}\")\n",
    "            print(f\"Total Energy Used: {metrics['total_energy']:.2f} kWh\")\n",
    "            print(f\"Total Reward: {metrics['total_reward']:.2f}\\n\")\n",
    "    \n",
    "    return episode_metrics\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_scheduler(scheduler, test_jobs, power_prices):\n",
    "    \"\"\"\n",
    "    Evaluate scheduler performance on test jobs\n",
    "    \"\"\"\n",
    "    baseline_cost = 0\n",
    "    optimized_cost = 0\n",
    "    \n",
    "    # Calculate baseline cost (FCFS scheduling)\n",
    "    current_time = pd.Timestamp.now()\n",
    "    for job in test_jobs:\n",
    "        power_price = power_prices[current_time.hour]\n",
    "        duration = job['run_time'] / 3600\n",
    "        energy = job['mean_node_power'] * duration\n",
    "        baseline_cost += energy * power_price\n",
    "        current_time += pd.Timedelta(hours=duration)\n",
    "    \n",
    "    # Calculate optimized cost using our scheduler\n",
    "    scheduler.cost_tracker = PowerCostTracker()\n",
    "    for job in test_jobs:\n",
    "        optimized_cost += scheduler.schedule_job(job, current_time, [True] * scheduler.max_nodes)\n",
    "    \n",
    "    savings = (baseline_cost - optimized_cost) / baseline_cost * 100\n",
    "    \n",
    "    return {\n",
    "        'baseline_cost': baseline_cost,\n",
    "        'optimized_cost': optimized_cost,\n",
    "        'savings_percentage': savings,\n",
    "        'energy_usage': scheduler.cost_tracker.total_energy_used,\n",
    "        'hourly_distribution': scheduler.cost_tracker.hourly_usage\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting energy-aware scheduling experiment...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run_experiment() got an unexpected keyword argument 'gnn_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Now we can run the previous experiment code\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting energy-aware scheduling experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m scheduler, agent, training_metrics, eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgnn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgnn_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgnn_test_data\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: run_experiment() got an unexpected keyword argument 'gnn_model'"
     ]
    }
   ],
   "source": [
    "# 1. First, let's define all necessary classes\n",
    "class PowerCostTracker:\n",
    "    def __init__(self):\n",
    "        self.total_energy_cost = 0\n",
    "        self.total_energy_used = 0\n",
    "        self.hourly_costs = []\n",
    "        self.hourly_usage = [0] * 24  # Initialize hourly usage tracking\n",
    "        \n",
    "    def update(self, power_usage, duration_hours, power_price, hour):\n",
    "        energy_used = power_usage * duration_hours\n",
    "        cost = energy_used * power_price\n",
    "        \n",
    "        self.total_energy_cost += cost\n",
    "        self.total_energy_used += energy_used\n",
    "        self.hourly_costs.append(cost)\n",
    "        self.hourly_usage[hour] += energy_used\n",
    "        \n",
    "    def get_metrics(self):\n",
    "        return {\n",
    "            'total_cost': self.total_energy_cost,\n",
    "            'total_energy': self.total_energy_used,\n",
    "            'avg_cost_per_kwh': self.total_energy_cost / (self.total_energy_used + 1e-8),\n",
    "            'hourly_costs': self.hourly_costs,\n",
    "            'hourly_usage': self.hourly_usage\n",
    "        }\n",
    "\n",
    "class PowerAwareScheduler:\n",
    "    def __init__(self, gnn_model, feature_scaler, power_scaler, max_nodes=100):\n",
    "        self.gnn_model = gnn_model\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.power_scaler = power_scaler\n",
    "        self.max_nodes = max_nodes\n",
    "        \n",
    "    def predict_power(self, job):\n",
    "        # Scale features\n",
    "        job_features = np.array([[\n",
    "            job[feature] for feature in gnn_features\n",
    "        ]])\n",
    "        scaled_features = self.feature_scaler.transform(job_features)\n",
    "        \n",
    "        # Create temporary graph for single job\n",
    "        x = torch.FloatTensor(scaled_features)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        \n",
    "        # Get prediction\n",
    "        self.gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self.gnn_model(x, edge_index)\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            pred = self.power_scaler.inverse_transform(pred)\n",
    "        \n",
    "        return pred.item()\n",
    "    \n",
    "    def can_schedule(self, job, node_status):\n",
    "        return sum(node_status) >= job['num_nodes_alloc']\n",
    "    \n",
    "    def allocate_nodes(self, job, node_status):\n",
    "        required_nodes = job['num_nodes_alloc']\n",
    "        allocated_nodes = []\n",
    "        \n",
    "        for i, available in enumerate(node_status):\n",
    "            if available and len(allocated_nodes) < required_nodes:\n",
    "                allocated_nodes.append(i)\n",
    "                node_status[i] = False\n",
    "        \n",
    "        return allocated_nodes\n",
    "\n",
    "class EnergyAwareScheduler(PowerAwareScheduler):\n",
    "    def __init__(self, gnn_model, feature_scaler, power_scaler, max_nodes=100):\n",
    "        super().__init__(gnn_model, feature_scaler, power_scaler, max_nodes)\n",
    "        self.cost_tracker = PowerCostTracker()\n",
    "        \n",
    "    def schedule_job(self, job, current_time, node_status):\n",
    "        predicted_power = self.predict_power(job)\n",
    "        power_price = get_power_price(current_time.hour)\n",
    "        \n",
    "        expected_duration = job['run_time'] / 3600\n",
    "        expected_energy_cost = predicted_power * expected_duration * power_price\n",
    "        \n",
    "        if self.can_schedule(job, node_status):\n",
    "            actual_power = job['mean_node_power']\n",
    "            self.cost_tracker.update(\n",
    "                actual_power, \n",
    "                expected_duration, \n",
    "                power_price, \n",
    "                current_time.hour\n",
    "            )\n",
    "            self.allocate_nodes(job, node_status)\n",
    "        \n",
    "        return expected_energy_cost\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = PPOSchedulerNet(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        \n",
    "    def get_action(self, state, valid_actions=None):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action_probs, _ = self.policy(state)\n",
    "        \n",
    "        if valid_actions is not None:\n",
    "            mask = torch.zeros_like(action_probs)\n",
    "            mask[valid_actions] = 1\n",
    "            action_probs = action_probs * mask\n",
    "            action_probs = action_probs / (action_probs.sum() + 1e-8)\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "class PPOSchedulerNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PPOSchedulerNet, self).__init__()\n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.feature_net(state)\n",
    "        action_probs = F.softmax(self.policy_net(features), dim=-1)\n",
    "        value = self.value_net(features)\n",
    "        return action_probs, value\n",
    "\n",
    "# Now we can run the previous experiment code\n",
    "print(\"Starting energy-aware scheduling experiment...\")\n",
    "scheduler, agent, training_metrics, eval_results = run_experiment(\n",
    "    gnn_model=model,\n",
    "    feature_scaler=feature_scaler,\n",
    "    power_scaler=power_scaler,\n",
    "    train_data=gnn_train_data,\n",
    "    test_data=gnn_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model state:\n",
      "GNN Model trained: True\n",
      "Training data size: 152984\n",
      "Test data size: 19121\n",
      "Feature scaler available: True\n",
      "Power scaler available: True\n",
      "Starting energy-aware scheduling experiment...\n",
      "Setting up experiment...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EnergyAwareScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting energy-aware scheduling experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     scheduler, agent, training_metrics, eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgnn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using the model we trained earlier\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgnn_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgnn_test_data\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Print final results\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 50\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(gnn_model, feature_scaler, power_scaler, train_data, test_data)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_experiment\u001b[39m(gnn_model, feature_scaler, power_scaler, train_data, test_data):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     scheduler, agent \u001b[38;5;241m=\u001b[39m \u001b[43msetup_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower_scaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     env_config \u001b[38;5;241m=\u001b[39m create_env_config(train_data)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 21\u001b[0m, in \u001b[0;36msetup_experiment\u001b[0;34m(gnn_model, feature_scaler, power_scaler)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup_experiment\u001b[39m(gnn_model, feature_scaler, power_scaler):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Create the energy-aware scheduler\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mEnergyAwareScheduler\u001b[49m(\n\u001b[1;32m     22\u001b[0m         gnn_model\u001b[38;5;241m=\u001b[39mgnn_model,\n\u001b[1;32m     23\u001b[0m         feature_scaler\u001b[38;5;241m=\u001b[39mfeature_scaler,\n\u001b[1;32m     24\u001b[0m         power_scaler\u001b[38;5;241m=\u001b[39mpower_scaler,\n\u001b[1;32m     25\u001b[0m         max_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Create the RL agent\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     state_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m  \u001b[38;5;66;03m# From our state vector definition\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EnergyAwareScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# First ensure we have our GNN model and data from earlier\n",
    "print(\"Current model state:\")\n",
    "print(f\"GNN Model trained: {model is not None}\")  # Using the model we trained earlier\n",
    "print(f\"Training data size: {len(gnn_train_data)}\")\n",
    "print(f\"Test data size: {len(gnn_test_data)}\")\n",
    "print(f\"Feature scaler available: {feature_scaler is not None}\")\n",
    "print(f\"Power scaler available: {power_scaler is not None}\")\n",
    "\n",
    "# If everything is available, proceed with the experiment setup\n",
    "def setup_experiment(gnn_model, feature_scaler, power_scaler):\n",
    "    # Create the energy-aware scheduler\n",
    "    scheduler = EnergyAwareScheduler(\n",
    "        gnn_model=gnn_model,\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        max_nodes=100\n",
    "    )\n",
    "    \n",
    "    # Create the RL agent\n",
    "    state_dim = 6  # From our state vector definition\n",
    "    action_dim = 2  # Schedule now or wait\n",
    "    agent = PPOAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "    \n",
    "    return scheduler, agent\n",
    "\n",
    "def create_env_config(train_data):\n",
    "    # Create a sample of jobs for training\n",
    "    train_jobs = train_data.sample(n=1000).to_dict('records')\n",
    "    \n",
    "    config = {\n",
    "        'start_time': pd.Timestamp.now(),\n",
    "        'num_nodes': 100,\n",
    "        'initial_jobs': train_jobs,\n",
    "        'power_prices': {i: get_power_price(i) for i in range(24)}\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "def run_experiment(gnn_model, feature_scaler, power_scaler, train_data, test_data):\n",
    "    print(\"Setting up experiment...\")\n",
    "    scheduler, agent = setup_experiment(gnn_model, feature_scaler, power_scaler)\n",
    "    env_config = create_env_config(train_data)\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    training_metrics = train_energy_aware_scheduler(\n",
    "        scheduler=scheduler,\n",
    "        agent=agent,\n",
    "        env_config=env_config,\n",
    "        num_episodes=100\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating scheduler...\")\n",
    "    test_jobs = test_data.sample(n=100).to_dict('records')\n",
    "    eval_results = evaluate_scheduler(\n",
    "        scheduler=scheduler,\n",
    "        test_jobs=test_jobs,\n",
    "        power_prices=env_config['power_prices']\n",
    "    )\n",
    "    \n",
    "    plot_results(training_metrics, eval_results)\n",
    "    \n",
    "    return scheduler, agent, training_metrics, eval_results\n",
    "\n",
    "# Run the experiment\n",
    "if 'model' in locals() and 'feature_scaler' in locals() and 'power_scaler' in locals():\n",
    "    print(\"Starting energy-aware scheduling experiment...\")\n",
    "    scheduler, agent, training_metrics, eval_results = run_experiment(\n",
    "        gnn_model=model,  # Using the model we trained earlier\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        train_data=gnn_train_data,\n",
    "        test_data=gnn_test_data\n",
    "    )\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Baseline Cost: ${eval_results['baseline_cost']:.2f}\")\n",
    "    print(f\"Optimized Cost: ${eval_results['optimized_cost']:.2f}\")\n",
    "    print(f\"Cost Savings: {eval_results['savings_percentage']:.2f}%\")\n",
    "    print(f\"Total Energy Usage: {eval_results['energy_usage']:.2f} kWh\")\n",
    "else:\n",
    "    print(\"Please ensure the GNN model, scalers, and data are available before running the experiment.\")\n",
    "    print(\"We need:\")\n",
    "    print(\"1. Trained GNN model (model)\")\n",
    "    print(\"2. Feature scaler (feature_scaler)\")\n",
    "    print(\"3. Power scaler (power_scaler)\")\n",
    "    print(\"4. Training data (gnn_train_data)\")\n",
    "    print(\"5. Test data (gnn_test_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting energy-aware scheduling experiment...\n",
      "Setting up experiment...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting energy-aware scheduling experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m scheduler, agent, training_metrics, eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Print final results\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[62], line 44\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_experiment\u001b[39m():\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     scheduler, agent \u001b[38;5;241m=\u001b[39m \u001b[43msetup_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     env_config \u001b[38;5;241m=\u001b[39m create_env_config()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[62], line 10\u001b[0m, in \u001b[0;36msetup_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup_experiment\u001b[39m():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Use our existing GNN model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mgnn_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Create the energy-aware scheduler\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m EnergyAwareScheduler(\n\u001b[1;32m     14\u001b[0m         gnn_model\u001b[38;5;241m=\u001b[39mgnn_model,\n\u001b[1;32m     15\u001b[0m         feature_scaler\u001b[38;5;241m=\u001b[39mfeature_scaler,\n\u001b[1;32m     16\u001b[0m         power_scaler\u001b[38;5;241m=\u001b[39mpower_scaler,\n\u001b[1;32m     17\u001b[0m         max_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     18\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize the schedulers and required components\n",
    "def setup_experiment():\n",
    "    # Use our existing GNN model\n",
    "    gnn_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Create the energy-aware scheduler\n",
    "    scheduler = EnergyAwareScheduler(\n",
    "        gnn_model=gnn_model,\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        max_nodes=100\n",
    "    )\n",
    "    \n",
    "    # Create the RL agent\n",
    "    state_dim = 6  # From our state vector definition\n",
    "    action_dim = 2  # Schedule now or wait\n",
    "    agent = PPOAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "    \n",
    "    return scheduler, agent\n",
    "\n",
    "# Create training environment configuration\n",
    "def create_env_config():\n",
    "    # Create a sample of jobs for training\n",
    "    train_jobs = gnn_train_data.sample(n=1000).to_dict('records')\n",
    "    \n",
    "    config = {\n",
    "        'start_time': pd.Timestamp.now(),\n",
    "        'num_nodes': 100,\n",
    "        'initial_jobs': train_jobs,\n",
    "        'power_prices': {i: get_power_price(i) for i in range(24)}\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Run training and evaluation\n",
    "def run_experiment():\n",
    "    print(\"Setting up experiment...\")\n",
    "    scheduler, agent = setup_experiment()\n",
    "    env_config = create_env_config()\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    training_metrics = train_energy_aware_scheduler(\n",
    "        scheduler=scheduler,\n",
    "        agent=agent,\n",
    "        env_config=env_config,\n",
    "        num_episodes=100  # Start with fewer episodes for testing\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating scheduler...\")\n",
    "    # Use test data for evaluation\n",
    "    test_jobs = gnn_test_data.sample(n=100).to_dict('records')\n",
    "    eval_results = evaluate_scheduler(\n",
    "        scheduler=scheduler,\n",
    "        test_jobs=test_jobs,\n",
    "        power_prices=env_config['power_prices']\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(training_metrics, eval_results)\n",
    "    \n",
    "    return scheduler, agent, training_metrics, eval_results\n",
    "\n",
    "def plot_results(training_metrics, eval_results):\n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot training progress\n",
    "    episodes = [m['episode'] for m in training_metrics]\n",
    "    costs = [m['total_cost'] for m in training_metrics]\n",
    "    axes[0, 0].plot(episodes, costs)\n",
    "    axes[0, 0].set_title('Training Progress - Total Cost')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Total Energy Cost ($)')\n",
    "    \n",
    "    # Plot energy usage distribution\n",
    "    axes[0, 1].bar(range(24), eval_results['hourly_distribution'])\n",
    "    axes[0, 1].set_title('Energy Usage Distribution by Hour')\n",
    "    axes[0, 1].set_xlabel('Hour of Day')\n",
    "    axes[0, 1].set_ylabel('Energy Usage (kWh)')\n",
    "    \n",
    "    # Plot cost comparison\n",
    "    costs = ['Baseline', 'Optimized']\n",
    "    values = [eval_results['baseline_cost'], eval_results['optimized_cost']]\n",
    "    axes[1, 0].bar(costs, values)\n",
    "    axes[1, 0].set_title('Cost Comparison')\n",
    "    axes[1, 0].set_ylabel('Cost ($)')\n",
    "    \n",
    "    # Plot rewards\n",
    "    rewards = [m['cumulative_reward'] for m in training_metrics]\n",
    "    axes[1, 1].plot(episodes, rewards)\n",
    "    axes[1, 1].set_title('Cumulative Rewards')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Reward')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the experiment\n",
    "print(\"Starting energy-aware scheduling experiment...\")\n",
    "scheduler, agent, training_metrics, eval_results = run_experiment()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Baseline Cost: ${eval_results['baseline_cost']:.2f}\")\n",
    "print(f\"Optimized Cost: ${eval_results['optimized_cost']:.2f}\")\n",
    "print(f\"Cost Savings: {eval_results['savings_percentage']:.2f}%\")\n",
    "print(f\"Total Energy Usage: {eval_results['energy_usage']:.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_episode(episode_df, gnn_power_predictions):\n",
    "    \"\"\"\n",
    "    Prepare episode data for RL training\n",
    "    \"\"\"\n",
    "    # State space\n",
    "    state_features = {\n",
    "        'current_time': episode_df['submit_time'],\n",
    "        'job_features': episode_df[gnn_features],\n",
    "        'predicted_power': gnn_power_predictions,\n",
    "        'node_status': None,  # You'll need to track this during training\n",
    "        'time_of_day': episode_df['submit_time'].dt.hour\n",
    "    }\n",
    "    \n",
    "    # Action space\n",
    "    action_space = {\n",
    "        'node_allocation': episode_df['num_nodes_alloc'].unique(),\n",
    "        'start_time': np.arange(0, 24*60, 5)  # 5-minute intervals\n",
    "    }\n",
    "    \n",
    "    return state_features, action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Variable Episode Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_episode(episode_df, max_jobs=8000):  # Based on max seen in training\n",
    "    \"\"\"\n",
    "    Normalize episode data for RL training\n",
    "    \"\"\"\n",
    "    # Pad shorter episodes with masked entries\n",
    "    # Sort jobs by submit time\n",
    "    episode_df = episode_df.sort_values('submit_time')\n",
    "    \n",
    "    # Create fixed-size tensors with masking\n",
    "    job_features = np.zeros((max_jobs, len(gnn_features)))\n",
    "    mask = np.zeros(max_jobs)\n",
    "    \n",
    "    # Fill actual jobs\n",
    "    n_jobs = len(episode_df)\n",
    "    job_features[:n_jobs] = episode_df[gnn_features].values\n",
    "    mask[:n_jobs] = 1\n",
    "    \n",
    "    return job_features, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyze the temporal distribution of jobs in your dataset?\n",
    "- Check for any seasonal patterns in job submissions?\n",
    "- Examine the distribution of job similarities for KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# df['mean_node_power'].plot.hist(bins=50)\n",
    "# plt.title('Distribution of Mean Node Power')\n",
    "# plt.xlabel('Mean Power Time')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyze daily/weekly patterns in more detail?\n",
    "- Check for any anomalies or special events in the data?\n",
    "- Examine the distribution of job types across different time periods?\n",
    "- Calculate the resource utilization patterns by week to ensure consistent distribution across splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Temporal Analysis ===\n",
      "Dataset spans: 160 days\n",
      "Average jobs per day: 1195.72\n",
      "\n",
      "=== Job Similarity Analysis ===\n",
      "mean_distance: 0.0029\n",
      "std_distance: 0.0216\n",
      "min_distance: 0.0000\n",
      "max_distance: 1.5721\n",
      "\n",
      "=== Resource Utilization ===\n",
      "avg_nodes_per_hour: 2.23\n",
      "max_nodes_per_hour: 31.48\n",
      "avg_cores_per_hour: 194.09\n",
      "max_cores_per_hour: 3765.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21557/3413504686.py:75: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  }).resample('1H').mean()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyze_temporal_patterns(df):\n",
    "    # 1. Basic temporal statistics\n",
    "    time_stats = {\n",
    "        'start_date': df['submit_time'].min(),\n",
    "        'end_date': df['submit_time'].max(),\n",
    "        'total_days': (df['submit_time'].max() - df['submit_time'].min()).days,\n",
    "        'jobs_per_day': len(df) / (df['submit_time'].max() - df['submit_time'].min()).days\n",
    "    }\n",
    "    \n",
    "    # 2. Jobs by day of week\n",
    "    df['day_of_week'] = df['submit_time'].dt.day_name()\n",
    "    jobs_by_day = df['day_of_week'].value_counts()\n",
    "    \n",
    "    # 3. Jobs by hour of day\n",
    "    df['hour_of_day'] = df['submit_time'].dt.hour\n",
    "    jobs_by_hour = df['hour_of_day'].value_counts().sort_index()\n",
    "    \n",
    "    # 4. Weekly job patterns\n",
    "    df['week_number'] = df['submit_time'].dt.isocalendar().week\n",
    "    jobs_by_week = df.groupby('week_number').size()\n",
    "    \n",
    "    # 5. Job characteristics over time\n",
    "    weekly_metrics = df.groupby('week_number').agg({\n",
    "        'run_time': 'mean',\n",
    "        'mean_node_power': 'mean',\n",
    "        'num_cores_alloc': 'mean'\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        'time_stats': time_stats,\n",
    "        'jobs_by_day': jobs_by_day,\n",
    "        'jobs_by_hour': jobs_by_hour,\n",
    "        'jobs_by_week': jobs_by_week,\n",
    "        'weekly_metrics': weekly_metrics\n",
    "    }\n",
    "\n",
    "def analyze_job_similarities(df):\n",
    "    # Create feature matrix for KNN analysis\n",
    "    features = ['cores_per_task', 'num_cores_alloc', 'num_nodes_alloc', \n",
    "               'mem_alloc', 'run_time']\n",
    "    \n",
    "    # Normalize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(df[features])\n",
    "    \n",
    "    # Calculate similarity distribution (using sample for efficiency)\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    sample_size = min(10000, len(df))\n",
    "    sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=6).fit(normalized_features)\n",
    "    distances, _ = nbrs.kneighbors(normalized_features[sample_indices])\n",
    "    \n",
    "    # Analyze distance distribution (excluding self-similarity)\n",
    "    similarity_stats = {\n",
    "        'mean_distance': distances[:, 1:].mean(),\n",
    "        'std_distance': distances[:, 1:].std(),\n",
    "        'min_distance': distances[:, 1:].min(),\n",
    "        'max_distance': distances[:, 1:].max()\n",
    "    }\n",
    "    \n",
    "    return similarity_stats\n",
    "\n",
    "def analyze_resource_patterns(df):\n",
    "    # Analyze node and resource utilization patterns\n",
    "    resource_patterns = df.groupby('start_time').agg({\n",
    "        'num_nodes_alloc': 'sum',\n",
    "        'num_cores_alloc': 'sum',\n",
    "        'num_gpus_alloc': 'sum'\n",
    "    }).resample('1H').mean()\n",
    "    \n",
    "    resource_stats = {\n",
    "        'avg_nodes_per_hour': resource_patterns['num_nodes_alloc'].mean(),\n",
    "        'max_nodes_per_hour': resource_patterns['num_nodes_alloc'].max(),\n",
    "        'avg_cores_per_hour': resource_patterns['num_cores_alloc'].mean(),\n",
    "        'max_cores_per_hour': resource_patterns['num_cores_alloc'].max()\n",
    "    }\n",
    "    \n",
    "    return resource_stats\n",
    "\n",
    "# Print results\n",
    "temporal_analysis = analyze_temporal_patterns(df)\n",
    "similarity_analysis = analyze_job_similarities(df)\n",
    "resource_analysis = analyze_resource_patterns(df)\n",
    "\n",
    "print(\"=== Temporal Analysis ===\")\n",
    "print(f\"Dataset spans: {temporal_analysis['time_stats']['total_days']} days\")\n",
    "print(f\"Average jobs per day: {temporal_analysis['time_stats']['jobs_per_day']:.2f}\")\n",
    "\n",
    "print(\"\\n=== Job Similarity Analysis ===\")\n",
    "for key, value in similarity_analysis.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== Resource Utilization ===\")\n",
    "for key, value in resource_analysis.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Daily Pattern Analysis ===\n",
      "             count\n",
      "day_of_week       \n",
      "Friday       32642\n",
      "Monday       24452\n",
      "Saturday     21010\n",
      "Sunday       13817\n",
      "Thursday     31801\n",
      "Tuesday      34127\n",
      "Wednesday    33466\n",
      "\n",
      "=== Job Type Distribution ===\n",
      "job_type\n",
      "very_short    1594.291667\n",
      "short         1594.291667\n",
      "medium        1594.291667\n",
      "long          1594.291667\n",
      "very_long     1594.291667\n",
      "dtype: float64\n",
      "\n",
      "=== Anomalous Days ===\n",
      "Number of anomalous days: 3\n",
      "\n",
      "=== Distribution Stability ===\n",
      "Coefficient of Variation:\n",
      "run_time_cv: 0.6444\n",
      "power_cv: 0.2104\n",
      "cores_cv: 0.8931\n",
      "\n",
      "=== Period Similarity (KS-test p-values) ===\n",
      "Q1_vs_Q2: 0.0000\n",
      "Q1_vs_Q3: 0.0000\n",
      "Q1_vs_Q4: 0.0000\n",
      "Q2_vs_Q3: 0.0000\n",
      "Q2_vs_Q4: 0.0000\n",
      "Q3_vs_Q4: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21557/3907222911.py:32: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  weekly_job_types = df.groupby(['week_number', 'job_type']).size().unstack(fill_value=0)\n",
      "/tmp/ipykernel_21557/3907222911.py:79: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  period_stats = df.groupby('period').agg({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "def detailed_temporal_analysis(df):\n",
    "    # 1. Daily and Weekly Patterns\n",
    "    df['day_of_week'] = df['submit_time'].dt.day_name()\n",
    "    df['hour_of_day'] = df['submit_time'].dt.hour\n",
    "    df['week_number'] = df['submit_time'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['submit_time'].dt.dayofyear\n",
    "\n",
    "    # Daily patterns\n",
    "    daily_patterns = df.groupby('day_of_week').agg({\n",
    "        'job_id': 'count',\n",
    "        'run_time': ['mean', 'std'],\n",
    "        'mean_node_power': ['mean', 'std'],\n",
    "        'num_cores_alloc': 'mean',\n",
    "        'num_nodes_alloc': 'mean'\n",
    "    })\n",
    "\n",
    "    # Hourly patterns\n",
    "    hourly_patterns = df.groupby('hour_of_day').agg({\n",
    "        'job_id': 'count',\n",
    "        'run_time': 'mean',\n",
    "        'mean_node_power': 'mean'\n",
    "    })\n",
    "\n",
    "    # 2. Job Type Distribution Over Time\n",
    "    job_types = pd.qcut(df['run_time'], q=5, labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n",
    "    df['job_type'] = job_types\n",
    "    weekly_job_types = df.groupby(['week_number', 'job_type']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # 3. Resource Utilization Patterns\n",
    "    weekly_resources = df.groupby('week_number').agg({\n",
    "        'num_cores_alloc': ['mean', 'max', 'std'],\n",
    "        'num_nodes_alloc': ['mean', 'max', 'std'],\n",
    "        'num_gpus_alloc': ['mean', 'max', 'std']\n",
    "    })\n",
    "\n",
    "    # 4. Anomaly Detection\n",
    "    def detect_anomalies(series, threshold=3):\n",
    "        z_scores = np.abs(stats.zscore(series))\n",
    "        return np.where(z_scores > threshold)[0]\n",
    "\n",
    "    daily_job_counts = df.groupby('day_of_year')['job_id'].count()\n",
    "    anomalous_days = detect_anomalies(daily_job_counts)\n",
    "    \n",
    "    # 5. Job Characteristics Stability\n",
    "    weekly_job_stats = df.groupby('week_number').agg({\n",
    "        'run_time': ['mean', 'std', 'median'],\n",
    "        'mean_node_power': ['mean', 'std', 'median'],\n",
    "        'num_cores_alloc': ['mean', 'std'],\n",
    "        'mem_alloc': ['mean', 'std']\n",
    "    })\n",
    "\n",
    "    # Calculate coefficient of variation for stability analysis\n",
    "    cv_stats = {\n",
    "        'run_time_cv': weekly_job_stats['run_time']['std'].mean() / weekly_job_stats['run_time']['mean'].mean(),\n",
    "        'power_cv': weekly_job_stats['mean_node_power']['std'].mean() / weekly_job_stats['mean_node_power']['mean'].mean(),\n",
    "        'cores_cv': weekly_job_stats['num_cores_alloc']['std'].mean() / weekly_job_stats['num_cores_alloc']['mean'].mean()\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'daily_patterns': daily_patterns,\n",
    "        'hourly_patterns': hourly_patterns,\n",
    "        'weekly_job_types': weekly_job_types,\n",
    "        'weekly_resources': weekly_resources,\n",
    "        'anomalous_days': anomalous_days,\n",
    "        'weekly_job_stats': weekly_job_stats,\n",
    "        'cv_stats': cv_stats\n",
    "    }\n",
    "\n",
    "def analyze_job_distribution_stability(df):\n",
    "    # Split the timeline into 4 equal periods\n",
    "    df['period'] = pd.qcut(df['submit_time'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "    \n",
    "    # Analyze job characteristics distribution in each period\n",
    "    period_stats = df.groupby('period').agg({\n",
    "        'run_time': ['mean', 'std', 'median'],\n",
    "        'mean_node_power': ['mean', 'std', 'median'],\n",
    "        'num_cores_alloc': ['mean', 'std'],\n",
    "        'num_nodes_alloc': ['mean', 'std'],\n",
    "        'job_id': 'count'\n",
    "    })\n",
    "    \n",
    "    # Test for distribution similarity between periods\n",
    "    periods = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    ks_tests = {}\n",
    "    for i in range(len(periods)-1):\n",
    "        for j in range(i+1, len(periods)):\n",
    "            p1, p2 = periods[i], periods[j]\n",
    "            ks_test = stats.ks_2samp(\n",
    "                df[df['period'] == p1]['run_time'],\n",
    "                df[df['period'] == p2]['run_time']\n",
    "            )\n",
    "            ks_tests[f'{p1}_vs_{p2}'] = ks_test.pvalue\n",
    "    \n",
    "    return {\n",
    "        'period_stats': period_stats,\n",
    "        'distribution_similarity': ks_tests\n",
    "    }\n",
    "\n",
    "# Generate analyses\n",
    "temporal_results = detailed_temporal_analysis(df)\n",
    "stability_results = analyze_job_distribution_stability(df)\n",
    "\n",
    "# Print key findings\n",
    "print(\"=== Daily Pattern Analysis ===\")\n",
    "print(temporal_results['daily_patterns']['job_id'])\n",
    "\n",
    "print(\"\\n=== Job Type Distribution ===\")\n",
    "print(temporal_results['weekly_job_types'].mean())\n",
    "\n",
    "print(\"\\n=== Anomalous Days ===\")\n",
    "print(f\"Number of anomalous days: {len(temporal_results['anomalous_days'])}\")\n",
    "\n",
    "print(\"\\n=== Distribution Stability ===\")\n",
    "print(\"Coefficient of Variation:\")\n",
    "for key, value in temporal_results['cv_stats'].items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== Period Similarity (KS-test p-values) ===\")\n",
    "for key, value in stability_results['distribution_similarity'].items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "- Exact split dates aligned with complete weeks\n",
    "- Detailed statistics about each split\n",
    "- Distribution similarity tests\n",
    "- Anomaly distribution across splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Split Dates ===\n",
      "first_monday: 2020-05-11 15:55:59\n",
      "training_end: 2020-08-10 15:55:59\n",
      "validation_end: 2020-09-07 15:55:59\n",
      "final_date: 2020-10-12 23:50:26\n",
      "\n",
      "=== Split Statistics ===\n",
      "\n",
      "TRAINING:\n",
      "Number of jobs: 85716\n",
      "Jobs per day: 952.40\n",
      "Mean runtime: 4949.54\n",
      "Mean power: 799.96\n",
      "Anomalous days: 2\n",
      "\n",
      "VALIDATION:\n",
      "Number of jobs: 33666\n",
      "Jobs per day: 1246.89\n",
      "Mean runtime: 5853.47\n",
      "Mean power: 849.18\n",
      "Anomalous days: 0\n",
      "\n",
      "TESTING:\n",
      "Number of jobs: 68756\n",
      "Jobs per day: 1964.46\n",
      "Mean runtime: 6674.00\n",
      "Mean power: 883.04\n",
      "Anomalous days: 1\n",
      "\n",
      "=== Distribution Similarity (KS-test p-values) ===\n",
      "\n",
      "run_time:\n",
      "train_val: 0.0000\n",
      "train_test: 0.0000\n",
      "val_test: 0.0000\n",
      "\n",
      "mean_node_power:\n",
      "train_val: 0.0000\n",
      "train_test: 0.0000\n",
      "val_test: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_split_points(df):\n",
    "    # Sort by submit time to ensure chronological order\n",
    "    df = df.sort_values('submit_time')\n",
    "    \n",
    "    # Get date range\n",
    "    start_date = df['submit_time'].min()\n",
    "    end_date = df['submit_time'].max()\n",
    "    \n",
    "    # Find first Monday for clean week splits\n",
    "    first_monday = start_date + timedelta(days=(7 - start_date.weekday()))\n",
    "    \n",
    "    # Calculate split points\n",
    "    total_days = (end_date - first_monday).days\n",
    "    training_days = int(total_days * 0.6)\n",
    "    validation_days = int(total_days * 0.2)\n",
    "    \n",
    "    # Adjust to complete weeks\n",
    "    training_days = training_days - (training_days % 7)\n",
    "    validation_days = validation_days - (validation_days % 7)\n",
    "    testing_days = total_days - training_days - validation_days\n",
    "    \n",
    "    # Calculate split dates\n",
    "    training_end = first_monday + timedelta(days=training_days)\n",
    "    validation_end = training_end + timedelta(days=validation_days)\n",
    "    \n",
    "    # Create masks for each split\n",
    "    train_mask = (df['submit_time'] >= first_monday) & (df['submit_time'] < training_end)\n",
    "    val_mask = (df['submit_time'] >= training_end) & (df['submit_time'] < validation_end)\n",
    "    test_mask = (df['submit_time'] >= validation_end)\n",
    "    \n",
    "    # Analyze each split\n",
    "    splits = {\n",
    "        'training': df[train_mask],\n",
    "        'validation': df[val_mask],\n",
    "        'testing': df[test_mask]\n",
    "    }\n",
    "    \n",
    "    split_stats = {}\n",
    "    for name, split_df in splits.items():\n",
    "        # Basic statistics\n",
    "        split_stats[name] = {\n",
    "            'start_date': split_df['submit_time'].min(),\n",
    "            'end_date': split_df['submit_time'].max(),\n",
    "            'num_jobs': len(split_df),\n",
    "            'days': (split_df['submit_time'].max() - split_df['submit_time'].min()).days,\n",
    "            'jobs_per_day': len(split_df) / (split_df['submit_time'].max() - split_df['submit_time'].min()).days,\n",
    "            'mean_runtime': split_df['run_time'].mean(),\n",
    "            'mean_power': split_df['mean_node_power'].mean(),\n",
    "            'weekday_dist': split_df['submit_time'].dt.day_name().value_counts().to_dict(),\n",
    "        }\n",
    "        \n",
    "        # Resource utilization\n",
    "        split_stats[name]['resource_util'] = {\n",
    "            'mean_cores': split_df['num_cores_alloc'].mean(),\n",
    "            'max_cores': split_df['num_cores_alloc'].max(),\n",
    "            'mean_nodes': split_df['num_nodes_alloc'].mean(),\n",
    "            'max_nodes': split_df['num_nodes_alloc'].max()\n",
    "        }\n",
    "        \n",
    "        # Job type distribution (using runtime quintiles)\n",
    "        split_stats[name]['job_types'] = pd.qcut(split_df['run_time'], \n",
    "                                                q=5, \n",
    "                                                labels=['very_short', 'short', 'medium', 'long', 'very_long']\n",
    "                                               ).value_counts().to_dict()\n",
    "        \n",
    "    # Check for anomalous days in each split\n",
    "    def detect_anomalies(split_df, threshold=3):\n",
    "        daily_jobs = split_df.groupby(split_df['submit_time'].dt.date)['job_id'].count()\n",
    "        z_scores = np.abs(stats.zscore(daily_jobs))\n",
    "        return daily_jobs[z_scores > threshold].index.tolist()\n",
    "    \n",
    "    for name, split_df in splits.items():\n",
    "        split_stats[name]['anomalous_days'] = detect_anomalies(split_df)\n",
    "    \n",
    "    # Calculate KS-test p-values between splits\n",
    "    ks_tests = {}\n",
    "    for metric in ['run_time', 'mean_node_power']:\n",
    "        ks_tests[metric] = {\n",
    "            'train_val': stats.ks_2samp(splits['training'][metric], \n",
    "                                      splits['validation'][metric]).pvalue,\n",
    "            'train_test': stats.ks_2samp(splits['training'][metric], \n",
    "                                       splits['testing'][metric]).pvalue,\n",
    "            'val_test': stats.ks_2samp(splits['validation'][metric], \n",
    "                                     splits['testing'][metric]).pvalue\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'split_dates': {\n",
    "            'first_monday': first_monday,\n",
    "            'training_end': training_end,\n",
    "            'validation_end': validation_end,\n",
    "            'final_date': end_date\n",
    "        },\n",
    "        'split_stats': split_stats,\n",
    "        'distribution_tests': ks_tests\n",
    "    }\n",
    "\n",
    "# Generate and print analysis\n",
    "results = analyze_split_points(df)\n",
    "\n",
    "print(\"=== Split Dates ===\")\n",
    "for name, date in results['split_dates'].items():\n",
    "    print(f\"{name}: {date}\")\n",
    "\n",
    "print(\"\\n=== Split Statistics ===\")\n",
    "for split_name, stats in results['split_stats'].items():\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"Number of jobs: {stats['num_jobs']}\")\n",
    "    print(f\"Jobs per day: {stats['jobs_per_day']:.2f}\")\n",
    "    print(f\"Mean runtime: {stats['mean_runtime']:.2f}\")\n",
    "    print(f\"Mean power: {stats['mean_power']:.2f}\")\n",
    "    print(f\"Anomalous days: {len(stats['anomalous_days'])}\")\n",
    "\n",
    "print(\"\\n=== Distribution Similarity (KS-test p-values) ===\")\n",
    "for metric, tests in results['distribution_tests'].items():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    for comparison, pvalue in tests.items():\n",
    "        print(f\"{comparison}: {pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m splits, split_stats, ks_tests\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Generate and analyze splits\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m splits, stats, distribution_tests \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_stratified_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Print analysis\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Split Statistics ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 80\u001b[0m, in \u001b[0;36mcreate_stratified_split\u001b[0;34m(df, chunk_size_days, seed)\u001b[0m\n\u001b[1;32m     63\u001b[0m split_stats[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(split_df),\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msorted\u001b[39m(split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     }\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Calculate runtime percentiles for job categorization\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m runtime_percentiles \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Categorize jobs based on runtime\u001b[39;00m\n\u001b[1;32m     83\u001b[0m conditions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     84\u001b[0m     (split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m runtime_percentiles[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     85\u001b[0m     (split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m runtime_percentiles[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m&\u001b[39m (split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m runtime_percentiles[\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     (split_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m runtime_percentiles[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     89\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4287\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, weights, interpolation)\u001b[0m\n\u001b[1;32m   4284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(weights \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   4285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights must be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4288\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4676\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, weights)\u001b[0m\n\u001b[1;32m   4667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[1;32m   4668\u001b[0m                         q,\n\u001b[1;32m   4669\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4673\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   4674\u001b[0m                         weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4675\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4677\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4678\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4679\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4680\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4681\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4682\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4683\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4684\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3764\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3761\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[1;32m   3762\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[0;32m-> 3764\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4853\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, weights, axis, out, overwrite_input, method)\u001b[0m\n\u001b[1;32m   4851\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   4852\u001b[0m         wgt \u001b[38;5;241m=\u001b[39m weights\n\u001b[0;32m-> 4853\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4856\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4858\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4975\u001b[0m, in \u001b[0;36m_quantile\u001b[0;34m(arr, quantiles, axis, method, out, weights)\u001b[0m\n\u001b[1;32m   4968\u001b[0m arr\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[1;32m   4969\u001b[0m     np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate(([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   4970\u001b[0m                               previous_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[1;32m   4971\u001b[0m                               next_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[1;32m   4972\u001b[0m                               ))),\n\u001b[1;32m   4973\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   4974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supports_nans:\n\u001b[0;32m-> 4975\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   4976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4977\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "def create_stratified_split(df, chunk_size_days=14, seed=42):\n",
    "    \"\"\"\n",
    "    Create stratified temporal split with 2-week chunks distributed across splits\n",
    "    while maintaining temporal continuity within chunks.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Sort by submit time\n",
    "    df = df.sort_values('submit_time')\n",
    "    \n",
    "    # Create chunks\n",
    "    start_date = df['submit_time'].min()\n",
    "    df['chunk_id'] = ((df['submit_time'] - start_date).dt.total_seconds() \n",
    "                      / (chunk_size_days * 24 * 3600)).astype(int)\n",
    "    \n",
    "    # Get unique chunk IDs and create a DataFrame for chunk management\n",
    "    chunks_df = pd.DataFrame({'chunk_id': sorted(df['chunk_id'].unique())})\n",
    "    chunks_df['position'] = pd.qcut(chunks_df.index, q=3, labels=['early', 'middle', 'late'])\n",
    "    \n",
    "    # Initialize split assignments\n",
    "    train_chunks = []\n",
    "    val_chunks = []\n",
    "    test_chunks = []\n",
    "    \n",
    "    # Distribute chunks across splits while maintaining temporal representation\n",
    "    for period in ['early', 'middle', 'late']:\n",
    "        period_chunks = chunks_df[chunks_df['position'] == period]['chunk_id'].values\n",
    "        n_period_chunks = len(period_chunks)\n",
    "        \n",
    "        # Calculate split sizes for this period\n",
    "        n_period_train = int(n_period_chunks * 0.6)\n",
    "        n_period_val = int(n_period_chunks * 0.2)\n",
    "        \n",
    "        # Randomly assign chunks to splits\n",
    "        period_chunk_indices = np.random.permutation(n_period_chunks)\n",
    "        \n",
    "        train_chunks.extend(period_chunks[period_chunk_indices[:n_period_train]])\n",
    "        val_chunks.extend(period_chunks[period_chunk_indices[n_period_train:n_period_train+n_period_val]])\n",
    "        test_chunks.extend(period_chunks[period_chunk_indices[n_period_train+n_period_val:]])\n",
    "    \n",
    "    # Create masks for each split\n",
    "    train_mask = df['chunk_id'].isin(train_chunks)\n",
    "    val_mask = df['chunk_id'].isin(val_chunks)\n",
    "    test_mask = df['chunk_id'].isin(test_chunks)\n",
    "    \n",
    "    # Create split DataFrames\n",
    "    splits = {\n",
    "        'training': df[train_mask].copy(),\n",
    "        'validation': df[val_mask].copy(),\n",
    "        'testing': df[test_mask].copy()\n",
    "    }\n",
    "    \n",
    "    split_stats = {}\n",
    "    for name, split_df in splits.items():\n",
    "        # Calculate basic statistics\n",
    "        total_days = (split_df['submit_time'].max() - split_df['submit_time'].min()).total_seconds() / (24 * 3600)\n",
    "        \n",
    "        split_stats[name] = {\n",
    "            'size': len(split_df),\n",
    "            'chunks': sorted(split_df['chunk_id'].unique()),\n",
    "            'date_range': (split_df['submit_time'].min(), split_df['submit_time'].max()),\n",
    "            'mean_runtime': split_df['run_time'].mean(),\n",
    "            'std_runtime': split_df['run_time'].std(),\n",
    "            'mean_power': split_df['mean_node_power'].mean(),\n",
    "            'std_power': split_df['mean_node_power'].std(),\n",
    "            'jobs_per_day': len(split_df) / total_days if total_days > 0 else len(split_df),\n",
    "            'resource_stats': {\n",
    "                'mean_cores': split_df['num_cores_alloc'].mean(),\n",
    "                'mean_nodes': split_df['num_nodes_alloc'].mean(),\n",
    "                'total_core_hours': (split_df['num_cores_alloc'] * split_df['run_time']).sum() / 3600\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Calculate runtime percentiles for job categorization\n",
    "        runtime_percentiles = np.percentile(split_df['run_time'], [20, 40, 60, 80])\n",
    "        \n",
    "        # Categorize jobs based on runtime\n",
    "        conditions = [\n",
    "            (split_df['run_time'] <= runtime_percentiles[0]),\n",
    "            (split_df['run_time'] > runtime_percentiles[0]) & (split_df['run_time'] <= runtime_percentiles[1]),\n",
    "            (split_df['run_time'] > runtime_percentiles[1]) & (split_df['run_time'] <= runtime_percentiles[2]),\n",
    "            (split_df['run_time'] > runtime_percentiles[2]) & (split_df['run_time'] <= runtime_percentiles[3]),\n",
    "            (split_df['run_time'] > runtime_percentiles[3])\n",
    "        ]\n",
    "        choices = ['very_short', 'short', 'medium', 'long', 'very_long']\n",
    "        job_categories = np.select(conditions, choices, default='medium')\n",
    "        \n",
    "        split_stats[name]['job_types'] = pd.Series(job_categories).value_counts(normalize=True).to_dict()\n",
    "    \n",
    "    # Calculate distribution similarity between splits\n",
    "    ks_tests = {}\n",
    "    for metric in ['run_time', 'mean_node_power']:\n",
    "        ks_tests[metric] = {\n",
    "            'train_val': stats.ks_2samp(splits['training'][metric], \n",
    "                                      splits['validation'][metric]).pvalue,\n",
    "            'train_test': stats.ks_2samp(splits['training'][metric], \n",
    "                                       splits['testing'][metric]).pvalue,\n",
    "            'val_test': stats.ks_2samp(splits['validation'][metric], \n",
    "                                     splits['testing'][metric]).pvalue\n",
    "        }\n",
    "    \n",
    "    return splits, split_stats, ks_tests\n",
    "\n",
    "# Generate and analyze splits\n",
    "splits, stats, distribution_tests = create_stratified_split(df)\n",
    "\n",
    "# Print analysis\n",
    "print(\"=== Split Statistics ===\")\n",
    "for split_name, split_stats in stats.items():\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"Size: {split_stats['size']} jobs\")\n",
    "    print(f\"Date range: {split_stats['date_range'][0]} to {split_stats['date_range'][1]}\")\n",
    "    print(f\"Jobs per day: {split_stats['jobs_per_day']:.2f}\")\n",
    "    print(f\"Mean runtime: {split_stats['mean_runtime']:.2f} (std: {split_stats['std_runtime']:.2f})\")\n",
    "    print(f\"Mean power: {split_stats['mean_power']:.2f} (std: {split_stats['std_power']:.2f})\")\n",
    "    print(f\"Total core hours: {split_stats['resource_stats']['total_core_hours']:.2f}\")\n",
    "    print(\"\\nJob type distribution:\")\n",
    "    for job_type, proportion in split_stats['job_types'].items():\n",
    "        print(f\"  {job_type}: {proportion:.3f}\")\n",
    "\n",
    "print(\"\\n=== Distribution Similarity (KS-test p-values) ===\")\n",
    "for metric, tests in distribution_tests.items():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    for comparison, pvalue in tests.items():\n",
    "        print(f\"{comparison}: {pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
