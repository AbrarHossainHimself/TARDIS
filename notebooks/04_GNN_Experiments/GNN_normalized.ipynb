{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989129/84532322.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(file_path, parse_dates=time_columns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '/home/abrar/Desktop/Code/Temporal HPC/normalized_data.csv'\n",
    "\n",
    "# Read CSV and ensure datetime columns are parsed correctly\n",
    "time_columns = ['submit_time', 'eligible_time', 'start_time', 'end_time', 'wait_time']\n",
    "df = pd.read_csv(file_path, parse_dates=time_columns)\n",
    "# Now 'df' contains the data from the second sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour of day distribution:\n",
      "hour_of_day\n",
      "0      4680\n",
      "1      3880\n",
      "2      3645\n",
      "3      3653\n",
      "4      3834\n",
      "5      4312\n",
      "6      6420\n",
      "7     11364\n",
      "8     12706\n",
      "9     11611\n",
      "10    10179\n",
      "11    11690\n",
      "12     8799\n",
      "13    13013\n",
      "14    14091\n",
      "15     9886\n",
      "16     8403\n",
      "17     8840\n",
      "18     4967\n",
      "19     7375\n",
      "20     6316\n",
      "21     6719\n",
      "22     7563\n",
      "23     7369\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Day of week distribution:\n",
      "day_of_week\n",
      "Tuesday      34127\n",
      "Wednesday    33466\n",
      "Friday       32642\n",
      "Thursday     31801\n",
      "Monday       24452\n",
      "Saturday     21010\n",
      "Sunday       13817\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Monthly distribution:\n",
      "month\n",
      "5     19376\n",
      "6     33923\n",
      "7     33178\n",
      "8     29692\n",
      "9     33887\n",
      "10    41259\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Temporal distribution analysis\n",
    "# Convert submit_time to hour of day and day of week\n",
    "df['hour_of_day'] = df['submit_time'].dt.hour\n",
    "df['day_of_week'] = df['submit_time'].dt.day_name()\n",
    "df['month'] = df['submit_time'].dt.month\n",
    "\n",
    "# Get distributions\n",
    "print(\"Hour of day distribution:\")\n",
    "print(df['hour_of_day'].value_counts().sort_index())\n",
    "print(\"\\nDay of week distribution:\")\n",
    "print(df['day_of_week'].value_counts())\n",
    "print(\"\\nMonthly distribution:\")\n",
    "print(df['month'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job size statistics:\n",
      "       num_cores_alloc  num_nodes_alloc  num_gpus_alloc      mem_alloc  \\\n",
      "count    191315.000000    191315.000000   191315.000000  191315.000000   \n",
      "mean        107.315872         1.173761        4.127314     213.540993   \n",
      "std         106.953631         0.732607        3.212431     190.284860   \n",
      "min           4.000000         1.000000        0.000000       0.000000   \n",
      "25%          32.000000         1.000000        4.000000     118.000000   \n",
      "50%         128.000000         1.000000        4.000000     237.000000   \n",
      "75%         128.000000         1.000000        4.000000     237.000000   \n",
      "max       20736.000000       162.000000      648.000000   38475.000000   \n",
      "\n",
      "            run_time  \n",
      "count  191315.000000  \n",
      "mean     5767.922732  \n",
      "std      3778.020112  \n",
      "min         1.000000  \n",
      "25%      2858.890321  \n",
      "50%      5581.545522  \n",
      "75%      8422.589206  \n",
      "max     23330.168557  \n",
      "\n",
      "Power usage statistics:\n",
      "       mean_cpu_power  mean_mem_power  mean_node_power\n",
      "count   191315.000000   191315.000000    191315.000000\n",
      "mean       142.347877       44.559368       840.158333\n",
      "std        118.132777       25.982101       183.097552\n",
      "min         22.666667       28.000000        44.778073\n",
      "25%         78.000000       36.000000       708.600413\n",
      "50%        102.780488       37.157895       840.644989\n",
      "75%        174.011173       40.428571       971.320238\n",
      "max      21683.333333     5848.000000      1635.693883\n"
     ]
    }
   ],
   "source": [
    "# Job size and duration statistics\n",
    "print(\"\\nJob size statistics:\")\n",
    "print(df[['num_cores_alloc', 'num_nodes_alloc', 'num_gpus_alloc', 'mem_alloc', 'run_time']].describe())\n",
    "\n",
    "# Power usage statistics\n",
    "print(\"\\nPower usage statistics:\")\n",
    "print(df[['mean_cpu_power', 'mean_mem_power', 'mean_node_power']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation matrix:\n",
      "                 cores_per_task  num_tasks  num_cores_alloc  num_nodes_alloc  \\\n",
      "cores_per_task         1.000000  -0.259773         0.268369         0.048834   \n",
      "num_tasks             -0.259773   1.000000         0.147112         0.238280   \n",
      "num_cores_alloc        0.268369   0.147112         1.000000         0.838456   \n",
      "num_nodes_alloc        0.048834   0.238280         0.838456         1.000000   \n",
      "num_gpus_alloc         0.072371   0.128644         0.818108         0.883935   \n",
      "mem_alloc              0.189469   0.145238         0.903711         0.862834   \n",
      "mean_cpu_power         0.055080   0.346748         0.639818         0.672793   \n",
      "mean_mem_power         0.049872   0.281220         0.781548         0.923168   \n",
      "mean_node_power        0.657201   0.011314         0.669077         0.402031   \n",
      "run_time               0.610215   0.028178         0.670590         0.436848   \n",
      "\n",
      "                 num_gpus_alloc  mem_alloc  mean_cpu_power  mean_mem_power  \\\n",
      "cores_per_task         0.072371   0.189469        0.055080        0.049872   \n",
      "num_tasks              0.128644   0.145238        0.346748        0.281220   \n",
      "num_cores_alloc        0.818108   0.903711        0.639818        0.781548   \n",
      "num_nodes_alloc        0.883935   0.862834        0.672793        0.923168   \n",
      "num_gpus_alloc         1.000000   0.819673        0.580790        0.809477   \n",
      "mem_alloc              0.819673   1.000000        0.600435        0.795476   \n",
      "mean_cpu_power         0.580790   0.600435        1.000000        0.755235   \n",
      "mean_mem_power         0.809477   0.795476        0.755235        1.000000   \n",
      "mean_node_power        0.503112   0.579767        0.394570        0.431393   \n",
      "run_time               0.534207   0.619414        0.410196        0.465482   \n",
      "\n",
      "                 mean_node_power  run_time  \n",
      "cores_per_task          0.657201  0.610215  \n",
      "num_tasks               0.011314  0.028178  \n",
      "num_cores_alloc         0.669077  0.670590  \n",
      "num_nodes_alloc         0.402031  0.436848  \n",
      "num_gpus_alloc          0.503112  0.534207  \n",
      "mem_alloc               0.579767  0.619414  \n",
      "mean_cpu_power          0.394570  0.410196  \n",
      "mean_mem_power          0.431393  0.465482  \n",
      "mean_node_power         1.000000  0.961768  \n",
      "run_time                0.961768  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations between numerical columns\n",
    "corr_cols = ['cores_per_task', 'num_tasks', 'num_cores_alloc', 'num_nodes_alloc', \n",
    "             'num_gpus_alloc', 'mem_alloc', 'mean_cpu_power', 'mean_mem_power', \n",
    "             'mean_node_power', 'run_time']\n",
    "correlation_matrix = df[corr_cols].corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strata: 90\n",
      "\n",
      "Sample sizes per stratum:\n",
      "Min: 1\n",
      "Max: 14242\n",
      "Mean: 2125.722222222222\n",
      "Median: 325.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For hour groups\n",
    "df['hour_group'] = pd.cut(df['hour_of_day'], \n",
    "                         bins=[0, 6, 12, 18, 24], \n",
    "                         labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "# For core allocation, use custom bins based on the distribution we saw\n",
    "df['size_group'] = pd.cut(df['num_cores_alloc'], \n",
    "                         bins=[0, 32, 64, 128, 256, float('inf')],\n",
    "                         labels=['very_small', 'small', 'medium', 'large', 'very_large'])\n",
    "\n",
    "# For runtime, use custom bins based on the distribution\n",
    "df['runtime_group'] = pd.cut(df['run_time'],\n",
    "                            bins=[0, 2000, 4000, 6000, 8000, float('inf')],\n",
    "                            labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n",
    "\n",
    "# Create combined stratification feature\n",
    "df['strata'] = df['hour_group'].astype(str) + '_' + \\\n",
    "               df['size_group'].astype(str) + '_' + \\\n",
    "               df['runtime_group'].astype(str)\n",
    "\n",
    "# Check strata distribution\n",
    "strata_counts = df['strata'].value_counts()\n",
    "print(\"Number of strata:\", len(strata_counts))\n",
    "print(\"\\nSample sizes per stratum:\")\n",
    "print(\"Min:\", strata_counts.min())\n",
    "print(\"Max:\", strata_counts.max())\n",
    "print(\"Mean:\", strata_counts.mean())\n",
    "print(\"Median:\", strata_counts.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original samples: 191315\n",
      "Filtered samples: 191207\n",
      "Percentage retained: 99.94354859786216 %\n",
      "\n",
      "New strata statistics:\n",
      "Number of strata: 78\n",
      "Min samples per stratum: 20\n",
      "Max samples per stratum: 14242\n"
     ]
    }
   ],
   "source": [
    "# First, let's remove tiny strata that could cause issues\n",
    "# Keep only strata with at least 20 samples to ensure meaningful splits\n",
    "valid_strata = strata_counts[strata_counts >= 20].index\n",
    "df_filtered = df[df['strata'].isin(valid_strata)].copy()\n",
    "\n",
    "# Check how many samples we retained\n",
    "print(\"Original samples:\", len(df))\n",
    "print(\"Filtered samples:\", len(df_filtered))\n",
    "print(\"Percentage retained:\", (len(df_filtered)/len(df))*100, \"%\")\n",
    "\n",
    "# Check new strata distribution\n",
    "new_strata_counts = df_filtered['strata'].value_counts()\n",
    "print(\"\\nNew strata statistics:\")\n",
    "print(\"Number of strata:\", len(new_strata_counts))\n",
    "print(\"Min samples per stratum:\", new_strata_counts.min())\n",
    "print(\"Max samples per stratum:\", new_strata_counts.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Split for GNN Training (Power Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Training set size: 152984\n",
      "GNN Validation set size: 19102\n",
      "GNN Test set size: 19121\n",
      "\n",
      "Train set statistics:\n",
      "Mean CPU power: 142.37\n",
      "Mean node power: 840.20\n",
      "Average cores: 107.41\n",
      "\n",
      "Val set statistics:\n",
      "Mean CPU power: 141.78\n",
      "Mean node power: 840.45\n",
      "Average cores: 107.05\n",
      "\n",
      "Test set statistics:\n",
      "Mean CPU power: 142.78\n",
      "Mean node power: 839.84\n",
      "Average cores: 106.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate out the final test set (10%)\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    df_filtered.index,\n",
    "    test_size=0.1,\n",
    "    stratify=df_filtered['strata'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Then split remaining data into train (80% of total) and validation (10% of total)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx,\n",
    "    test_size=0.111,  # 0.111 of 90% is 10% of total\n",
    "    stratify=df_filtered.loc[train_val_idx, 'strata'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create the datasets\n",
    "gnn_train_data = df_filtered.loc[train_idx]\n",
    "gnn_val_data = df_filtered.loc[val_idx]\n",
    "gnn_test_data = df_filtered.loc[test_idx]\n",
    "\n",
    "# Print split sizes\n",
    "print(\"GNN Training set size:\", len(gnn_train_data))\n",
    "print(\"GNN Validation set size:\", len(gnn_val_data))\n",
    "print(\"GNN Test set size:\", len(gnn_test_data))\n",
    "\n",
    "# Verify distribution preservation\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    print(f\"\\n{split_name} set statistics:\")\n",
    "    print(f\"Mean CPU power: {split_data['mean_cpu_power'].mean():.2f}\")\n",
    "    print(f\"Mean node power: {split_data['mean_node_power'].mean():.2f}\")\n",
    "    print(f\"Average cores: {split_data['num_cores_alloc'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set:\n",
      "Time span: 160 days\n",
      "Average jobs per day: 954.19\n",
      "\n",
      "Val set:\n",
      "Time span: 159 days\n",
      "Average jobs per day: 119.65\n",
      "\n",
      "Test set:\n",
      "Time span: 159 days\n",
      "Average jobs per day: 119.78\n"
     ]
    }
   ],
   "source": [
    "# Sort data by submit_time within each split\n",
    "gnn_train_data = gnn_train_data.sort_values('submit_time')\n",
    "gnn_val_data = gnn_val_data.sort_values('submit_time')\n",
    "gnn_test_data = gnn_test_data.sort_values('submit_time')\n",
    "\n",
    "# Create temporal sequences for RL\n",
    "# Let's first analyze the time spans\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    time_span = split_data['submit_time'].max() - split_data['submit_time'].min()\n",
    "    jobs_per_day = len(split_data) / (time_span.total_seconds() / (24*3600))\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"Time span: {time_span.days} days\")\n",
    "    print(f\"Average jobs per day: {jobs_per_day:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the RL episodes. For HPC scheduling, we want episodes that:\n",
    "\n",
    "- Are long enough to capture meaningful scheduling decisions\n",
    "- Short enough to provide many training episodes\n",
    "- Account for daily power price variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train episodes:\n",
      "Number of episodes: 155\n",
      "Average jobs per episode: 986.99\n",
      "Min jobs per episode: 9\n",
      "Max jobs per episode: 7888\n",
      "\n",
      "Val episodes:\n",
      "Number of episodes: 154\n",
      "Average jobs per episode: 124.04\n",
      "Min jobs per episode: 1\n",
      "Max jobs per episode: 1119\n",
      "\n",
      "Test episodes:\n",
      "Number of episodes: 154\n",
      "Average jobs per episode: 124.16\n",
      "Min jobs per episode: 1\n",
      "Max jobs per episode: 1115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create episodes of 24 hours each (to capture full day cycles)\n",
    "def create_episodes(df, episode_duration_hours=24):\n",
    "    start_time = df['submit_time'].min()\n",
    "    end_time = df['submit_time'].max()\n",
    "    \n",
    "    episodes = []\n",
    "    current_time = start_time\n",
    "    \n",
    "    while current_time < end_time:\n",
    "        episode_end = current_time + pd.Timedelta(hours=episode_duration_hours)\n",
    "        \n",
    "        # Get jobs submitted during this episode\n",
    "        episode_jobs = df[\n",
    "            (df['submit_time'] >= current_time) & \n",
    "            (df['submit_time'] < episode_end)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(episode_jobs) > 0:\n",
    "            episodes.append(episode_jobs)\n",
    "            \n",
    "        current_time = episode_end\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Create episodes for each split\n",
    "train_episodes = create_episodes(gnn_train_data)\n",
    "val_episodes = create_episodes(gnn_val_data)\n",
    "test_episodes = create_episodes(gnn_test_data)\n",
    "\n",
    "# Print episode statistics\n",
    "for split_name, split_episodes in [(\"Train\", train_episodes), \n",
    "                                 (\"Val\", val_episodes), \n",
    "                                 (\"Test\", test_episodes)]:\n",
    "    num_episodes = len(split_episodes)\n",
    "    avg_jobs = np.mean([len(ep) for ep in split_episodes])\n",
    "    min_jobs = np.min([len(ep) for ep in split_episodes])\n",
    "    max_jobs = np.max([len(ep) for ep in split_episodes])\n",
    "    \n",
    "    print(f\"\\n{split_name} episodes:\")\n",
    "    print(f\"Number of episodes: {num_episodes}\")\n",
    "    print(f\"Average jobs per episode: {avg_jobs:.2f}\")\n",
    "    print(f\"Min jobs per episode: {min_jobs}\")\n",
    "    print(f\"Max jobs per episode: {max_jobs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN training structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing GNN data splits:\n",
      "\n",
      "Train set:\n",
      "Number of jobs: 152984\n",
      "Mean node power: 840.20\n",
      "Std node power: 182.99\n",
      "\n",
      "Val set:\n",
      "Number of jobs: 19102\n",
      "Mean node power: 840.45\n",
      "Std node power: 183.30\n",
      "\n",
      "Test set:\n",
      "Number of jobs: 19121\n",
      "Mean node power: 839.84\n",
      "Std node power: 184.18\n"
     ]
    }
   ],
   "source": [
    "# Features for GNN (selected based on correlations with mean_node_power)\n",
    "gnn_features = [\n",
    "    'cores_per_task',     # Strong correlation (0.657)\n",
    "    'num_cores_alloc',    # Strong correlation (0.669)\n",
    "    'num_nodes_alloc',    # Important for node-level prediction\n",
    "    'num_gpus_alloc',     # Relevant for power consumption\n",
    "    'mem_alloc',          # Moderate correlation (0.580)\n",
    "    'run_time'           # Strongest correlation (0.962)\n",
    "]\n",
    "\n",
    "# Single target variable\n",
    "power_target = ['mean_node_power']\n",
    "\n",
    "# Prepare the data structure for GNN\n",
    "def prepare_gnn_data(df, features=gnn_features, target=power_target):\n",
    "    \"\"\"\n",
    "    Prepare data for GNN training with KNN structure\n",
    "    \"\"\"\n",
    "    # Normalize features (important for KNN)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[features])\n",
    "    y = df[target].values\n",
    "    \n",
    "    # For each job, we'll find k similar jobs\n",
    "    k = 5  # Can be tuned\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 because it includes the point itself\n",
    "    knn.fit(X)\n",
    "    \n",
    "    # Get indices of k nearest neighbors for each job\n",
    "    distances, indices = knn.kneighbors(X)\n",
    "    \n",
    "    return X, y, indices[:, 1:], scaler  # Exclude self from neighbors\n",
    "\n",
    "print(\"Preparing GNN data splits:\")\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"Number of jobs: {len(split_data)}\")\n",
    "    print(f\"Mean node power: {split_data['mean_node_power'].mean():.2f}\")\n",
    "    print(f\"Std node power: {split_data['mean_node_power'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing GNN data splits:\n",
      "\n",
      "Train set:\n",
      "Number of jobs: 152984\n",
      "Mean node power: 840.20\n",
      "Std node power: 182.99\n",
      "\n",
      "Val set:\n",
      "Number of jobs: 19102\n",
      "Mean node power: 840.45\n",
      "Std node power: 183.30\n",
      "\n",
      "Test set:\n",
      "Number of jobs: 19121\n",
      "Mean node power: 839.84\n",
      "Std node power: 184.18\n"
     ]
    }
   ],
   "source": [
    "# Features for GNN (selected based on correlations with mean_node_power)\n",
    "gnn_features = [\n",
    "    'cores_per_task',     # Strong correlation (0.657)\n",
    "    'num_cores_alloc',    # Strong correlation (0.669)\n",
    "    'num_nodes_alloc',    # Important for node-level prediction\n",
    "    'num_gpus_alloc',     # Relevant for power consumption\n",
    "    'mem_alloc',          # Moderate correlation (0.580)\n",
    "    'run_time'           # Strongest correlation (0.962)\n",
    "]\n",
    "\n",
    "# Single target variable\n",
    "power_target = ['mean_node_power']\n",
    "\n",
    "# Prepare the data structure for GNN\n",
    "def prepare_gnn_data(df, features=gnn_features, target=power_target):\n",
    "    \"\"\"\n",
    "    Prepare data for GNN training with KNN structure\n",
    "    \"\"\"\n",
    "    # Normalize features (important for KNN)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[features])\n",
    "    y = df[target].values\n",
    "    \n",
    "    # For each job, we'll find k similar jobs\n",
    "    k = 5  # Can be tuned\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 because it includes the point itself\n",
    "    knn.fit(X)\n",
    "    \n",
    "    # Get indices of k nearest neighbors for each job\n",
    "    distances, indices = knn.kneighbors(X)\n",
    "    \n",
    "    return X, y, indices[:, 1:], scaler  # Exclude self from neighbors\n",
    "\n",
    "print(\"Preparing GNN data splits:\")\n",
    "for split_name, split_data in [(\"Train\", gnn_train_data), \n",
    "                              (\"Val\", gnn_val_data), \n",
    "                              (\"Test\", gnn_test_data)]:\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"Number of jobs: {len(split_data)}\")\n",
    "    print(f\"Mean node power: {split_data['mean_node_power'].mean():.2f}\")\n",
    "    print(f\"Std node power: {split_data['mean_node_power'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs in training: 152984\n",
      "Total number of jobs in validation: 19102\n",
      "Total number of jobs in test: 19121\n",
      "\n",
      "Preparing data...\n",
      "Calculating neighbors for training set...\n",
      "Calculating neighbors for validation set...\n",
      "Calculating neighbors for test set...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 104\u001b[0m\n\u001b[1;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m PowerPredictionGNN(input_dim\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPreparing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m train_graphs, val_graphs, test_graphs, scaler, train_X, val_X, test_X \u001b[38;5;241m=\u001b[39m prepare_training_data()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Print graph statistics\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGraph statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 5)"
     ]
    }
   ],
   "source": [
    "class PowerPredictionGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super(PowerPredictionGNN, self).__init__()\n",
    "        \n",
    "        # Wider network\n",
    "        self.node_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Deeper graph convolutions\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # More robust prediction head\n",
    "        self.power_pred = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Initial embeddings\n",
    "        x = self.node_embed(x)\n",
    "        \n",
    "        # Graph convolutions with residual connections\n",
    "        x1 = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1, edge_index))) + x1\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2, edge_index))) + x2\n",
    "        \n",
    "        # Power prediction\n",
    "        return self.power_pred(x3)\n",
    "    \n",
    "    \n",
    "def create_graph_batch(features, neighbor_indices, targets, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Create graph batches ensuring consistent shapes between features and targets\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    num_samples = len(features)\n",
    "    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, num_samples)\n",
    "        batch_features = features[i:batch_end]\n",
    "        batch_neighbors = neighbor_indices[i:batch_end]\n",
    "        batch_targets = targets[i:batch_end]\n",
    "        \n",
    "        # Create edges based on KNN within this batch\n",
    "        edge_index = []\n",
    "        for job_idx, neighbors in enumerate(batch_neighbors):\n",
    "            # Only add edges for neighbors that are within this batch\n",
    "            valid_neighbors = [n - i for n in neighbors if i <= n < batch_end]\n",
    "            for neighbor in valid_neighbors:\n",
    "                edge_index.append([job_idx, neighbor])\n",
    "                edge_index.append([neighbor, job_idx])\n",
    "        \n",
    "        if not edge_index:  # If no valid edges in this batch\n",
    "            continue\n",
    "            \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        \n",
    "        # Create graph with matching features and targets\n",
    "        graph = Data(\n",
    "            x=torch.FloatTensor(batch_features),\n",
    "            edge_index=edge_index,\n",
    "            y=torch.FloatTensor(batch_targets).unsqueeze(-1)  # Add dimension for consistency\n",
    "        )\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "# Modified training configuration\n",
    "config = {\n",
    "    'hidden_dim': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 100,\n",
    "    'early_stopping_patience': 15\n",
    "}\n",
    "\n",
    "# Print some statistics about the data\n",
    "print(f\"Total number of jobs in training: {len(gnn_train_data)}\")\n",
    "print(f\"Total number of jobs in validation: {len(gnn_val_data)}\")\n",
    "print(f\"Total number of jobs in test: {len(gnn_test_data)}\")\n",
    "\n",
    "# Initialize model and start training\n",
    "input_dim = len(gnn_features)\n",
    "model = PowerPredictionGNN(input_dim=input_dim)\n",
    "\n",
    "\n",
    "print(\"\\nPreparing data...\")\n",
    "train_graphs, val_graphs, test_graphs, scaler,  *_ = prepare_training_data()\n",
    "\n",
    "# Print graph statistics\n",
    "print(\"\\nGraph statistics:\")\n",
    "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
    "print(f\"Number of validation graphs: {len(val_graphs)}\")\n",
    "print(f\"Number of test graphs: {len(test_graphs)}\")\n",
    "\n",
    "print(\"\\nSample graph details:\")\n",
    "if train_graphs:\n",
    "    g = train_graphs[0]\n",
    "    print(f\"First training graph - Nodes: {g.x.size(0)}, Edges: {g.edge_index.size(1)//2}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, val_losses = train_gnn(model, train_graphs, val_graphs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_prediction_loss(pred_power, true_power):\n",
    "    \"\"\"\n",
    "    Custom loss function ensuring shape consistency\n",
    "    \"\"\"\n",
    "    # Ensure shapes match\n",
    "    pred_power = pred_power.squeeze()\n",
    "    true_power = true_power.squeeze()\n",
    "    \n",
    "    # Check shapes are identical\n",
    "    assert pred_power.shape == true_power.shape, f\"Shape mismatch: pred={pred_power.shape}, true={true_power.shape}\"\n",
    "    \n",
    "    # Compute normalized MSE\n",
    "    pred_norm = (pred_power - pred_power.mean()) / (pred_power.std() + 1e-8)\n",
    "    true_norm = (true_power - true_power.mean()) / (true_power.std() + 1e-8)\n",
    "    \n",
    "    mse_loss = F.mse_loss(pred_norm, true_norm)\n",
    "    \n",
    "    # Compute relative error\n",
    "    relative_error = torch.abs(pred_power - true_power) / (torch.abs(true_power) + 1e-8)\n",
    "    relative_loss = torch.mean(relative_error)\n",
    "    \n",
    "    return mse_loss + 0.1 * relative_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rl_state(episode_df, gnn_model, scaler):\n",
    "    \"\"\"\n",
    "    Prepare RL state with GNN power predictions\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    features_normalized = scaler.transform(episode_df[gnn_features])\n",
    "    \n",
    "    # Get power predictions from GNN\n",
    "    with torch.no_grad():\n",
    "        power_pred = gnn_model(features_normalized)\n",
    "    \n",
    "    # Create state dictionary\n",
    "    state = {\n",
    "        'job_features': episode_df[gnn_features].values,\n",
    "        'predicted_power': power_pred.numpy(),\n",
    "        'submit_time': episode_df['submit_time'].values,\n",
    "        'time_of_day': episode_df['submit_time'].dt.hour.values\n",
    "    }\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_gnn(model, train_graphs, val_graphs, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    early_stopping = EarlyStopping(patience=config['early_stopping_patience'])\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for graph in train_graphs:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(graph.x, graph.edge_index)\n",
    "            loss = power_prediction_loss(out, graph.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for graph in val_graphs:\n",
    "                out = model(graph.x, graph.edge_index)\n",
    "                val_loss += power_prediction_loss(out, graph.y).item()\n",
    "        \n",
    "        train_loss /= len(train_graphs)\n",
    "        val_loss /= len(val_graphs)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_predictions(model, test_graphs, power_scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for graph in test_graphs:\n",
    "            pred = model(graph.x, graph.edge_index)\n",
    "            # Ensure 2D shape before inverse transform\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            true = graph.y.squeeze().numpy().reshape(-1, 1)\n",
    "            \n",
    "            # Inverse transform\n",
    "            pred = power_scaler.inverse_transform(pred)\n",
    "            true = power_scaler.inverse_transform(true)\n",
    "            \n",
    "            predictions.extend(pred.flatten())\n",
    "            true_values.extend(true.flatten())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    \n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mean_relative_error': np.mean(np.abs(predictions - true_values) / (true_values + 1e-8)),\n",
    "        'mean_pred': np.mean(predictions),\n",
    "        'mean_true': np.mean(true_values),\n",
    "        'std_pred': np.std(predictions),\n",
    "        'std_true': np.std(true_values)\n",
    "    }\n",
    "    \n",
    "    # Add some percentile errors\n",
    "    for p in [25, 50, 75, 90]:\n",
    "        error_percentile = np.percentile(np.abs(predictions - true_values), p)\n",
    "        results[f'error_p{p}'] = error_percentile\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "PowerPredictionGNN(\n",
      "  (node_embed): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (power_pred): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Preparing data...\n",
      "Calculating neighbors for training set...\n",
      "Calculating neighbors for validation set...\n",
      "Calculating neighbors for test set...\n",
      "\n",
      "Model architecture:\n",
      "PowerPredictionGNN(\n",
      "  (node_embed): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv1): GCNConv(128, 128)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (power_pred): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Starting training...\n",
      "Epoch 001, Train Loss: 0.3346, Val Loss: 0.2103\n",
      "Epoch 002, Train Loss: 0.1936, Val Loss: 0.2106\n",
      "Epoch 003, Train Loss: 0.1674, Val Loss: 0.2331\n",
      "Epoch 004, Train Loss: 0.1433, Val Loss: 0.1890\n",
      "Epoch 005, Train Loss: 0.1384, Val Loss: 0.1680\n",
      "Epoch 006, Train Loss: 0.1300, Val Loss: 0.1347\n",
      "Epoch 007, Train Loss: 0.1300, Val Loss: 0.1130\n",
      "Epoch 008, Train Loss: 0.1223, Val Loss: 0.1653\n",
      "Epoch 009, Train Loss: 0.1185, Val Loss: 0.1510\n",
      "Epoch 010, Train Loss: 0.1125, Val Loss: 0.1595\n",
      "Epoch 011, Train Loss: 0.1201, Val Loss: 0.1620\n",
      "Epoch 012, Train Loss: 0.1106, Val Loss: 0.1301\n",
      "Epoch 013, Train Loss: 0.1212, Val Loss: 0.1049\n",
      "Epoch 014, Train Loss: 0.1144, Val Loss: 0.1121\n",
      "Epoch 015, Train Loss: 0.1096, Val Loss: 0.1227\n",
      "Epoch 016, Train Loss: 0.1009, Val Loss: 0.1107\n",
      "Epoch 017, Train Loss: 0.0973, Val Loss: 0.0920\n",
      "Epoch 018, Train Loss: 0.0905, Val Loss: 0.1065\n",
      "Epoch 019, Train Loss: 0.0865, Val Loss: 0.1477\n",
      "Epoch 020, Train Loss: 0.0851, Val Loss: 0.1081\n",
      "Epoch 021, Train Loss: 0.0842, Val Loss: 0.1162\n",
      "Epoch 022, Train Loss: 0.0830, Val Loss: 0.1055\n",
      "Epoch 023, Train Loss: 0.0910, Val Loss: 0.1049\n",
      "Epoch 024, Train Loss: 0.0857, Val Loss: 0.0862\n",
      "Epoch 025, Train Loss: 0.0882, Val Loss: 0.1938\n",
      "Epoch 026, Train Loss: 0.0835, Val Loss: 0.1262\n",
      "Epoch 027, Train Loss: 0.0842, Val Loss: 0.0980\n",
      "Epoch 028, Train Loss: 0.0751, Val Loss: 0.1362\n",
      "Epoch 029, Train Loss: 0.0741, Val Loss: 0.1155\n",
      "Epoch 030, Train Loss: 0.0718, Val Loss: 0.1130\n",
      "Epoch 031, Train Loss: 0.0729, Val Loss: 0.0973\n",
      "Epoch 032, Train Loss: 0.0723, Val Loss: 0.1098\n",
      "Epoch 033, Train Loss: 0.0704, Val Loss: 0.1264\n",
      "Epoch 034, Train Loss: 0.0690, Val Loss: 0.0882\n",
      "Epoch 035, Train Loss: 0.0730, Val Loss: 0.0936\n",
      "Epoch 036, Train Loss: 0.0691, Val Loss: 0.1216\n",
      "Epoch 037, Train Loss: 0.0661, Val Loss: 0.1100\n",
      "Epoch 038, Train Loss: 0.0626, Val Loss: 0.0778\n",
      "Epoch 039, Train Loss: 0.0646, Val Loss: 0.1386\n",
      "Epoch 040, Train Loss: 0.0578, Val Loss: 0.0748\n",
      "Epoch 041, Train Loss: 0.0590, Val Loss: 0.0995\n",
      "Epoch 042, Train Loss: 0.0650, Val Loss: 0.0875\n",
      "Epoch 043, Train Loss: 0.0669, Val Loss: 0.0815\n",
      "Epoch 044, Train Loss: 0.0644, Val Loss: 0.1201\n",
      "Epoch 045, Train Loss: 0.0573, Val Loss: 0.1095\n",
      "Epoch 046, Train Loss: 0.0707, Val Loss: 0.1541\n",
      "Epoch 047, Train Loss: 0.0620, Val Loss: 0.1419\n",
      "Epoch 048, Train Loss: 0.0586, Val Loss: 0.1424\n",
      "Epoch 049, Train Loss: 0.0540, Val Loss: 0.0926\n",
      "Epoch 050, Train Loss: 0.0523, Val Loss: 0.1431\n",
      "Epoch 051, Train Loss: 0.0536, Val Loss: 0.1497\n",
      "Epoch 052, Train Loss: 0.0507, Val Loss: 0.1285\n",
      "Epoch 053, Train Loss: 0.0510, Val Loss: 0.1780\n",
      "Epoch 054, Train Loss: 0.0483, Val Loss: 0.1378\n",
      "Epoch 055, Train Loss: 0.0496, Val Loss: 0.1688\n",
      "Early stopping triggered\n",
      "\n",
      "Evaluating model...\n",
      "mse: 1722.6917\n",
      "rmse: 41.5053\n",
      "mae: 28.5520\n",
      "r2: 0.9492\n",
      "mean_relative_error: 0.0386\n",
      "mean_pred: 833.9962\n",
      "mean_true: 839.8434\n",
      "std_pred: 154.1247\n",
      "std_true: 184.1739\n",
      "error_p25: 7.8291\n",
      "error_p50: 18.8044\n",
      "error_p75: 36.8706\n",
      "error_p90: 65.7590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_neighbors(features, k=5):\n",
    "    \"\"\"Calculate k-nearest neighbors for each job\"\"\"\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)  # +1 because it includes the point itself\n",
    "    knn.fit(features)\n",
    "    distances, indices = knn.kneighbors(features)\n",
    "    return indices[:, 1:]  # Exclude self from neighbors\n",
    "\n",
    "def prepare_training_data():\n",
    "    \"\"\"\n",
    "    Prepare training data with proper scaling\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    feature_scaler = StandardScaler()\n",
    "    train_features = feature_scaler.fit_transform(gnn_train_data[gnn_features])\n",
    "    val_features = feature_scaler.transform(gnn_val_data[gnn_features])\n",
    "    test_features = feature_scaler.transform(gnn_test_data[gnn_features])\n",
    "    \n",
    "    # Scale power values\n",
    "    power_scaler = StandardScaler()\n",
    "    train_target = power_scaler.fit_transform(gnn_train_data[['mean_node_power']])\n",
    "    val_target = power_scaler.transform(gnn_val_data[['mean_node_power']])\n",
    "    test_target = power_scaler.transform(gnn_test_data[['mean_node_power']])\n",
    "    \n",
    "    # Calculate neighbor indices\n",
    "    print(\"Calculating neighbors for training set...\")\n",
    "    train_neighbors = calculate_neighbors(train_features)\n",
    "    print(\"Calculating neighbors for validation set...\")\n",
    "    val_neighbors = calculate_neighbors(val_features)\n",
    "    print(\"Calculating neighbors for test set...\")\n",
    "    test_neighbors = calculate_neighbors(test_features)\n",
    "    \n",
    "    # Create graph batches\n",
    "    train_graphs = create_graph_batch(train_features, train_neighbors, train_target)\n",
    "    val_graphs = create_graph_batch(val_features, val_neighbors, val_target)\n",
    "    test_graphs = create_graph_batch(test_features, test_neighbors, test_target)\n",
    "    \n",
    "    return train_graphs, val_graphs, test_graphs, feature_scaler, power_scaler\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = len(gnn_features)  # Number of features\n",
    "model = PowerPredictionGNN(input_dim=input_dim)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Initialize and train\n",
    "print(\"\\nPreparing data...\")\n",
    "# Fix the unpacking to match the 5 return values\n",
    "train_graphs, val_graphs, test_graphs, feature_scaler, power_scaler = prepare_training_data()\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "# Initialize model with the new architecture\n",
    "model = PowerPredictionGNN(input_dim=len(gnn_features), hidden_dim=config['hidden_dim'])\n",
    "print(model)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, val_losses = train_gnn(model, train_graphs, val_graphs, config)\n",
    "\n",
    "print(\"\\nEvaluating model...\")\n",
    "# Pass both scalers to evaluation\n",
    "test_metrics = evaluate_predictions(model, test_graphs, power_scaler)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working GNN + RL Code to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989129/1740890553.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(file_path, parse_dates=time_columns)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 145\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graphs\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Prepare data (fit scalers on train, transform all)\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m train_X, train_y, train_neighbors, feature_scaler, power_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_gnn_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgnn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m val_X, val_y, val_neighbors, _, _ \u001b[38;5;241m=\u001b[39m prepare_gnn_data(gnn_val_data, gnn_features, power_target, feature_scaler, power_scaler)\n\u001b[1;32m    147\u001b[0m test_X, test_y, test_neighbors, _, _ \u001b[38;5;241m=\u001b[39m prepare_gnn_data(gnn_test_data, gnn_features, power_target, feature_scaler, power_scaler)\n",
      "Cell \u001b[0;32mIn[3], line 97\u001b[0m, in \u001b[0;36mprepare_gnn_data\u001b[0;34m(df, features, target, feature_scaler, power_scaler)\u001b[0m\n\u001b[1;32m     95\u001b[0m knn \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)  \u001b[38;5;66;03m# k=5 + 1 (itself)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m---> 97\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m neighbor_indices \u001b[38;5;241m=\u001b[39m indices[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, neighbor_indices, feature_scaler, power_scaler\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/neighbors/_base.py:923\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method\n\u001b[1;32m    922\u001b[0m         )\n\u001b[0;32m--> 923\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GNN\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "file_path = '/home/abrar/Desktop/Code/Temporal HPC/normalized_data.csv'\n",
    "\n",
    "# Load data\n",
    "time_columns = ['submit_time', 'eligible_time', 'start_time', 'end_time', 'wait_time']\n",
    "df = pd.read_csv(file_path, parse_dates=time_columns)\n",
    "\n",
    "# Feature Engineering (Simplified for GNN Focus)\n",
    "df['hour_of_day'] = df['submit_time'].dt.hour\n",
    "df['day_of_week'] = df['submit_time'].dt.day_name()\n",
    "df['month'] = df['submit_time'].dt.month\n",
    "\n",
    "# --- Data Analysis (Keep relevant parts) ---\n",
    "\n",
    "# print(\"Hour of day distribution:\")\n",
    "# print(df['hour_of_day'].value_counts().sort_index())\n",
    "# print(\"\\nDay of week distribution:\")\n",
    "# print(df['day_of_week'].value_counts())\n",
    "# print(\"\\nMonthly distribution:\")\n",
    "# print(df['month'].value_counts().sort_index())\n",
    "\n",
    "# print(\"\\nJob size statistics:\")\n",
    "# print(df[['num_cores_alloc', 'num_nodes_alloc', 'num_gpus_alloc', 'mem_alloc', 'run_time']].describe())\n",
    "\n",
    "# print(\"\\nPower usage statistics:\")\n",
    "# print(df[['mean_cpu_power', 'mean_mem_power', 'mean_node_power']].describe())\n",
    "\n",
    "corr_cols = ['cores_per_task', 'num_tasks', 'num_cores_alloc', 'num_nodes_alloc',\n",
    "             'num_gpus_alloc', 'mem_alloc', 'mean_cpu_power', 'mean_mem_power',\n",
    "             'mean_node_power', 'run_time']\n",
    "correlation_matrix = df[corr_cols].corr()\n",
    "# print(\"\\nCorrelation matrix:\")\n",
    "# print(correlation_matrix)\n",
    "\n",
    "# --- Stratified Splitting (Keep, but simplified) ---\n",
    "\n",
    "# Create simplified stratification based on core allocation and runtime\n",
    "df['size_group'] = pd.cut(df['num_cores_alloc'],\n",
    "                         bins=[0, 32, 64, 128, 256, float('inf')],\n",
    "                         labels=['very_small', 'small', 'medium', 'large', 'very_large'])\n",
    "df['runtime_group'] = pd.cut(df['run_time'],\n",
    "                            bins=[0, 2000, 4000, 6000, 8000, float('inf')],\n",
    "                            labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n",
    "df['strata'] = df['size_group'].astype(str) + '_' + df['runtime_group'].astype(str)\n",
    "\n",
    "# Remove small strata\n",
    "valid_strata = df['strata'].value_counts()[df['strata'].value_counts() >= 20].index\n",
    "df_filtered = df[df['strata'].isin(valid_strata)].copy()\n",
    "\n",
    "# Split data (stratified)\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    df_filtered.index, test_size=0.1, stratify=df_filtered['strata'], random_state=42\n",
    ")\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx, test_size=0.111, stratify=df_filtered.loc[train_val_idx, 'strata'], random_state=42\n",
    ")\n",
    "\n",
    "gnn_train_data = df_filtered.loc[train_idx]\n",
    "gnn_val_data = df_filtered.loc[val_idx]\n",
    "gnn_test_data = df_filtered.loc[test_idx]\n",
    "\n",
    "# --- GNN Data Preparation ---\n",
    "\n",
    "# Features and Target (Clearly Defined)\n",
    "gnn_features = [\n",
    "    'cores_per_task', 'num_cores_alloc', 'num_nodes_alloc',\n",
    "    'num_gpus_alloc', 'mem_alloc', 'shared', 'priority',\n",
    "    'num_tasks',\n",
    "]\n",
    "\n",
    "\n",
    "power_target = ['mean_node_power']\n",
    "\n",
    "def prepare_gnn_data(df, features, target, feature_scaler=None, power_scaler=None):\n",
    "    \"\"\"\n",
    "    Prepares data for GNN, including feature scaling and neighbor calculation.\n",
    "    \"\"\"\n",
    "    # Fit scalers only on training data, transform others\n",
    "    if feature_scaler is None:\n",
    "        feature_scaler = StandardScaler()\n",
    "        X = feature_scaler.fit_transform(df[features])\n",
    "    else:\n",
    "        X = feature_scaler.transform(df[features])\n",
    "\n",
    "    if power_scaler is None:\n",
    "        power_scaler = StandardScaler()\n",
    "        y = power_scaler.fit_transform(df[target])\n",
    "    else:\n",
    "        y = power_scaler.transform(df[target])\n",
    "\n",
    "    # KNN calculation\n",
    "    knn = NearestNeighbors(n_neighbors=6)  # k=5 + 1 (itself)\n",
    "    knn.fit(X)\n",
    "    distances, indices = knn.kneighbors(X)\n",
    "    neighbor_indices = indices[:, 1:]\n",
    "\n",
    "    return X, y, neighbor_indices, feature_scaler, power_scaler\n",
    "\n",
    "def create_graph_batch(features, neighbor_indices, targets, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Creates graph batches with edges based on KNN.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    num_samples = len(features)\n",
    "    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_end = min(i + batch_size, num_samples)\n",
    "        batch_features = features[i:batch_end]\n",
    "        batch_neighbors = neighbor_indices[i:batch_end]\n",
    "        batch_targets = targets[i:batch_end]\n",
    "        \n",
    "        # Create edges based on KNN within this batch\n",
    "        edge_index = []\n",
    "        \n",
    "        for job_idx, neighbors in enumerate(batch_neighbors):\n",
    "            for neighbor_idx in neighbors:\n",
    "                # Convert global indices to local batch indices\n",
    "                local_job_idx = job_idx\n",
    "                local_neighbor_idx = neighbor_idx - i\n",
    "        \n",
    "                # Only add edges for neighbors that are within this batch\n",
    "                if 0 <= local_neighbor_idx < len(batch_features):\n",
    "                    edge_index.append([local_job_idx, local_neighbor_idx])\n",
    "                    edge_index.append([local_neighbor_idx, local_job_idx])\n",
    "        \n",
    "        if not edge_index:  # If no valid edges in this batch\n",
    "            continue\n",
    "            \n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "        \n",
    "        # Create graph with matching features and targets\n",
    "        graph = Data(\n",
    "            x=torch.FloatTensor(batch_features),\n",
    "            edge_index=edge_index,\n",
    "            y=torch.FloatTensor(batch_targets)  # Add dimension for consistency\n",
    "        )\n",
    "        graphs.append(graph)\n",
    "        \n",
    "    return graphs\n",
    "\n",
    "# Prepare data (fit scalers on train, transform all)\n",
    "train_X, train_y, train_neighbors, feature_scaler, power_scaler = prepare_gnn_data(gnn_train_data, gnn_features, power_target)\n",
    "val_X, val_y, val_neighbors, _, _ = prepare_gnn_data(gnn_val_data, gnn_features, power_target, feature_scaler, power_scaler)\n",
    "test_X, test_y, test_neighbors, _, _ = prepare_gnn_data(gnn_test_data, gnn_features, power_target, feature_scaler, power_scaler)\n",
    "\n",
    "# Create graph batches\n",
    "train_graphs = create_graph_batch(train_X, train_neighbors, train_y)\n",
    "val_graphs = create_graph_batch(val_X, val_neighbors, val_y)\n",
    "test_graphs = create_graph_batch(test_X, test_neighbors, test_y)\n",
    "\n",
    "# --- GNN Model ---\n",
    "\n",
    "class PowerPredictionGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super(PowerPredictionGNN, self).__init__()\n",
    "\n",
    "        self.node_embed = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.power_pred = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.node_embed(x)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1, edge_index))) + x1\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2, edge_index))) + x2\n",
    "        return self.power_pred(x3)\n",
    "\n",
    "# --- Loss Function and Training ---\n",
    "\n",
    "def power_prediction_loss(pred_power, true_power):\n",
    "    \"\"\"\n",
    "    Custom loss function: Normalized MSE + Relative Error\n",
    "    \"\"\"\n",
    "    pred_power = pred_power.squeeze()\n",
    "    true_power = true_power.squeeze()\n",
    "\n",
    "    pred_norm = (pred_power - pred_power.mean()) / (pred_power.std() + 1e-8)\n",
    "    true_norm = (true_power - true_power.mean()) / (true_power.std() + 1e-8)\n",
    "\n",
    "    mse_loss = F.mse_loss(pred_norm, true_norm)\n",
    "    relative_error = torch.abs(pred_power - true_power) / (torch.abs(true_power) + 1e-8)\n",
    "    relative_loss = torch.mean(relative_error)\n",
    "\n",
    "    return mse_loss + 0.5 * relative_loss\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_gnn(model, train_graphs, val_graphs, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    early_stopping = EarlyStopping(patience=config['early_stopping_patience'])\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for graph in train_graphs:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(graph.x, graph.edge_index)\n",
    "            loss = power_prediction_loss(out, graph.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for graph in val_graphs:\n",
    "                out = model(graph.x, graph.edge_index)\n",
    "                val_loss += power_prediction_loss(out, graph.y).item()\n",
    "\n",
    "        train_loss /= len(train_graphs)\n",
    "        val_loss /= len(val_graphs)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "def evaluate_predictions(model, test_graphs, power_scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph in test_graphs:\n",
    "            pred = model(graph.x, graph.edge_index)\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            true = graph.y.squeeze().numpy().reshape(-1, 1)\n",
    "            pred = power_scaler.inverse_transform(pred)\n",
    "            true = power_scaler.inverse_transform(true)\n",
    "            predictions.extend(pred.flatten())\n",
    "            true_values.extend(true.flatten())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "\n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "\n",
    "    results = {\n",
    "        'mse': mse,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mean_relative_error': np.mean(np.abs(predictions - true_values) / (true_values + 1e-8)),\n",
    "        'mean_pred': np.mean(predictions),\n",
    "        'mean_true': np.mean(true_values),\n",
    "        'std_pred': np.std(predictions),\n",
    "        'std_true': np.std(true_values)\n",
    "    }\n",
    "\n",
    "    for p in [25, 50, 75, 90]:\n",
    "        error_percentile = np.percentile(np.abs(predictions - true_values), p)\n",
    "        results[f'error_p{p}'] = error_percentile\n",
    "\n",
    "    return results, predictions, true_values\n",
    "\n",
    "# --- Training Configuration and Initialization ---\n",
    "\n",
    "config = {\n",
    "    'hidden_dim': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 100,\n",
    "    'early_stopping_patience': 15\n",
    "}\n",
    "\n",
    "input_dim = len(gnn_features)\n",
    "model = PowerPredictionGNN(input_dim=input_dim, hidden_dim=config['hidden_dim'])\n",
    "print(model)\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_losses, val_losses = train_gnn(model, train_graphs, val_graphs, config)\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "test_metrics, predictions, true_values = evaluate_predictions(model, test_graphs, power_scaler)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "#RL\n",
    "\n",
    "class HPCEnvironment:\n",
    "    def __init__(self, gnn_model, feature_scaler, power_scaler, num_nodes=100):\n",
    "        self.gnn_model = gnn_model\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.power_scaler = power_scaler\n",
    "        self.num_nodes = num_nodes\n",
    "        self.peak_hour_cost = 2.5\n",
    "        self.off_peak_cost = 0.6\n",
    "        self.power_history = []\n",
    "        self.peak_threshold = None\n",
    "        self.waiting_jobs = []\n",
    "        self.completed_jobs = []\n",
    "        self.running_jobs = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment state\"\"\"\n",
    "        self.node_status = [True] * self.num_nodes\n",
    "        self.current_time = pd.Timestamp.now().replace(hour=0, minute=0)\n",
    "        self.running_jobs = []\n",
    "        self.energy_consumption = 0\n",
    "        self.peak_consumption = 0\n",
    "        self.off_peak_consumption = 0\n",
    "        self.total_cost = 0\n",
    "        self.waiting_jobs = []\n",
    "        self.completed_jobs = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Enhanced state representation\"\"\"\n",
    "        is_peak_hour = 1.0 if 8 <= self.current_time.hour < 20 else 0.0\n",
    "        hours_until_change = min(\n",
    "            abs(8 - self.current_time.hour) if self.current_time.hour < 8 else\n",
    "            abs(20 - self.current_time.hour) if 8 <= self.current_time.hour < 20 else\n",
    "            abs(32 - self.current_time.hour),\n",
    "            12\n",
    "        ) / 12.0\n",
    "        \n",
    "        return np.array([\n",
    "            sum(self.node_status) / self.num_nodes,  # Available nodes\n",
    "            len(self.running_jobs) / self.num_nodes,  # System load\n",
    "            self.current_time.hour / 24.0,  # Time of day\n",
    "            is_peak_hour,  # Peak hour indicator\n",
    "            hours_until_change,  # Hours until price change\n",
    "            self.peak_consumption / (self.energy_consumption + 1e-8),  # Peak ratio\n",
    "            len(self.waiting_jobs) / 50,  # Waiting job pressure\n",
    "            self.total_cost / 100000  # Normalized total cost\n",
    "        ])\n",
    "    def _can_schedule(self, job):\n",
    "        \"\"\"Check if job can be scheduled\"\"\"\n",
    "        return sum(self.node_status) >= job['num_nodes_alloc']\n",
    "\n",
    "    def _allocate_nodes(self, job):\n",
    "        \"\"\"Allocate nodes to a job\"\"\"\n",
    "        nodes_needed = job['num_nodes_alloc']\n",
    "        allocated_nodes = []\n",
    "        \n",
    "        for i, available in enumerate(self.node_status):\n",
    "            if available and len(allocated_nodes) < nodes_needed:\n",
    "                allocated_nodes.append(i)\n",
    "                self.node_status[i] = False\n",
    "        \n",
    "        return allocated_nodes\n",
    "\n",
    "    def _update_running_jobs(self):\n",
    "        \"\"\"Update status of running jobs\"\"\"\n",
    "        remaining_jobs = []\n",
    "        for job, nodes, end_time in self.running_jobs:\n",
    "            if end_time <= self.current_time:\n",
    "                for node in nodes:\n",
    "                    self.node_status[node] = True\n",
    "                self.completed_jobs.append(job)\n",
    "            else:\n",
    "                remaining_jobs.append((job, nodes, end_time))\n",
    "        self.running_jobs = remaining_jobs\n",
    "\n",
    "    def _predict_power(self, job):\n",
    "        \"\"\"Predict power consumption for a job\"\"\"\n",
    "        job_features = np.array([[job[f] for f in gnn_features]])\n",
    "        scaled_features = self.feature_scaler.transform(job_features)\n",
    "        x = torch.FloatTensor(scaled_features)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        \n",
    "        self.gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self.gnn_model(x, edge_index)\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            pred = self.power_scaler.inverse_transform(pred)\n",
    "        return pred.item()\n",
    "\n",
    "    def step(self, action, job):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        info = {'scheduled': False}\n",
    "        \n",
    "        is_peak_hour = 8 <= self.current_time.hour < 20\n",
    "        power_price = self.peak_hour_cost if is_peak_hour else self.off_peak_cost\n",
    "        \n",
    "        if action == 0:  # Schedule now\n",
    "            if self._can_schedule(job):\n",
    "                power = job['mean_node_power']\n",
    "                duration_hours = job['run_time'] / 3600\n",
    "                \n",
    "                # Calculate costs\n",
    "                energy_cost = power * duration_hours * power_price\n",
    "                min_possible_cost = power * duration_hours * self.off_peak_cost\n",
    "                \n",
    "                # Much stronger penalties for peak hour scheduling\n",
    "                if is_peak_hour:\n",
    "                    base_penalty = -5.0  # Increased base penalty\n",
    "                    power_penalty = -2.0 * (power / 1000)  # Additional penalty for high-power jobs\n",
    "                    reward = base_penalty + power_penalty\n",
    "                else:\n",
    "                    reward = 2.0  # Bonus for off-peak scheduling\n",
    "                \n",
    "                # Cost efficiency reward\n",
    "                cost_efficiency = (min_possible_cost - energy_cost) / min_possible_cost\n",
    "                reward += 3.0 * cost_efficiency\n",
    "                \n",
    "                # Update metrics\n",
    "                self.energy_consumption += power * duration_hours\n",
    "                self.total_cost += energy_cost\n",
    "                if is_peak_hour:\n",
    "                    self.peak_consumption += power * duration_hours\n",
    "                \n",
    "                info = {\n",
    "                    'scheduled': True,\n",
    "                    'energy_cost': energy_cost,\n",
    "                    'is_peak': is_peak_hour,\n",
    "                    'power': power\n",
    "                }\n",
    "            else:\n",
    "                reward = -3.0\n",
    "        else:  # Wait\n",
    "            if is_peak_hour:\n",
    "                reward = 1.0  # Reward for waiting during peak\n",
    "                if job['mean_node_power'] > 1000:  # Extra reward for waiting with high-power jobs\n",
    "                    reward += 1.0\n",
    "            else:\n",
    "                reward = -1.0  # Penalty for waiting during off-peak\n",
    "            \n",
    "            self.current_time += pd.Timedelta(minutes=30)\n",
    "        \n",
    "        # Update running jobs\n",
    "        self._update_running_jobs()\n",
    "        \n",
    "        # End of day check\n",
    "        if self.current_time.hour >= 23:\n",
    "            done = True\n",
    "            # Final reward based on peak ratio\n",
    "            peak_ratio = self.peak_consumption / (self.energy_consumption + 1e-8)\n",
    "            if peak_ratio < 0.3:\n",
    "                reward += 10.0\n",
    "            elif peak_ratio > 0.6:\n",
    "                reward -= 10.0\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    \n",
    "    def _predict_power(self, job):\n",
    "        job_features = np.array([[job[f] for f in gnn_features]])\n",
    "        scaled_features = self.feature_scaler.transform(job_features)\n",
    "        x = torch.FloatTensor(scaled_features)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        \n",
    "        self.gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self.gnn_model(x, edge_index)\n",
    "            pred = pred.squeeze().numpy().reshape(-1, 1)\n",
    "            pred = self.power_scaler.inverse_transform(pred)\n",
    "        return pred.item()\n",
    "    \n",
    "    def _can_schedule(self, job):\n",
    "        return sum(self.node_status) >= job['num_nodes_alloc']\n",
    "    \n",
    "    def _allocate_nodes(self, job):\n",
    "        nodes_needed = job['num_nodes_alloc']\n",
    "        allocated_nodes = []\n",
    "        \n",
    "        for i, available in enumerate(self.node_status):\n",
    "            if available and len(allocated_nodes) < nodes_needed:\n",
    "                allocated_nodes.append(i)\n",
    "                self.node_status[i] = False\n",
    "                \n",
    "        return allocated_nodes\n",
    "    \n",
    "    def _update_running_jobs(self):\n",
    "        # Remove completed jobs and free their nodes\n",
    "        remaining_jobs = []\n",
    "        for job, nodes, end_time in self.running_jobs:\n",
    "            if end_time <= self.current_time:\n",
    "                for node in nodes:\n",
    "                    self.node_status[node] = True\n",
    "            else:\n",
    "                remaining_jobs.append((job, nodes, end_time))\n",
    "        self.running_jobs = remaining_jobs\n",
    "    \n",
    "    def _get_power_price(self):\n",
    "        hour = self.current_time.hour\n",
    "        if 8 <= hour < 20:  # Peak hours\n",
    "            return 1.5\n",
    "        return 1.0\n",
    "    \n",
    "    def get_expected_cost_difference(self, job):\n",
    "        \"\"\"Calculate cost difference between peak and off-peak scheduling\"\"\"\n",
    "        power = job['mean_node_power']\n",
    "        duration = job['run_time'] / 3600\n",
    "        peak_cost = power * duration * self.peak_hour_cost\n",
    "        off_peak_cost = power * duration * self.off_peak_cost\n",
    "        return peak_cost - off_peak_cost\n",
    "\n",
    "    def get_waiting_penalty(self, job):\n",
    "        \"\"\"Calculate penalty for waiting based on job characteristics\"\"\"\n",
    "        waiting_jobs_count = len(self.waiting_jobs)\n",
    "        system_load = len(self.running_jobs) / self.num_nodes\n",
    "        urgency = min(job['run_time'] / 3600, 24)  # Cap at 24 hours\n",
    "        \n",
    "        base_penalty = -0.1 * waiting_jobs_count\n",
    "        load_factor = -0.2 if system_load < 0.3 else 0.0  # Penalize waiting when system is underutilized\n",
    "        urgency_factor = -0.1 * urgency / 24  # Higher penalty for long jobs\n",
    "        \n",
    "        return base_penalty + load_factor + urgency_factor\n",
    "    \n",
    "\n",
    "    class PPOMemory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def generate_batches(self, batch_size):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+batch_size] for i in batch_start]\n",
    "        return np.array(self.states),\\\n",
    "               np.array(self.actions),\\\n",
    "               np.array(self.probs),\\\n",
    "               np.array(self.vals),\\\n",
    "               np.array(self.rewards),\\\n",
    "               np.array(self.dones),\\\n",
    "               batches\n",
    "    \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions=3, hidden_dim=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.actor(state)\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dim=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "\n",
    "# 3. Add experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "\n",
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.peak_schedules = 0\n",
    "        self.off_peak_schedules = 0\n",
    "        self.total_wait_time = 0\n",
    "        self.energy_costs = []\n",
    "        self.peak_ratios = []\n",
    "        \n",
    "    def update(self, time, action, energy_cost=0, is_peak=False):\n",
    "        if action == 0:  # Schedule\n",
    "            if is_peak:\n",
    "                self.peak_schedules += 1\n",
    "            else:\n",
    "                self.off_peak_schedules += 1\n",
    "            self.energy_costs.append(energy_cost)\n",
    "        else:  # Wait\n",
    "            self.total_wait_time += 1 if action == 1 else 4\n",
    "            \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'peak_schedule_ratio': self.peak_schedules / (self.peak_schedules + self.off_peak_schedules + 1e-8),\n",
    "            'total_wait_hours': self.total_wait_time,\n",
    "            'avg_energy_cost': np.mean(self.energy_costs) if self.energy_costs else 0\n",
    "        }\n",
    "    \n",
    "    def train_scheduler(env, n_episodes=1000):\n",
    "    input_dims = 8  # Updated state dimension for enhanced state space\n",
    "    n_actions = 2  # Schedule now or wait\n",
    "    \n",
    "    actor = ActorNetwork(input_dims, n_actions)\n",
    "    critic = CriticNetwork(input_dims)\n",
    "    memory = PPOMemory()\n",
    "    \n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # Prepare job queue once\n",
    "    train_jobs = prepare_job_queue(gnn_train_data, n_jobs=100)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        job_queue = train_jobs.copy()  # Reset job queue for each episode\n",
    "        peak_schedules = 0\n",
    "        total_schedules = 0\n",
    "        \n",
    "        while not done and len(job_queue) > 0:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            \n",
    "            # Get action\n",
    "            action_probs = actor(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            # Get next job from queue\n",
    "            current_job = job_queue[0]\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "            \n",
    "            # Track peak vs off-peak scheduling\n",
    "            if info['scheduled']:\n",
    "                total_schedules += 1\n",
    "                job_queue.pop(0)\n",
    "                if info.get('is_peak', False):\n",
    "                    peak_schedules += 1\n",
    "            \n",
    "            # Store transition\n",
    "            memory.store_memory(state, action.item(), action_probs[action.item()].item(),\n",
    "                              critic(state_tensor).item(), reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        # Update policy after episode\n",
    "        if episode % 10 == 0:\n",
    "            update_policy(memory, actor, critic)\n",
    "            memory.clear_memory()\n",
    "            \n",
    "            # Print detailed metrics\n",
    "            peak_ratio = peak_schedules / (total_schedules + 1e-8)\n",
    "            print(f'Episode {episode}:')\n",
    "            print(f'Total Reward: {total_reward:.2f}')\n",
    "            print(f'Energy Cost: ${env.total_cost:.2f}')\n",
    "            print(f'Energy Usage: {env.energy_consumption:.2f} kWh')\n",
    "            print(f'Peak Schedule Ratio: {peak_ratio:.2%}')\n",
    "            print(f'Average Cost per kWh: ${env.total_cost/env.energy_consumption:.2f}\\n')\n",
    "        \n",
    "        # Save best model\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            torch.save(actor.state_dict(), 'best_scheduler_actor.pth')\n",
    "            torch.save(critic.state_dict(), 'best_scheduler_critic.pth')\n",
    "\n",
    "    return actor, critic\n",
    "\n",
    "# Create a job queue for testing\n",
    "def prepare_job_queue(data, n_jobs=100):\n",
    "    \"\"\"Prepare a queue of jobs for testing\"\"\"\n",
    "    jobs = data.sample(n=n_jobs).to_dict('records')\n",
    "    # Sort by submit time to maintain temporal order\n",
    "    jobs = sorted(jobs, key=lambda x: x['submit_time'])\n",
    "    return jobs\n",
    "\n",
    "# Add the ReplayBuffer class definition before the training loop\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "# Modify the training section\n",
    "print(\"Starting experiment...\")\n",
    "env = HPCEnvironment(\n",
    "    gnn_model=model,\n",
    "    feature_scaler=feature_scaler,\n",
    "    power_scaler=power_scaler,\n",
    "    num_nodes=100\n",
    ")\n",
    "\n",
    "input_dims = 8  # New state dimension\n",
    "n_actions = 2\n",
    "actor = ActorNetwork(input_dims, n_actions)\n",
    "critic = CriticNetwork(input_dims)\n",
    "\n",
    "# Add optimizers and scheduler\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=0.001)\n",
    "actor_scheduler = ReduceLROnPlateau(actor_optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Create replay buffer\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "batch_size = 64\n",
    "\n",
    "train_jobs = prepare_job_queue(gnn_train_data, n_jobs=1000)\n",
    "test_jobs = prepare_job_queue(gnn_test_data, n_jobs=100)\n",
    "\n",
    "print(\"\\nTraining scheduler...\")\n",
    "training_stats = []\n",
    "best_reward = float('-inf')\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    episode_rewards = []\n",
    "    episode_actions = []\n",
    "    episode_costs = []\n",
    "    job_queue = train_jobs.copy()\n",
    "    \n",
    "    while len(job_queue) > 0:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action_probs = actor(state_tensor)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        current_job = job_queue[0]\n",
    "        next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "        \n",
    "        # Store experience in buffer\n",
    "        buffer.push(state, action.item(), reward, next_state, done)\n",
    "        \n",
    "        # Update networks if we have enough samples\n",
    "        if len(buffer.buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            states_tensor = torch.FloatTensor(states)\n",
    "            actions_tensor = torch.LongTensor(actions)\n",
    "            rewards_tensor = torch.FloatTensor(rewards)\n",
    "            next_states_tensor = torch.FloatTensor(next_states)\n",
    "            dones_tensor = torch.FloatTensor(dones)\n",
    "            \n",
    "            # Get values\n",
    "            current_values = critic(states_tensor).squeeze()\n",
    "            next_values = critic(next_states_tensor).squeeze().detach()\n",
    "            \n",
    "            # Calculate advantage\n",
    "            advantages = rewards_tensor + (1 - dones_tensor) * 0.99 * next_values - current_values\n",
    "            \n",
    "            # Actor loss\n",
    "            action_probs = actor(states_tensor)\n",
    "            log_probs = torch.log(action_probs + 1e-10)\n",
    "            actor_loss = -(advantages.detach() * log_probs[range(batch_size), actions_tensor]).mean()\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = F.mse_loss(current_values, rewards_tensor + (1 - dones_tensor) * 0.99 * next_values)\n",
    "            \n",
    "            # Update networks\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "        \n",
    "        if info['scheduled']:\n",
    "            job_queue.pop(0)\n",
    "        \n",
    "        episode_rewards.append(reward)\n",
    "        episode_actions.append(action.item())\n",
    "        if 'energy_cost' in info:\n",
    "            episode_costs.append(info['energy_cost'])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    total_reward = sum(episode_rewards)\n",
    "    schedule_rate = sum(1 for a in episode_actions if a == 0) / len(episode_actions)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    actor_scheduler.step(total_reward)\n",
    "    \n",
    "    training_stats.append({\n",
    "        'episode': episode,\n",
    "        'total_reward': total_reward,\n",
    "        'schedule_rate': schedule_rate,\n",
    "        'energy_cost': env.total_cost,\n",
    "        'energy_consumption': env.energy_consumption\n",
    "    })\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"\\nEpisode {episode}\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Schedule Rate: {schedule_rate:.2%}\")\n",
    "        print(f\"Energy Cost: ${env.total_cost:.2f}\")\n",
    "        print(f\"Energy Consumption: {env.energy_consumption:.2f} kWh\")\n",
    "    \n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        torch.save(actor.state_dict(), 'best_scheduler_actor.pth')\n",
    "\n",
    "# Add optimizers and scheduler\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=0.001)\n",
    "actor_scheduler = ReduceLROnPlateau(actor_optimizer, mode='max', factor=0.5, patience=5) #, verbose=True\n",
    "\n",
    "# Create replay buffer\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "batch_size = 64\n",
    "\n",
    "train_jobs = prepare_job_queue(gnn_train_data, n_jobs=1000)\n",
    "test_jobs = prepare_job_queue(gnn_test_data, n_jobs=100)\n",
    "\n",
    "print(\"\\nTraining scheduler...\")\n",
    "training_stats = []\n",
    "best_reward = float('-inf')\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    episode_rewards = []\n",
    "    episode_actions = []\n",
    "    episode_costs = []\n",
    "    job_queue = train_jobs.copy()\n",
    "    \n",
    "    while len(job_queue) > 0:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action_probs = actor(state_tensor)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        current_job = job_queue[0]\n",
    "        next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "        \n",
    "        # Store experience in buffer\n",
    "        buffer.push(state, action.item(), reward, next_state, done)\n",
    "        \n",
    "        # Update networks if we have enough samples\n",
    "        if len(buffer.buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            states_tensor = torch.FloatTensor(states)\n",
    "            actions_tensor = torch.LongTensor(actions)\n",
    "            rewards_tensor = torch.FloatTensor(rewards)\n",
    "            next_states_tensor = torch.FloatTensor(next_states)\n",
    "            dones_tensor = torch.FloatTensor(dones)\n",
    "            \n",
    "            # Get values\n",
    "            current_values = critic(states_tensor).squeeze()\n",
    "            next_values = critic(next_states_tensor).squeeze().detach()\n",
    "            \n",
    "            # Calculate advantage\n",
    "            advantages = rewards_tensor + (1 - dones_tensor) * 0.99 * next_values - current_values\n",
    "            \n",
    "            # Actor loss\n",
    "            action_probs = actor(states_tensor)\n",
    "            log_probs = torch.log(action_probs + 1e-10)\n",
    "            actor_loss = -(advantages.detach() * log_probs[range(batch_size), actions_tensor]).mean()\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = F.mse_loss(current_values, rewards_tensor + (1 - dones_tensor) * 0.99 * next_values)\n",
    "            \n",
    "            # Update networks\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "        \n",
    "        if info['scheduled']:\n",
    "            job_queue.pop(0)\n",
    "        \n",
    "        episode_rewards.append(reward)\n",
    "        episode_actions.append(action.item())\n",
    "        if 'energy_cost' in info:\n",
    "            episode_costs.append(info['energy_cost'])\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    total_reward = sum(episode_rewards)\n",
    "    schedule_rate = sum(1 for a in episode_actions if a == 0) / len(episode_actions)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    actor_scheduler.step(total_reward)\n",
    "    \n",
    "    training_stats.append({\n",
    "        'episode': episode,\n",
    "        'total_reward': total_reward,\n",
    "        'schedule_rate': schedule_rate,\n",
    "        'energy_cost': env.total_cost,\n",
    "        'energy_consumption': env.energy_consumption\n",
    "    })\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"\\nEpisode {episode}\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Schedule Rate: {schedule_rate:.2%}\")\n",
    "        print(f\"Energy Cost: ${env.total_cost:.2f}\")\n",
    "        print(f\"Energy Consumption: {env.energy_consumption:.2f} kWh\")\n",
    "    \n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        torch.save(actor.state_dict(), 'best_scheduler_actor.pth')\n",
    "\n",
    "\n",
    "\n",
    "class SchedulerAnalytics:\n",
    "    def __init__(self):\n",
    "        self.episode_metrics = []\n",
    "        self.hourly_distributions = {}\n",
    "        self.cost_metrics = []\n",
    "        self.power_patterns = []\n",
    "    \n",
    "    def update(self, env, episode_num):\n",
    "        \"\"\"Update metrics after each episode\"\"\"\n",
    "        # Peak vs Off-peak distribution\n",
    "        peak_metrics = self.analyze_peak_distribution(env)\n",
    "        \n",
    "        # Cost efficiency\n",
    "        cost_metrics = self.calculate_cost_efficiency(env)\n",
    "        \n",
    "        # Hourly job distribution\n",
    "        hourly_dist = self.get_hourly_distribution(env)\n",
    "        \n",
    "        # Power consumption patterns\n",
    "        power_metrics = self.analyze_power_patterns(env)\n",
    "        \n",
    "        metrics = {\n",
    "            'episode': episode_num,\n",
    "            'total_cost': env.total_cost,\n",
    "            'energy_consumption': env.energy_consumption,\n",
    "            **peak_metrics,\n",
    "            **cost_metrics,\n",
    "            'hourly_distribution': hourly_dist,\n",
    "            **power_metrics\n",
    "        }\n",
    "        \n",
    "        self.episode_metrics.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def get_hourly_distribution(self, env):\n",
    "        \"\"\"Fixed version of hourly distribution calculation\"\"\"\n",
    "        hourly_counts = {hour: 0 for hour in range(24)}\n",
    "        for job in env.completed_jobs:\n",
    "            hour = job['submit_time'].hour\n",
    "            hourly_counts[hour] += 1\n",
    "        return hourly_counts\n",
    "    \n",
    "    def analyze_peak_distribution(self, env):\n",
    "        peak_hours = env.peak_consumption\n",
    "        off_peak_hours = env.energy_consumption - env.peak_consumption\n",
    "        peak_ratio = peak_hours / (env.energy_consumption + 1e-8)\n",
    "        return {\n",
    "            'peak_consumption': peak_hours,\n",
    "            'off_peak_consumption': off_peak_hours,\n",
    "            'peak_ratio': peak_ratio\n",
    "        }\n",
    "    \n",
    "    def calculate_cost_efficiency(self, env):\n",
    "        avg_cost_per_kwh = env.total_cost / (env.energy_consumption + 1e-8)\n",
    "        potential_min_cost = env.energy_consumption * env.off_peak_cost\n",
    "        cost_efficiency = potential_min_cost / (env.total_cost + 1e-8)\n",
    "        return {\n",
    "            'avg_cost_per_kwh': avg_cost_per_kwh,\n",
    "            'cost_efficiency': cost_efficiency\n",
    "        }\n",
    "    \n",
    "    def analyze_power_patterns(self, env):\n",
    "        if not env.power_history:\n",
    "            return {'avg_power': 0, 'high_power_ratio': 0}\n",
    "        \n",
    "        avg_power = np.mean(env.power_history)\n",
    "        high_power_threshold = np.percentile(env.power_history, 75)\n",
    "        high_power_jobs = sum(1 for p in env.power_history if p > high_power_threshold)\n",
    "        high_power_ratio = high_power_jobs / len(env.power_history)\n",
    "        \n",
    "        return {\n",
    "            'avg_power': avg_power,\n",
    "            'high_power_ratio': high_power_ratio\n",
    "        }\n",
    "\n",
    "def run_experiment():\n",
    "    print(\"Initializing environment...\")\n",
    "    env = HPCEnvironment(\n",
    "        gnn_model=model,\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        num_nodes=100\n",
    "    )\n",
    "    \n",
    "    print(\"Creating networks...\")\n",
    "    input_dims = 8\n",
    "    n_actions = 2\n",
    "    actor = ActorNetwork(input_dims, n_actions)\n",
    "    critic = CriticNetwork(input_dims)\n",
    "    \n",
    "    print(\"Creating analytics tracker...\")\n",
    "    analytics = SchedulerAnalytics()\n",
    "    \n",
    "    print(\"Preparing job queue...\")\n",
    "    # Start with a smaller job queue for testing\n",
    "    train_jobs = prepare_job_queue(gnn_train_data, n_jobs=100)\n",
    "    print(f\"Job queue size: {len(train_jobs)}\")\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    for episode in range(100):\n",
    "        print(f\"\\nStarting Episode {episode}\")\n",
    "        state = env.reset()\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        job_queue = train_jobs.copy()\n",
    "        step_count = 0\n",
    "        max_steps = 1000\n",
    "        \n",
    "        while len(job_queue) > 0 and step_count < max_steps:\n",
    "            if step_count % 50 == 0:\n",
    "                print(f\"Step {step_count}, Remaining jobs: {len(job_queue)}\")\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs = actor(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            current_job = job_queue[0]\n",
    "            next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "            \n",
    "            if info['scheduled']:\n",
    "                job_queue.pop(0)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            episode_actions.append(action.item())\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode finished after {step_count} steps\")\n",
    "                break\n",
    "        \n",
    "        # Print basic metrics before analytics update\n",
    "        total_reward = sum(episode_rewards)\n",
    "        schedule_rate = sum(1 for a in episode_actions if a == 0) / len(episode_actions)\n",
    "        \n",
    "        print(f\"Basic metrics for episode {episode}:\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Schedule Rate: {schedule_rate:.2%}\")\n",
    "        print(f\"Total Cost: ${env.total_cost:.2f}\")\n",
    "        \n",
    "        # Update analytics with error handling\n",
    "        try:\n",
    "            metrics = analytics.update(env, episode)\n",
    "            print(f\"Peak Ratio: {metrics['peak_ratio']:.2%}\")\n",
    "            print(f\"Cost Efficiency: {metrics['cost_efficiency']:.2%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Analytics update failed: {str(e)}\")\n",
    "            print(\"Continuing with training...\")\n",
    "        \n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            torch.save(actor.state_dict(), 'best_scheduler_actor.pth')\n",
    "    \n",
    "    return analytics\n",
    "\n",
    "# Run with error handling\n",
    "try:\n",
    "    print(\"Starting experiment...\")\n",
    "    analytics = run_experiment()\n",
    "    print(\"Experiment completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Create visualization of metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Peak vs Off-peak consumption\n",
    "        episodes = [m['episode'] for m in self.episode_metrics]\n",
    "        peak_ratios = [m['peak_ratio'] for m in self.episode_metrics]\n",
    "        axes[0, 0].plot(episodes, peak_ratios)\n",
    "        axes[0, 0].set_title('Peak Hour Consumption Ratio')\n",
    "        axes[0, 0].set_ylabel('Peak/Total Ratio')\n",
    "        axes[0, 0].axhline(y=0.4, color='g', linestyle='--', label='Target Ratio')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Plot 2: Cost Efficiency\n",
    "        cost_efficiency = [m['cost_efficiency'] for m in self.episode_metrics]\n",
    "        axes[0, 1].plot(episodes, cost_efficiency)\n",
    "        axes[0, 1].set_title('Cost Efficiency Over Time')\n",
    "        axes[0, 1].set_ylabel('Efficiency Ratio')\n",
    "        \n",
    "        # Plot 3: Power Distribution\n",
    "        avg_power = [m['avg_power'] for m in self.episode_metrics]\n",
    "        high_power_ratio = [m['high_power_ratio'] for m in self.episode_metrics]\n",
    "        axes[1, 0].plot(episodes, avg_power, label='Avg Power')\n",
    "        axes[1, 0].plot(episodes, high_power_ratio, label='High Power Ratio')\n",
    "        axes[1, 0].set_title('Power Consumption Patterns')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: Cost per kWh\n",
    "        cost_per_kwh = [m['avg_cost_per_kwh'] for m in self.episode_metrics]\n",
    "        axes[1, 1].plot(episodes, cost_per_kwh)\n",
    "        axes[1, 1].set_title('Average Cost per kWh')\n",
    "        axes[1, 1].set_ylabel('Cost ($)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Modify training loop to use analytics\n",
    "def run_experiment():\n",
    "    print(\"Initializing environment...\")\n",
    "    env = HPCEnvironment(\n",
    "        gnn_model=model,\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        num_nodes=100\n",
    "    )\n",
    "    \n",
    "    print(\"Creating networks...\")\n",
    "    input_dims = 8\n",
    "    n_actions = 2\n",
    "    actor = ActorNetwork(input_dims, n_actions)\n",
    "    critic = CriticNetwork(input_dims)\n",
    "    \n",
    "    print(\"Creating analytics tracker...\")\n",
    "    analytics = SchedulerAnalytics()\n",
    "    \n",
    "    print(\"Preparing job queue...\")\n",
    "    # Start with a smaller job queue for testing\n",
    "    train_jobs = prepare_job_queue(gnn_train_data, n_jobs=100)\n",
    "    print(f\"Job queue size: {len(train_jobs)}\")\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    for episode in range(100):\n",
    "        print(f\"\\nStarting Episode {episode}\")\n",
    "        state = env.reset()\n",
    "        episode_rewards = []\n",
    "        episode_actions = []\n",
    "        job_queue = train_jobs.copy()\n",
    "        step_count = 0\n",
    "        max_steps = 1000\n",
    "        \n",
    "        while len(job_queue) > 0 and step_count < max_steps:\n",
    "            if step_count % 50 == 0:\n",
    "                print(f\"Step {step_count}, Remaining jobs: {len(job_queue)}\")\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs = actor(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            current_job = job_queue[0]\n",
    "            next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "            \n",
    "            if info['scheduled']:\n",
    "                job_queue.pop(0)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            episode_actions.append(action.item())\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode finished after {step_count} steps\")\n",
    "                break\n",
    "        \n",
    "        # Print basic metrics before analytics update\n",
    "        total_reward = sum(episode_rewards)\n",
    "        schedule_rate = sum(1 for a in episode_actions if a == 0) / len(episode_actions)\n",
    "        \n",
    "        print(f\"Basic metrics for episode {episode}:\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Schedule Rate: {schedule_rate:.2%}\")\n",
    "        print(f\"Total Cost: ${env.total_cost:.2f}\")\n",
    "        \n",
    "        # Update analytics with error handling\n",
    "        try:\n",
    "            metrics = analytics.update(env, episode)\n",
    "            print(f\"Peak Ratio: {metrics['peak_ratio']:.2%}\")\n",
    "            print(f\"Cost Efficiency: {metrics['cost_efficiency']:.2%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Analytics update failed: {str(e)}\")\n",
    "            print(\"Continuing with training...\")\n",
    "        \n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            torch.save(actor.state_dict(), 'best_scheduler_actor.pth')\n",
    "    \n",
    "    return analytics\n",
    "\n",
    "# Run with error handling\n",
    "try:\n",
    "    print(\"Starting experiment...\")\n",
    "    analytics = run_experiment()\n",
    "    print(\"Experiment completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "class BaselineScheduler:\n",
    "    def __init__(self, num_nodes=100):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.peak_hour_cost = 2.5\n",
    "        self.off_peak_cost = 0.6\n",
    "    \n",
    "    def run_simulation(self, jobs):\n",
    "        \"\"\"Run FCFS scheduling simulation\"\"\"\n",
    "        current_time = pd.Timestamp.now().replace(hour=0, minute=0)\n",
    "        node_status = [True] * self.num_nodes\n",
    "        running_jobs = []\n",
    "        total_cost = 0\n",
    "        peak_consumption = 0\n",
    "        total_consumption = 0\n",
    "        job_metrics = []\n",
    "        \n",
    "        # Process each job in order\n",
    "        for job in jobs:\n",
    "            # Wait until enough nodes are available\n",
    "            while sum(node_status) < job['num_nodes_alloc']:\n",
    "                # Find next job completion\n",
    "                next_completion = min([end_time for _, _, end_time in running_jobs])\n",
    "                current_time = next_completion\n",
    "                \n",
    "                # Update node status\n",
    "                new_running = []\n",
    "                for run_job, nodes, end_time in running_jobs:\n",
    "                    if end_time > current_time:\n",
    "                        new_running.append((run_job, nodes, end_time))\n",
    "                    else:\n",
    "                        for node in nodes:\n",
    "                            node_status[node] = True\n",
    "                running_jobs = new_running\n",
    "            \n",
    "            # Schedule job\n",
    "            allocated_nodes = []\n",
    "            for i, available in enumerate(node_status):\n",
    "                if available and len(allocated_nodes) < job['num_nodes_alloc']:\n",
    "                    allocated_nodes.append(i)\n",
    "                    node_status[i] = False\n",
    "            \n",
    "            # Calculate power and cost\n",
    "            power = job['mean_node_power']\n",
    "            duration_hours = job['run_time'] / 3600\n",
    "            is_peak = 8 <= current_time.hour < 20\n",
    "            price = self.peak_hour_cost if is_peak else self.off_peak_cost\n",
    "            \n",
    "            energy_used = power * duration_hours\n",
    "            cost = energy_used * price\n",
    "            \n",
    "            if is_peak:\n",
    "                peak_consumption += energy_used\n",
    "            total_consumption += energy_used\n",
    "            total_cost += cost\n",
    "            \n",
    "            # Record job completion\n",
    "            end_time = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "            running_jobs.append((job, allocated_nodes, end_time))\n",
    "            \n",
    "            job_metrics.append({\n",
    "                'job_id': job['job_id'],\n",
    "                'start_time': current_time,\n",
    "                'end_time': end_time,\n",
    "                'is_peak': is_peak,\n",
    "                'energy_used': energy_used,\n",
    "                'cost': cost\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'total_cost': total_cost,\n",
    "            'total_consumption': total_consumption,\n",
    "            'peak_consumption': peak_consumption,\n",
    "            'peak_ratio': peak_consumption / total_consumption,\n",
    "            'job_metrics': job_metrics\n",
    "        }\n",
    "\n",
    "def compare_schedulers():\n",
    "    \"\"\"Compare power-aware vs baseline scheduling\"\"\"\n",
    "    # Prepare test jobs\n",
    "    test_jobs = prepare_job_queue(gnn_test_data, n_jobs=100)\n",
    "    \n",
    "    # Run baseline scheduler\n",
    "    print(\"Running baseline scheduler...\")\n",
    "    baseline = BaselineScheduler()\n",
    "    baseline_results = baseline.run_simulation(test_jobs)\n",
    "    \n",
    "    # Run power-aware scheduler\n",
    "    print(\"\\nRunning power-aware scheduler...\")\n",
    "    env = HPCEnvironment(\n",
    "        gnn_model=model,\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        num_nodes=100\n",
    "    )\n",
    "    \n",
    "    state = env.reset()\n",
    "    power_aware_metrics = []\n",
    "    job_queue = test_jobs.copy()\n",
    "    \n",
    "    while len(job_queue) > 0:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action_probs = actor(state_tensor)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        current_job = job_queue[0]\n",
    "        next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "        \n",
    "        if info['scheduled']:\n",
    "            job_queue.pop(0)\n",
    "            if 'energy_cost' in info:\n",
    "                power_aware_metrics.append(info)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(\"Baseline Scheduler:\")\n",
    "    print(f\"Total Cost: ${baseline_results['total_cost']:.2f}\")\n",
    "    print(f\"Total Energy: {baseline_results['total_consumption']:.2f} kWh\")\n",
    "    print(f\"Peak Ratio: {baseline_results['peak_ratio']*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nPower-Aware Scheduler:\")\n",
    "    print(f\"Total Cost: ${env.total_cost:.2f}\")\n",
    "    print(f\"Total Energy: {env.energy_consumption:.2f} kWh\")\n",
    "    print(f\"Peak Ratio: {(env.peak_consumption/env.energy_consumption)*100:.2f}%\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    cost_improvement = (baseline_results['total_cost'] - env.total_cost) / baseline_results['total_cost'] * 100\n",
    "    peak_improvement = (baseline_results['peak_ratio'] - env.peak_consumption/env.energy_consumption) * 100\n",
    "    \n",
    "    print(\"\\nImprovements:\")\n",
    "    print(f\"Cost Reduction: {cost_improvement:.2f}%\")\n",
    "    print(f\"Peak Ratio Reduction: {peak_improvement:.2f} percentage points\")\n",
    "    \n",
    "    return baseline_results, env\n",
    "\n",
    "# Run comparison\n",
    "baseline_results, power_aware_env = compare_schedulers()\n",
    "\n",
    "\n",
    "class BaselineScheduler:\n",
    "    def __init__(self, num_nodes=100):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.peak_hour_cost = 2.5\n",
    "        self.off_peak_cost = 0.6\n",
    "        \n",
    "    def run_simulation(self, jobs):\n",
    "        current_time = pd.Timestamp.now().replace(hour=0, minute=0)\n",
    "        node_status = [True] * self.num_nodes\n",
    "        running_jobs = []\n",
    "        total_cost = 0\n",
    "        peak_consumption = 0\n",
    "        total_consumption = 0\n",
    "        scheduled_jobs = []\n",
    "        \n",
    "        print(f\"Starting simulation at {current_time}\")\n",
    "        \n",
    "        for job_idx, job in enumerate(jobs):\n",
    "            print(f\"\\nProcessing job {job_idx}\")\n",
    "            # Calculate power and time\n",
    "            power = job['mean_node_power']\n",
    "            duration_hours = job['run_time'] / 3600\n",
    "            \n",
    "            # Check if it's peak hours\n",
    "            is_peak = 8 <= current_time.hour < 20\n",
    "            price = self.peak_hour_cost if is_peak else self.off_peak_cost\n",
    "            \n",
    "            print(f\"Time: {current_time.hour}:00, Peak hour: {is_peak}\")\n",
    "            \n",
    "            energy_used = power * duration_hours\n",
    "            cost = energy_used * price\n",
    "            \n",
    "            if is_peak:\n",
    "                peak_consumption += energy_used\n",
    "            total_consumption += energy_used\n",
    "            total_cost += cost\n",
    "            \n",
    "            scheduled_jobs.append({\n",
    "                'job_id': job_idx,\n",
    "                'start_time': current_time,\n",
    "                'is_peak': is_peak,\n",
    "                'energy_used': energy_used,\n",
    "                'cost': cost\n",
    "            })\n",
    "            \n",
    "            # Move time forward\n",
    "            current_time += pd.Timedelta(minutes=30)\n",
    "        \n",
    "        print(\"\\nSimulation complete\")\n",
    "        return {\n",
    "            'total_cost': total_cost,\n",
    "            'total_consumption': total_consumption,\n",
    "            'peak_consumption': peak_consumption,\n",
    "            'peak_ratio': peak_consumption / (total_consumption + 1e-8),\n",
    "            'scheduled_jobs': scheduled_jobs\n",
    "        }\n",
    "\n",
    "def run_power_aware_simulation(jobs, env, actor):\n",
    "    print(\"\\nRunning power-aware simulation...\")\n",
    "    state = env.reset()\n",
    "    metrics = []\n",
    "    job_queue = jobs.copy()\n",
    "    current_time = pd.Timestamp.now().replace(hour=0, minute=0)\n",
    "    \n",
    "    while len(job_queue) > 0:\n",
    "        current_job = job_queue[0]\n",
    "        \n",
    "        # Get current hour and peak status\n",
    "        is_peak = 8 <= current_time.hour < 20\n",
    "        print(f\"\\nTime: {current_time.hour}:00, Peak hour: {is_peak}\")\n",
    "        print(f\"Job power: {current_job['mean_node_power']:.2f}, Duration: {current_job['run_time']/3600:.2f}h\")\n",
    "        \n",
    "        # Get action from policy\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action_probs = actor(state_tensor)\n",
    "        probs = action_probs.detach().numpy()\n",
    "        print(f\"Action probabilities - Schedule: {probs[0]:.2f}, Wait: {probs[1]:.2f}\")\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, info = env.step(action.item(), current_job)\n",
    "        \n",
    "        if info['scheduled']:\n",
    "            print(\"Job scheduled\")\n",
    "            job_queue.pop(0)\n",
    "            metrics.append(info)\n",
    "        else:\n",
    "            print(\"Job delayed\")\n",
    "        \n",
    "        current_time += pd.Timedelta(minutes=30)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return env, metrics\n",
    "\n",
    "def compare_schedulers(test_jobs, max_jobs=20):\n",
    "    # Run baseline\n",
    "    print(f\"\\nTesting with first {max_jobs} jobs...\")\n",
    "    test_subset = test_jobs[:max_jobs]\n",
    "    \n",
    "    print(\"\\nRunning baseline scheduler...\")\n",
    "    baseline = BaselineScheduler()\n",
    "    baseline_results = baseline.run_simulation(test_subset)\n",
    "    \n",
    "    # Run power-aware\n",
    "    env = HPCEnvironment(\n",
    "        gnn_model=model,\n",
    "        feature_scaler=feature_scaler,\n",
    "        power_scaler=power_scaler,\n",
    "        num_nodes=100\n",
    "    )\n",
    "    \n",
    "    power_aware_env, power_metrics = run_power_aware_simulation(test_subset, env, actor)\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"\\nDetailed Results Comparison:\")\n",
    "    print(\"\\nBaseline Scheduler:\")\n",
    "    print(f\"Total Cost: ${baseline_results['total_cost']:.2f}\")\n",
    "    print(f\"Total Energy: {baseline_results['total_consumption']:.2f} kWh\")\n",
    "    print(f\"Peak Energy: {baseline_results['peak_consumption']:.2f} kWh\")\n",
    "    print(f\"Peak Ratio: {baseline_results['peak_ratio']*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nPower-Aware Scheduler:\")\n",
    "    print(f\"Total Cost: ${power_aware_env.total_cost:.2f}\")\n",
    "    print(f\"Total Energy: {power_aware_env.energy_consumption:.2f} kWh\")\n",
    "    print(f\"Peak Energy: {power_aware_env.peak_consumption:.2f} kWh\")\n",
    "    print(f\"Peak Ratio: {(power_aware_env.peak_consumption/power_aware_env.energy_consumption)*100:.2f}%\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    cost_improvement = (baseline_results['total_cost'] - power_aware_env.total_cost) / baseline_results['total_cost'] * 100\n",
    "    peak_improvement = (baseline_results['peak_ratio'] - power_aware_env.peak_consumption/power_aware_env.energy_consumption) * 100\n",
    "    \n",
    "    print(\"\\nImprovements:\")\n",
    "    print(f\"Cost Reduction: {cost_improvement:.2f}%\")\n",
    "    print(f\"Peak Ratio Reduction: {peak_improvement:.2f} percentage points\")\n",
    "    \n",
    "    return baseline_results, power_aware_env\n",
    "\n",
    "# Run comparison\n",
    "test_jobs = prepare_job_queue(gnn_test_data, n_jobs=100)\n",
    "baseline_results, power_aware_env = compare_schedulers(test_jobs, max_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
