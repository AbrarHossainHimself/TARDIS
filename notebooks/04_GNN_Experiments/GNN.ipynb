{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install torch torch_geometric networkx seaborn tqdm optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'clean_data.csv'\n",
    "\n",
    "# Read CSV and ensure datetime columns are parsed correctly\n",
    "time_columns = ['submit_time', 'eligible_time', 'start_time', 'end_time']\n",
    "df = pd.read_csv('clean_data.csv', parse_dates=time_columns)\n",
    "# Now 'df' contains the data from the second sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabulate import tabulate\n",
    "\n",
    "# # print(tabulate(df.head(5), headers='keys', tablefmt='psql'))\n",
    "# print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset into GNN, RL and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Create the subset\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     GNN_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_representative_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.35\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Validate the representation\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     validation_stats \u001b[38;5;241m=\u001b[39m validate_representation(df, GNN_dataset)\n",
      "Cell \u001b[0;32mIn[396], line 71\u001b[0m, in \u001b[0;36mcreate_representative_subset\u001b[0;34m(df, subset_size, random_state)\u001b[0m\n\u001b[1;32m     69\u001b[0m sampled_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stratum \u001b[38;5;129;01min\u001b[39;00m stratum_sizes\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m---> 71\u001b[0m     stratum_df \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstrat_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstratum\u001b[49m]\n\u001b[1;32m     72\u001b[0m     frac \u001b[38;5;241m=\u001b[39m sampling_fractions[stratum]\n\u001b[1;32m     73\u001b[0m     sampled_df \u001b[38;5;241m=\u001b[39m stratum_df\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m     74\u001b[0m         frac\u001b[38;5;241m=\u001b[39mfrac,\n\u001b[1;32m     75\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state\n\u001b[1;32m     76\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Needs to be changedimport pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def create_representative_subset(df, subset_size=0.35, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a representative subset of the data using stratified sampling.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        subset_size: Fraction of data to keep (default: 0.35)\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing the representative subset\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Create bins for numerical features\n",
    "    # Use KBinsDiscretizer for more robust binning\n",
    "    n_bins = 5  # Number of bins for each feature\n",
    "    \n",
    "    # Features to bin\n",
    "    numerical_features = {\n",
    "        'run_time': n_bins,\n",
    "        'num_cores_alloc': n_bins,\n",
    "        'mem_alloc': n_bins,\n",
    "        'mean_node_power': n_bins\n",
    "    }\n",
    "    \n",
    "    # Create binned features\n",
    "    for feature, bins in numerical_features.items():\n",
    "        # Handle skewed distributions with quantile-based binning\n",
    "        kbd = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='quantile')\n",
    "        \n",
    "        # Reshape for KBinsDiscretizer\n",
    "        binned_values = kbd.fit_transform(df[[feature]])\n",
    "        df[f'{feature}_bin'] = binned_values\n",
    "    \n",
    "    # 2. Create time-based bins to ensure temporal coverage\n",
    "    df['hour_of_day'] = pd.to_datetime(df['submit_time']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['submit_time']).dt.dayofweek\n",
    "    \n",
    "    # 3. Define stratification features\n",
    "    strat_features = [\n",
    "        'run_time_bin',\n",
    "        'num_cores_alloc_bin',\n",
    "        'mem_alloc_bin',\n",
    "        'mean_node_power_bin',\n",
    "        'hour_of_day',\n",
    "        'day_of_week',\n",
    "        'job_state'\n",
    "    ]\n",
    "    \n",
    "    # 4. Create a composite key for stratification\n",
    "    df['strat_key'] = df[strat_features].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    # 5. Calculate desired number of samples\n",
    "    n_samples = int(len(df) * subset_size)\n",
    "    \n",
    "    # 6. Calculate sampling fractions for each stratum\n",
    "    stratum_sizes = df['strat_key'].value_counts()\n",
    "    sampling_fractions = (n_samples * stratum_sizes / len(df)) / stratum_sizes\n",
    "    # Cap sampling fractions at 1.0\n",
    "    sampling_fractions = sampling_fractions.clip(upper=1.0)\n",
    "    \n",
    "    # 7. Sample from each stratum\n",
    "    sampled_dfs = []\n",
    "    for stratum in stratum_sizes.index:\n",
    "        stratum_df = df[df['strat_key'] == stratum]\n",
    "        frac = sampling_fractions[stratum]\n",
    "        sampled_df = stratum_df.sample(\n",
    "            frac=frac,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        sampled_dfs.append(sampled_df)\n",
    "    \n",
    "    # 8. Combine samples\n",
    "    subset_df = pd.concat(sampled_dfs)\n",
    "    \n",
    "    # 9. Clean up temporary columns\n",
    "    subset_df = subset_df.drop(columns=[\n",
    "        'run_time_bin', 'num_cores_alloc_bin', 'mem_alloc_bin',\n",
    "        'mean_node_power_bin', 'strat_key'\n",
    "    ])\n",
    "    \n",
    "    return subset_df\n",
    "\n",
    "def validate_representation(original_df, subset_df):\n",
    "    \"\"\"\n",
    "    Validate how well the subset represents the original data.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Numerical features to check\n",
    "    numerical_features = [\n",
    "        'run_time', 'num_cores_alloc', 'mem_alloc', \n",
    "        'mean_node_power', 'mean_cpu_power', 'mean_mem_power'\n",
    "    ]\n",
    "    \n",
    "    # Calculate statistics for numerical features\n",
    "    for feature in numerical_features:\n",
    "        orig_stats = original_df[feature].describe()\n",
    "        subset_stats = subset_df[feature].describe()\n",
    "        \n",
    "        stats[feature] = {\n",
    "            'original_mean': orig_stats['mean'],\n",
    "            'subset_mean': subset_stats['mean'],\n",
    "            'original_std': orig_stats['std'],\n",
    "            'subset_std': subset_stats['std'],\n",
    "            'mean_diff_pct': ((subset_stats['mean'] - orig_stats['mean']) / orig_stats['mean']) * 100\n",
    "        }\n",
    "    \n",
    "    # Calculate categorical distribution differences\n",
    "    categorical_features = ['job_state', 'state_reason']\n",
    "    for feature in categorical_features:\n",
    "        orig_dist = original_df[feature].value_counts(normalize=True)\n",
    "        subset_dist = subset_df[feature].value_counts(normalize=True)\n",
    "        \n",
    "        # Calculate Jensen-Shannon divergence\n",
    "        stats[feature] = {\n",
    "            'original_distribution': orig_dist,\n",
    "            'subset_distribution': subset_dist\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the subset\n",
    "    GNN_dataset = create_representative_subset(df, subset_size=0.35)\n",
    "    \n",
    "    # Validate the representation\n",
    "    validation_stats = validate_representation(df, GNN_dataset)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"Subset size: {len(GNN_dataset)}\")\n",
    "    print(f\"Subset percentage: {(len(GNN_dataset) / len(df)) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    for feature, stats in validation_stats.items():\n",
    "        if isinstance(stats, dict) and 'mean_diff_pct' in stats:\n",
    "            print(f\"\\n{feature}:\")\n",
    "            print(f\"Mean difference: {stats['mean_diff_pct']:.2f}%\")\n",
    "            print(f\"Original mean: {stats['original_mean']:.2f}\")\n",
    "            print(f\"Subset mean: {stats['subset_mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191315 entries, 0 to 191314\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   job_id           191315 non-null  int64         \n",
      " 1   job_state        191315 non-null  object        \n",
      " 2   submit_time      191315 non-null  datetime64[ns]\n",
      " 3   eligible_time    191315 non-null  datetime64[ns]\n",
      " 4   start_time       191315 non-null  datetime64[ns]\n",
      " 5   end_time         191315 non-null  datetime64[ns]\n",
      " 6   run_time         191315 non-null  int64         \n",
      " 7   wait_time        191315 non-null  float64       \n",
      " 8   cores_per_task   191315 non-null  int64         \n",
      " 9   shared           191315 non-null  int64         \n",
      " 10  partition        191315 non-null  int64         \n",
      " 11  priority         191315 non-null  int64         \n",
      " 12  num_tasks        191315 non-null  float64       \n",
      " 13  state_reason     191315 non-null  object        \n",
      " 14  num_cores_alloc  191315 non-null  int64         \n",
      " 15  num_nodes_alloc  191315 non-null  int64         \n",
      " 16  num_gpus_alloc   191315 non-null  int64         \n",
      " 17  mem_alloc        191315 non-null  int64         \n",
      " 18  mean_node_power  191315 non-null  float64       \n",
      " 19  mean_cpu_power   191315 non-null  float64       \n",
      " 20  mean_mem_power   191315 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(5), int64(10), object(2)\n",
      "memory usage: 30.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>submit_time</th>\n",
       "      <th>eligible_time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>run_time</th>\n",
       "      <th>wait_time</th>\n",
       "      <th>cores_per_task</th>\n",
       "      <th>shared</th>\n",
       "      <th>partition</th>\n",
       "      <th>priority</th>\n",
       "      <th>num_tasks</th>\n",
       "      <th>num_cores_alloc</th>\n",
       "      <th>num_nodes_alloc</th>\n",
       "      <th>num_gpus_alloc</th>\n",
       "      <th>mem_alloc</th>\n",
       "      <th>mean_node_power</th>\n",
       "      <th>mean_cpu_power</th>\n",
       "      <th>mean_mem_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315</td>\n",
       "      <td>191315</td>\n",
       "      <td>191315</td>\n",
       "      <td>191315</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>1.913150e+05</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "      <td>191315.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95658.000000</td>\n",
       "      <td>2020-08-08 09:56:55.977790720</td>\n",
       "      <td>2020-08-08 08:44:01.693646336</td>\n",
       "      <td>2020-08-08 11:59:31.200783872</td>\n",
       "      <td>2020-08-08 13:33:06.483469568</td>\n",
       "      <td>5615.282686</td>\n",
       "      <td>5615.282686</td>\n",
       "      <td>15.597376</td>\n",
       "      <td>0.829517</td>\n",
       "      <td>1.864435e+05</td>\n",
       "      <td>0.992928</td>\n",
       "      <td>12.852698</td>\n",
       "      <td>107.315872</td>\n",
       "      <td>1.173761</td>\n",
       "      <td>4.127314</td>\n",
       "      <td>213.540993</td>\n",
       "      <td>841.263481</td>\n",
       "      <td>142.347877</td>\n",
       "      <td>44.559368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2020-05-05 15:55:59</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>2020-05-05 15:56:00</td>\n",
       "      <td>2020-05-06 08:52:54</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.666667</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47829.500000</td>\n",
       "      <td>2020-06-24 15:58:30</td>\n",
       "      <td>2020-06-24 15:58:30</td>\n",
       "      <td>2020-06-24 15:58:33</td>\n",
       "      <td>2020-06-24 16:43:21</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.286500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>95658.000000</td>\n",
       "      <td>2020-08-19 15:48:11</td>\n",
       "      <td>2020-08-19 16:36:37</td>\n",
       "      <td>2020-08-19 16:36:46</td>\n",
       "      <td>2020-08-19 19:11:31</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.866030e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>102.780488</td>\n",
       "      <td>37.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>143486.500000</td>\n",
       "      <td>2020-09-25 08:08:56</td>\n",
       "      <td>2020-09-25 12:54:46</td>\n",
       "      <td>2020-09-25 14:25:17.500000</td>\n",
       "      <td>2020-09-25 17:04:37.500000</td>\n",
       "      <td>2032.000000</td>\n",
       "      <td>2032.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.680450e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.852698</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>174.011173</td>\n",
       "      <td>40.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>191315.000000</td>\n",
       "      <td>2020-10-12 23:50:26</td>\n",
       "      <td>2020-10-12 23:50:26</td>\n",
       "      <td>2020-10-12 23:51:21</td>\n",
       "      <td>2020-10-13 05:15:54</td>\n",
       "      <td>125311.000000</td>\n",
       "      <td>125311.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.306973e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>648.000000</td>\n",
       "      <td>20736.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>648.000000</td>\n",
       "      <td>38475.000000</td>\n",
       "      <td>2440.526316</td>\n",
       "      <td>21683.333333</td>\n",
       "      <td>5848.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>55228.027712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16328.279164</td>\n",
       "      <td>16328.279164</td>\n",
       "      <td>17.354117</td>\n",
       "      <td>0.376058</td>\n",
       "      <td>9.496610e+04</td>\n",
       "      <td>0.083798</td>\n",
       "      <td>24.143125</td>\n",
       "      <td>106.953631</td>\n",
       "      <td>0.732607</td>\n",
       "      <td>3.212431</td>\n",
       "      <td>190.284860</td>\n",
       "      <td>369.343803</td>\n",
       "      <td>118.132777</td>\n",
       "      <td>25.982101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              job_id                    submit_time  \\\n",
       "count  191315.000000                         191315   \n",
       "mean    95658.000000  2020-08-08 09:56:55.977790720   \n",
       "min         1.000000            2020-05-05 15:55:59   \n",
       "25%     47829.500000            2020-06-24 15:58:30   \n",
       "50%     95658.000000            2020-08-19 15:48:11   \n",
       "75%    143486.500000            2020-09-25 08:08:56   \n",
       "max    191315.000000            2020-10-12 23:50:26   \n",
       "std     55228.027712                            NaN   \n",
       "\n",
       "                       eligible_time                     start_time  \\\n",
       "count                         191315                         191315   \n",
       "mean   2020-08-08 08:44:01.693646336  2020-08-08 11:59:31.200783872   \n",
       "min              1970-01-01 00:00:00            2020-05-05 15:56:00   \n",
       "25%              2020-06-24 15:58:30            2020-06-24 15:58:33   \n",
       "50%              2020-08-19 16:36:37            2020-08-19 16:36:46   \n",
       "75%              2020-09-25 12:54:46     2020-09-25 14:25:17.500000   \n",
       "max              2020-10-12 23:50:26            2020-10-12 23:51:21   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "                            end_time       run_time      wait_time  \\\n",
       "count                         191315  191315.000000  191315.000000   \n",
       "mean   2020-08-08 13:33:06.483469568    5615.282686    5615.282686   \n",
       "min              2020-05-06 08:52:54       1.000000       1.000000   \n",
       "25%              2020-06-24 16:43:21       7.000000       7.000000   \n",
       "50%              2020-08-19 19:11:31     146.000000     146.000000   \n",
       "75%       2020-09-25 17:04:37.500000    2032.000000    2032.000000   \n",
       "max              2020-10-13 05:15:54  125311.000000  125311.000000   \n",
       "std                              NaN   16328.279164   16328.279164   \n",
       "\n",
       "       cores_per_task         shared     partition       priority  \\\n",
       "count   191315.000000  191315.000000  1.913150e+05  191315.000000   \n",
       "mean        15.597376       0.829517  1.864435e+05       0.992928   \n",
       "min          1.000000       0.000000  0.000000e+00       0.000000   \n",
       "25%          1.000000       1.000000  9.286500e+04       1.000000   \n",
       "50%          8.000000       1.000000  1.866030e+05       1.000000   \n",
       "75%         32.000000       1.000000  2.680450e+05       1.000000   \n",
       "max        128.000000       1.000000  1.306973e+06       1.000000   \n",
       "std         17.354117       0.376058  9.496610e+04       0.083798   \n",
       "\n",
       "           num_tasks  num_cores_alloc  num_nodes_alloc  num_gpus_alloc  \\\n",
       "count  191315.000000    191315.000000    191315.000000   191315.000000   \n",
       "mean       12.852698       107.315872         1.173761        4.127314   \n",
       "min         0.000000         4.000000         1.000000        0.000000   \n",
       "25%         4.000000        32.000000         1.000000        4.000000   \n",
       "50%         4.000000       128.000000         1.000000        4.000000   \n",
       "75%        12.852698       128.000000         1.000000        4.000000   \n",
       "max       648.000000     20736.000000       162.000000      648.000000   \n",
       "std        24.143125       106.953631         0.732607        3.212431   \n",
       "\n",
       "           mem_alloc  mean_node_power  mean_cpu_power  mean_mem_power  \n",
       "count  191315.000000    191315.000000   191315.000000   191315.000000  \n",
       "mean      213.540993       841.263481      142.347877       44.559368  \n",
       "min         0.000000        20.000000       22.666667       28.000000  \n",
       "25%       118.000000       560.000000       78.000000       36.000000  \n",
       "50%       237.000000       740.000000      102.780488       37.157895  \n",
       "75%       237.000000       940.000000      174.011173       40.428571  \n",
       "max     38475.000000      2440.526316    21683.333333     5848.000000  \n",
       "std       190.284860       369.343803      118.132777       25.982101  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# df['mean_node_power'].plot.hist(bins=50)\n",
    "# plt.title('Distribution of Mean Node Power')\n",
    "# plt.xlabel('Mean Node Power')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineered Features Summary:\n",
      "              job_id                    submit_time  \\\n",
      "count  191315.000000                         191315   \n",
      "mean    95658.000000  2020-08-08 09:56:55.977790720   \n",
      "min         1.000000            2020-05-05 15:55:59   \n",
      "25%     47829.500000            2020-06-24 15:58:30   \n",
      "50%     95658.000000            2020-08-19 15:48:11   \n",
      "75%    143486.500000            2020-09-25 08:08:56   \n",
      "max    191315.000000            2020-10-12 23:50:26   \n",
      "std     55228.027712                            NaN   \n",
      "\n",
      "                       eligible_time                     start_time  \\\n",
      "count                         191315                         191315   \n",
      "mean   2020-08-08 08:44:01.693646336  2020-08-08 11:59:31.200783872   \n",
      "min              1970-01-01 00:00:00            2020-05-05 15:56:00   \n",
      "25%              2020-06-24 15:58:30            2020-06-24 15:58:33   \n",
      "50%              2020-08-19 16:36:37            2020-08-19 16:36:46   \n",
      "75%              2020-09-25 12:54:46     2020-09-25 14:25:17.500000   \n",
      "max              2020-10-12 23:50:26            2020-10-12 23:51:21   \n",
      "std                              NaN                            NaN   \n",
      "\n",
      "                            end_time       run_time      wait_time  \\\n",
      "count                         191315  191315.000000  191315.000000   \n",
      "mean   2020-08-08 13:33:06.483469568    5615.282686    5615.282686   \n",
      "min              2020-05-06 08:52:54       1.000000       1.000000   \n",
      "25%              2020-06-24 16:43:21       7.000000       7.000000   \n",
      "50%              2020-08-19 19:11:31     146.000000     146.000000   \n",
      "75%       2020-09-25 17:04:37.500000    2032.000000    2032.000000   \n",
      "max              2020-10-13 05:15:54  125311.000000  125311.000000   \n",
      "std                              NaN   16328.279164   16328.279164   \n",
      "\n",
      "       cores_per_task         shared     partition  ...  mem_runtime_ratio  \\\n",
      "count   191315.000000  191315.000000  1.913150e+05  ...       1.913150e+05   \n",
      "mean        15.597376       0.829517  1.864435e+05  ...       1.338149e+06   \n",
      "min          1.000000       0.000000  0.000000e+00  ...       0.000000e+00   \n",
      "25%          1.000000       1.000000  9.286500e+04  ...       9.480000e+02   \n",
      "50%          8.000000       1.000000  1.866030e+05  ...       2.767600e+04   \n",
      "75%         32.000000       1.000000  2.680450e+05  ...       3.406760e+05   \n",
      "max        128.000000       1.000000  1.306973e+06  ...       8.295168e+07   \n",
      "std         17.354117       0.376058  9.496610e+04  ...       4.520991e+06   \n",
      "\n",
      "          is_gpu_job  is_large_memory  is_long_running  concurrent_jobs  \\\n",
      "count  191315.000000    191315.000000    191315.000000    191315.000000   \n",
      "mean        0.920053         0.145812         0.499888       135.922385   \n",
      "min         0.000000         0.000000         0.000000         1.000000   \n",
      "25%         1.000000         0.000000         0.000000        69.000000   \n",
      "50%         1.000000         0.000000         0.000000       113.000000   \n",
      "75%         1.000000         0.000000         1.000000       170.000000   \n",
      "max         1.000000         1.000000         1.000000       764.000000   \n",
      "std         0.271211         0.352918         0.500001        96.017691   \n",
      "\n",
      "       hourly_job_count  total_cores_requested  total_memory_requested  \\\n",
      "count     191315.000000           1.913150e+05            1.913150e+05   \n",
      "mean         143.393628           1.540255e+04            3.100084e+04   \n",
      "min            0.000000           8.000000e+00            0.000000e+00   \n",
      "25%           19.000000           3.296000e+03            9.794000e+03   \n",
      "50%           62.000000           1.036800e+04            2.190000e+04   \n",
      "75%          183.000000           2.073600e+04            3.957900e+04   \n",
      "max         1856.000000           3.048192e+06            5.655825e+06   \n",
      "std          216.195638           1.844204e+04            3.483634e+04   \n",
      "\n",
      "       normalized_runtime  normalized_memory  \n",
      "count       191315.000000      191315.000000  \n",
      "mean             1.000000           1.000000  \n",
      "min              0.000178           0.000000  \n",
      "25%              0.001247           0.552587  \n",
      "50%              0.026000           1.109857  \n",
      "75%              0.361870           1.109857  \n",
      "max             22.316063         180.176178  \n",
      "std              2.907829           0.891093  \n",
      "\n",
      "[8 rows x 37 columns]\n",
      "\n",
      "Graph Structure Summary:\n",
      "Number of nodes: 191315\n",
      "Number of edges: 1747088\n",
      "Average edge weight: 0.9567\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191315 entries, 0 to 191314\n",
      "Data columns (total 39 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   job_id                  191315 non-null  int64         \n",
      " 1   job_state               191315 non-null  object        \n",
      " 2   submit_time             191315 non-null  datetime64[ns]\n",
      " 3   eligible_time           191315 non-null  datetime64[ns]\n",
      " 4   start_time              191315 non-null  datetime64[ns]\n",
      " 5   end_time                191315 non-null  datetime64[ns]\n",
      " 6   run_time                191315 non-null  int64         \n",
      " 7   wait_time               191315 non-null  float64       \n",
      " 8   cores_per_task          191315 non-null  int64         \n",
      " 9   shared                  191315 non-null  int64         \n",
      " 10  partition               191315 non-null  int64         \n",
      " 11  priority                191315 non-null  int64         \n",
      " 12  num_tasks               191315 non-null  float64       \n",
      " 13  state_reason            191315 non-null  object        \n",
      " 14  num_cores_alloc         191315 non-null  int64         \n",
      " 15  num_nodes_alloc         191315 non-null  int64         \n",
      " 16  num_gpus_alloc          191315 non-null  int64         \n",
      " 17  mem_alloc               191315 non-null  int64         \n",
      " 18  mean_node_power         191315 non-null  float64       \n",
      " 19  mean_cpu_power          191315 non-null  float64       \n",
      " 20  mean_mem_power          191315 non-null  float64       \n",
      " 21  cores_per_node          191315 non-null  float64       \n",
      " 22  mem_per_core            191315 non-null  float64       \n",
      " 23  gpus_per_node           191315 non-null  float64       \n",
      " 24  queue_time              191315 non-null  float64       \n",
      " 25  hour_of_day             191315 non-null  int32         \n",
      " 26  day_of_week             191315 non-null  int32         \n",
      " 27  is_weekend              191315 non-null  int64         \n",
      " 28  runtime_per_core        191315 non-null  float64       \n",
      " 29  mem_runtime_ratio       191315 non-null  int64         \n",
      " 30  is_gpu_job              191315 non-null  int64         \n",
      " 31  is_large_memory         191315 non-null  int64         \n",
      " 32  is_long_running         191315 non-null  int64         \n",
      " 33  concurrent_jobs         191315 non-null  int64         \n",
      " 34  hourly_job_count        191315 non-null  int64         \n",
      " 35  total_cores_requested   191315 non-null  int64         \n",
      " 36  total_memory_requested  191315 non-null  int64         \n",
      " 37  normalized_runtime      191315 non-null  float64       \n",
      " 38  normalized_memory       191315 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(12), int32(2), int64(19), object(2)\n",
      "memory usage: 55.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_engineered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest Date: 2020-05-05 15:55:59\n",
      "Latest Date: 2020-10-12 23:50:26\n"
     ]
    }
   ],
   "source": [
    "# # Get earliest and latest dates\n",
    "# earliest_date = df['submit_time'].min()\n",
    "# latest_date = df['submit_time'].max()\n",
    "\n",
    "# print(\"Earliest Date:\", earliest_date)\n",
    "# print(\"Latest Date:\", latest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to filter the dataset by a user-specified date range\n",
    "# def filter_by_date_range(df, start_date, end_date, date_column='submit_time'):\n",
    "#     \"\"\"\n",
    "#     Filters the DataFrame based on the specified date range.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df (pd.DataFrame): The DataFrame to filter.\n",
    "#     - start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "#     - end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "#     - date_column (str): The column name containing datetime values (default: 'submit_time').\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: A filtered DataFrame containing rows within the date range.\n",
    "#     \"\"\"\n",
    "#     # Ensure the date column is in datetime format\n",
    "#     df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "#     # Filter the DataFrame\n",
    "#     return df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training date range\n",
    "# start_date = '2020-05-01'\n",
    "# end_date = '2020-05-31'\n",
    "# Training date range\n",
    "# start_date = '2020-06-01'\n",
    "# end_date = '2020-06-30'\n",
    "# date_filtered_df = filter_by_date_range(df, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63415 entries, 30421 to 124863\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   job_id           63415 non-null  int64         \n",
      " 1   job_state        63415 non-null  object        \n",
      " 2   submit_time      63415 non-null  datetime64[ns]\n",
      " 3   eligible_time    63415 non-null  datetime64[ns]\n",
      " 4   start_time       63415 non-null  datetime64[ns]\n",
      " 5   end_time         63415 non-null  datetime64[ns]\n",
      " 6   run_time         63415 non-null  int64         \n",
      " 7   wait_time        63415 non-null  float64       \n",
      " 8   cores_per_task   63415 non-null  int64         \n",
      " 9   shared           63415 non-null  int64         \n",
      " 10  partition        63415 non-null  int64         \n",
      " 11  priority         63415 non-null  int64         \n",
      " 12  num_tasks        63415 non-null  float64       \n",
      " 13  state_reason     63415 non-null  object        \n",
      " 14  num_cores_alloc  63415 non-null  int64         \n",
      " 15  num_nodes_alloc  63415 non-null  int64         \n",
      " 16  num_gpus_alloc   63415 non-null  int64         \n",
      " 17  mem_alloc        63415 non-null  int64         \n",
      " 18  mean_node_power  63415 non-null  float64       \n",
      " 19  mean_cpu_power   63415 non-null  float64       \n",
      " 20  mean_mem_power   63415 non-null  float64       \n",
      " 21  hour_of_day      63415 non-null  int32         \n",
      " 22  day_of_week      63415 non-null  int32         \n",
      "dtypes: datetime64[ns](4), float64(5), int32(2), int64(10), object(2)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "GNN_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'hour_of_day', \"day_of_week\"\n",
    "]\n",
    "\n",
    "GNN_dataset.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNN_dataset.to_csv('GNN_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63415 entries, 0 to 63414\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   job_id           63415 non-null  int64         \n",
      " 1   job_state        63415 non-null  object        \n",
      " 2   submit_time      63415 non-null  datetime64[ns]\n",
      " 3   eligible_time    63415 non-null  datetime64[ns]\n",
      " 4   start_time       63415 non-null  datetime64[ns]\n",
      " 5   end_time         63415 non-null  datetime64[ns]\n",
      " 6   run_time         63415 non-null  int64         \n",
      " 7   wait_time        63415 non-null  float64       \n",
      " 8   cores_per_task   63415 non-null  int64         \n",
      " 9   shared           63415 non-null  int64         \n",
      " 10  partition        63415 non-null  int64         \n",
      " 11  priority         63415 non-null  int64         \n",
      " 12  num_tasks        63415 non-null  float64       \n",
      " 13  state_reason     63415 non-null  object        \n",
      " 14  num_cores_alloc  63415 non-null  int64         \n",
      " 15  num_nodes_alloc  63415 non-null  int64         \n",
      " 16  num_gpus_alloc   63415 non-null  int64         \n",
      " 17  mem_alloc        63415 non-null  int64         \n",
      " 18  mean_node_power  63415 non-null  float64       \n",
      " 19  mean_cpu_power   63415 non-null  float64       \n",
      " 20  mean_mem_power   63415 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(5), int64(10), object(2)\n",
      "memory usage: 10.2+ MB\n"
     ]
    }
   ],
   "source": [
    "GNN_df = pd.read_csv('GNN_dataset.csv', parse_dates=time_columns)\n",
    "GNN_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split GNN dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Split Summary:\n",
      "Total samples: 50732\n",
      "Training samples: 50732 (100.0%)\n",
      "Test samples: 12683 (25.0%)\n",
      "\n",
      "Training set time range:\n",
      "Start: 2020-05-05 16:01:00\n",
      "End: 2020-10-02 04:20:19\n",
      "\n",
      "Test set time range:\n",
      "Start: 2020-10-02 04:20:38\n",
      "End: 2020-10-12 23:50:26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def split_gnn_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and test sets for GNN while preserving temporal ordering.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        test_size: Proportion of data to use for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_df, test_df: Training and test DataFrames\n",
    "    \"\"\"\n",
    "    # Sort by submit time to maintain temporal ordering\n",
    "    df = df.sort_values('submit_time').copy()\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_df = df.iloc[:split_idx].copy()\n",
    "    test_df = df.iloc[split_idx:].copy()\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Split the data\n",
    "    GNN_train_df, GNN_test_df = split_gnn_data(GNN_df, test_size=0.2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nDataset Split Summary:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Training samples: {len(GNN_train_df)} ({len(GNN_train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Test samples: {len(GNN_test_df)} ({len(GNN_test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nTraining set time range:\")\n",
    "    print(f\"Start: {GNN_train_df['submit_time'].min()}\")\n",
    "    print(f\"End: {GNN_train_df['submit_time'].max()}\")\n",
    "    \n",
    "    print(\"\\nTest set time range:\")\n",
    "    print(f\"Start: {GNN_test_df['submit_time'].min()}\")\n",
    "    print(f\"End: {GNN_test_df['submit_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50732 entries, 33494 to 8653\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   job_id           50732 non-null  int64         \n",
      " 1   job_state        50732 non-null  object        \n",
      " 2   submit_time      50732 non-null  datetime64[ns]\n",
      " 3   eligible_time    50732 non-null  datetime64[ns]\n",
      " 4   start_time       50732 non-null  datetime64[ns]\n",
      " 5   end_time         50732 non-null  datetime64[ns]\n",
      " 6   run_time         50732 non-null  int64         \n",
      " 7   wait_time        50732 non-null  float64       \n",
      " 8   cores_per_task   50732 non-null  int64         \n",
      " 9   shared           50732 non-null  int64         \n",
      " 10  partition        50732 non-null  int64         \n",
      " 11  priority         50732 non-null  int64         \n",
      " 12  num_tasks        50732 non-null  float64       \n",
      " 13  state_reason     50732 non-null  object        \n",
      " 14  num_cores_alloc  50732 non-null  int64         \n",
      " 15  num_nodes_alloc  50732 non-null  int64         \n",
      " 16  num_gpus_alloc   50732 non-null  int64         \n",
      " 17  mem_alloc        50732 non-null  int64         \n",
      " 18  mean_node_power  50732 non-null  float64       \n",
      " 19  mean_cpu_power   50732 non-null  float64       \n",
      " 20  mean_mem_power   50732 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(5), int64(10), object(2)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "GNN_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# GNN_dataset['mean_node_power'].plot.hist(bins=50)\n",
    "# plt.title('Distribution of Mean Node Power')\n",
    "# plt.xlabel('Mean Node Power')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# print(tabulate(GNN_dataset.head(5), headers='keys', tablefmt='psql'))\n",
    "# # print(tabulate(df_filtered.head(), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_filtered_df.info()\n",
    "# # date_filtered_df.describe()\n",
    "# # print(date_filtered_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# GNN_train_df['mean_node_power'].plot.hist(bins=50)\n",
    "# plt.title('Distribution of Mean Node Power')\n",
    "# plt.xlabel('Mean Node Power')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_filtered_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN_dataset['wait_time'] = GNN_dataset['end_time'] - GNN_dataset['start_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['job_id', 'job_state', 'submit_time', 'eligible_time', 'start_time',\n",
      "       'end_time', 'run_time', 'wait_time', 'cores_per_task', 'shared',\n",
      "       'partition', 'priority', 'num_tasks', 'state_reason', 'num_cores_alloc',\n",
      "       'num_nodes_alloc', 'num_gpus_alloc', 'mem_alloc', 'mean_node_power',\n",
      "       'mean_cpu_power', 'mean_mem_power'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(GNN_train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to datetime\n",
    "GNN_train_df['submit_time'] = pd.to_datetime(GNN_train_df['submit_time'], unit='s')\n",
    "GNN_train_df['eligible_time'] = pd.to_datetime(GNN_train_df['eligible_time'], unit='s')\n",
    "GNN_train_df['start_time'] = pd.to_datetime(GNN_train_df['start_time'], unit='s')\n",
    "GNN_train_df['end_time'] = pd.to_datetime(GNN_train_df['end_time'], unit='s')\n",
    "\n",
    "# For 'wait_time', convert object to timedelta (assuming 'HH:MM:SS' format)\n",
    "# GNN_train_df['wait_time'] = pd.to_timedelta(GNN_train_df['wait_time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_drop = [\n",
    "#     'job_state_CANCELLED',\n",
    "#     'job_state_COMPLETED',\n",
    "#     'job_state_FAILED',\n",
    "#     'job_state_NODE_FAIL',\n",
    "#     'job_state_OUT_OF_MEMORY',\n",
    "#     'job_state_TIMEOUT',\n",
    "#     'state_reason_JobLaunchFailure',\n",
    "#     'state_reason_No reason',\n",
    "#     'state_reason_NodeDown',\n",
    "#     'state_reason_NonZeroExitCode',\n",
    "#     'state_reason_OutOfMemory',\n",
    "#     'state_reason_Prolog',\n",
    "#     'state_reason_TimeLimit'\n",
    "# ]\n",
    "\n",
    "# GNN_train_df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50732 entries, 33494 to 8653\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   job_id           50732 non-null  int64         \n",
      " 1   job_state        50732 non-null  object        \n",
      " 2   submit_time      50732 non-null  datetime64[ns]\n",
      " 3   eligible_time    50732 non-null  datetime64[ns]\n",
      " 4   start_time       50732 non-null  datetime64[ns]\n",
      " 5   end_time         50732 non-null  datetime64[ns]\n",
      " 6   run_time         50732 non-null  int64         \n",
      " 7   wait_time        50732 non-null  float64       \n",
      " 8   cores_per_task   50732 non-null  int64         \n",
      " 9   shared           50732 non-null  int64         \n",
      " 10  partition        50732 non-null  int64         \n",
      " 11  priority         50732 non-null  int64         \n",
      " 12  num_tasks        50732 non-null  float64       \n",
      " 13  state_reason     50732 non-null  object        \n",
      " 14  num_cores_alloc  50732 non-null  int64         \n",
      " 15  num_nodes_alloc  50732 non-null  int64         \n",
      " 16  num_gpus_alloc   50732 non-null  int64         \n",
      " 17  mem_alloc        50732 non-null  int64         \n",
      " 18  mean_node_power  50732 non-null  float64       \n",
      " 19  mean_cpu_power   50732 non-null  float64       \n",
      " 20  mean_mem_power   50732 non-null  float64       \n",
      "dtypes: datetime64[ns](4), float64(5), int64(10), object(2)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "GNN_train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with advanced Features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['submit_time', 'run_time', 'wait_time', 'cores_per_task', 'shared', 'priority', 'num_tasks', 'num_cores_alloc', 'num_nodes_alloc', 'num_gpus_alloc', 'mem_alloc', 'mean_cpu_power', 'mean_mem_power']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:   2%|▏         | 10/500 [00:02<01:49,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Training Loss: 0.3273, Validation Loss: 0.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:   4%|▍         | 20/500 [00:04<01:54,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500, Training Loss: 0.2958, Validation Loss: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:   6%|▌         | 30/500 [00:06<01:45,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500, Training Loss: 0.2781, Validation Loss: 0.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:   8%|▊         | 40/500 [00:09<01:43,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/500, Training Loss: 0.2612, Validation Loss: 0.2573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  10%|█         | 50/500 [00:11<01:39,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500, Training Loss: 0.2518, Validation Loss: 0.2508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  12%|█▏        | 60/500 [00:13<01:40,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500, Training Loss: 0.2432, Validation Loss: 0.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  14%|█▍        | 70/500 [00:15<01:35,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500, Training Loss: 0.2386, Validation Loss: 0.2419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  16%|█▌        | 80/500 [00:18<01:46,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500, Training Loss: 0.2346, Validation Loss: 0.2394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  18%|█▊        | 88/500 [00:20<01:36,  4.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[383], line 194\u001b[0m\n\u001b[1;32m    191\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Plot training and validation losses\u001b[39;00m\n\u001b[1;32m    197\u001b[0m plot_gnn_training_loss(train_losses, val_losses)\n",
      "Cell \u001b[0;32mIn[383], line 44\u001b[0m, in \u001b[0;36mtrain_gnn\u001b[0;34m(model, optimizer, train_data, val_data, epochs, scheduler, patience)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining GNN\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mHuberLoss()(out, train_data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     46\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[383], line 30\u001b[0m, in \u001b[0;36mGNNScheduler.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index))\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x, edge_index))\n\u001b[0;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x, edge_index))\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6(x, edge_index)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_qoqfrn4v.py:215\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_weight, size)\u001b[0m\n\u001b[1;32m    209\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage(\n\u001b[1;32m    210\u001b[0m     x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    211\u001b[0m     edge_weight\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    217\u001b[0m         hook_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    218\u001b[0m             x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    219\u001b[0m             edge_weight\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[1;32m    220\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/_jit_internal.py:103\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m                return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GNNScheduler(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNScheduler, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv5 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv6 = GCNConv(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.relu(self.conv4(x, edge_index))\n",
    "        x = self.relu(self.conv5(x, edge_index))\n",
    "        x = self.conv6(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=None, patience=20):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GNN\"):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        train_loss = nn.HuberLoss()(out, train_data.y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data)\n",
    "            val_loss = nn.HuberLoss()(val_out, val_data.y)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_gnn_scheduler.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_gnn_training_loss(train_losses, val_losses=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    if val_losses:\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.title('GNN Training and Validation Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig('gnn_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_job_graph(df, features, submit_time_weight=0.0):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply weight to submit_time\n",
    "    df['submit_time_weighted'] = df['submit_time'] * submit_time_weight\n",
    "    features_for_graph = features + ['submit_time_weighted']\n",
    "    \n",
    "    # Fill any remaining NaN values with 0\n",
    "    df[features_for_graph] = df[features_for_graph].fillna(0)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_graph = StandardScaler()\n",
    "    df[features_for_graph] = scaler_graph.fit_transform(df[features_for_graph])\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=5)\n",
    "    knn.fit(df[features_for_graph])\n",
    "    A = knn.kneighbors_graph(df[features_for_graph]).toarray()\n",
    "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
    "    \n",
    "    # Drop the temporary column\n",
    "    df.drop('submit_time_weighted', axis=1, inplace=True)\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "# Assuming date_filtered_df is your DataFrame after filtering\n",
    "df = GNN_train_df.copy()\n",
    "\n",
    "# Convert datetime columns to numerical representation (seconds since the earliest date)\n",
    "datetime_cols = ['submit_time', 'eligible_time', 'start_time', 'end_time']\n",
    "for col in datetime_cols:\n",
    "    df[col] = (df[col] - df[col].min()).dt.total_seconds()\n",
    "\n",
    "# Select only numeric columns, excluding job_id and target variables\n",
    "numeric_features = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "features = [col for col in numeric_features if col not in ['job_id', 'mean_node_power', 'partition',  'eligible_time', 'start_time', 'end_time']]\n",
    "print(\"Selected Features:\", features)\n",
    "\n",
    "# One-hot encode only state_reason\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_data = encoder.fit_transform(df[['state_reason']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data, \n",
    "    columns=encoder.get_feature_names_out(['state_reason']),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Join encoded state_reason with numeric features\n",
    "df_features = pd.concat([df[features], encoded_df], axis=1)\n",
    "\n",
    "# Update features list to include encoded state_reason columns\n",
    "features = list(df_features.columns)\n",
    "target = 'mean_node_power'\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_idx, val_idx = train_test_split(df.index, test_size=0.2, random_state=42)\n",
    "train_df = df_features.loc[train_idx]\n",
    "val_df = df_features.loc[val_idx]\n",
    "\n",
    "# Get target values\n",
    "train_y = df.loc[train_idx, target]\n",
    "val_y = df.loc[val_idx, target]\n",
    "\n",
    "# Scale features\n",
    "scaler_X = StandardScaler()\n",
    "train_df_scaled = pd.DataFrame(\n",
    "    scaler_X.fit_transform(train_df),\n",
    "    columns=features,\n",
    "    index=train_df.index\n",
    ")\n",
    "val_df_scaled = pd.DataFrame(\n",
    "    scaler_X.transform(val_df),\n",
    "    columns=features,\n",
    "    index=val_df.index\n",
    ")\n",
    "\n",
    "# Scale target\n",
    "scaler_y = StandardScaler()\n",
    "train_y_scaled = scaler_y.fit_transform(train_y.values.reshape(-1, 1))\n",
    "val_y_scaled = scaler_y.transform(val_y.values.reshape(-1, 1))\n",
    "\n",
    "# Create graph data\n",
    "train_G = create_job_graph(train_df_scaled, features)\n",
    "train_edge_index = train_G\n",
    "train_x = torch.tensor(train_df_scaled.values, dtype=torch.float)\n",
    "train_y = torch.tensor(train_y_scaled, dtype=torch.float)\n",
    "train_data = Data(x=train_x, edge_index=train_edge_index, y=train_y)\n",
    "\n",
    "val_G = create_job_graph(val_df_scaled, features)\n",
    "val_edge_index = val_G\n",
    "val_x = torch.tensor(val_df_scaled.values, dtype=torch.float)\n",
    "val_y = torch.tensor(val_y_scaled, dtype=torch.float)\n",
    "val_data = Data(x=val_x, edge_index=val_edge_index, y=val_y)\n",
    "\n",
    "# Initialize model, optimizer, and scheduler\n",
    "model = GNNScheduler(input_dim=len(features), hidden_dim=128, output_dim=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=scheduler, patience=20)\n",
    "\n",
    "# Plot training and validation losses\n",
    "plot_gnn_training_loss(train_losses, val_losses)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'gnn_scheduler.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22703/4114276315.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()).dt.total_seconds()\n",
      "/tmp/ipykernel_22703/4114276315.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()).dt.total_seconds()\n",
      "/tmp/ipykernel_22703/4114276315.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - df[col].min()).dt.total_seconds()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- state_reason_JobHeldUser\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[399], line 225\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, scaler_X, scaler_y, encoder, test_data\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m     model, scaler_X, scaler_y, encoder, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Print dataset sizes\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset sizes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[399], line 171\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[1;32m    165\u001b[0m train_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    166\u001b[0m     scaler_X\u001b[38;5;241m.\u001b[39mfit_transform(train_features),\n\u001b[1;32m    167\u001b[0m     columns\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    168\u001b[0m     index\u001b[38;5;241m=\u001b[39mtrain_features\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m val_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m--> 171\u001b[0m     \u001b[43mscaler_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_features\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    172\u001b[0m     columns\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    173\u001b[0m     index\u001b[38;5;241m=\u001b[39mval_features\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    175\u001b[0m test_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    176\u001b[0m     scaler_X\u001b[38;5;241m.\u001b[39mtransform(test_features),\n\u001b[1;32m    177\u001b[0m     columns\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    178\u001b[0m     index\u001b[38;5;241m=\u001b[39mtest_features\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m    179\u001b[0m )\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Scale target\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/utils/validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[1;32m   2836\u001b[0m     _estimator,\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m   2844\u001b[0m ):\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[1;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[1;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/sklearn/utils/validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- state_reason_JobHeldUser\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class GNNScheduler(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNScheduler, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv5 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv6 = GCNConv(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.relu(self.conv4(x, edge_index))\n",
    "        x = self.relu(self.conv5(x, edge_index))\n",
    "        x = self.conv6(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare data for GNN training.\"\"\"\n",
    "    # Convert datetime columns to numerical\n",
    "    datetime_cols = ['submit_time', 'eligible_time', 'start_time', 'end_time']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = (df[col] - df[col].min()).dt.total_seconds()\n",
    "    \n",
    "    # Select features\n",
    "    numeric_features = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "    features = [col for col in numeric_features \n",
    "               if col not in ['job_id', 'mean_node_power', 'partition', 'eligible_time', 'start_time', 'end_time']]\n",
    "    \n",
    "    # One-hot encode state_reason\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    encoded_data = encoder.fit_transform(df[['state_reason']])\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_data,\n",
    "        columns=encoder.get_feature_names_out(['state_reason']),\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    df_features = pd.concat([df[features], encoded_df], axis=1)\n",
    "    features = list(df_features.columns)\n",
    "    \n",
    "    return df_features, features, encoder\n",
    "\n",
    "def create_job_graph(df, features, n_neighbors=5):\n",
    "    \"\"\"Create graph structure using KNN.\"\"\"\n",
    "    # Scale features for graph creation\n",
    "    scaler_graph = StandardScaler()\n",
    "    df_scaled = scaler_graph.fit_transform(df)\n",
    "    \n",
    "    # Create KNN graph\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    knn.fit(df_scaled)\n",
    "    A = knn.kneighbors_graph(df_scaled).toarray()\n",
    "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "def train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=None, patience=20):\n",
    "    \"\"\"Train the GNN model with early stopping.\"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GNN\"):\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        train_loss = nn.HuberLoss()(out, train_data.y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data)\n",
    "            val_loss = nn.HuberLoss()(val_out, val_data.y)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss.item():.4f}, '\n",
    "                  f'Validation Loss: {val_loss.item():.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_gnn_scheduler.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plot training and validation losses.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main training pipeline\n",
    "def main():\n",
    "    df = df_engineered.copy()\n",
    "    # 1. Sort data by timestamp\n",
    "    df_sorted = df.sort_values('submit_time').copy()\n",
    "    \n",
    "    # 2. Split into train (70%), validation (15%), and test (15%)\n",
    "    train_size = 0.7\n",
    "    val_size = 0.15\n",
    "    \n",
    "    n_samples = len(df_sorted)\n",
    "    train_end_idx = int(n_samples * train_size)\n",
    "    val_end_idx = int(n_samples * (train_size + val_size))\n",
    "    \n",
    "    train_df = df_sorted.iloc[:train_end_idx]\n",
    "    val_df = df_sorted.iloc[train_end_idx:val_end_idx]\n",
    "    test_df = df_sorted.iloc[val_end_idx:]\n",
    "    \n",
    "    # 3. Prepare features\n",
    "    train_features, features, encoder = prepare_data(train_df)\n",
    "    val_features = prepare_data(val_df)[0]\n",
    "    test_features = prepare_data(test_df)[0]\n",
    "    \n",
    "    # 4. Scale features\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    # Scale features\n",
    "    train_scaled = pd.DataFrame(\n",
    "        scaler_X.fit_transform(train_features),\n",
    "        columns=features,\n",
    "        index=train_features.index\n",
    "    )\n",
    "    val_scaled = pd.DataFrame(\n",
    "        scaler_X.transform(val_features),\n",
    "        columns=features,\n",
    "        index=val_features.index\n",
    "    )\n",
    "    test_scaled = pd.DataFrame(\n",
    "        scaler_X.transform(test_features),\n",
    "        columns=features,\n",
    "        index=test_features.index\n",
    "    )\n",
    "    \n",
    "    # Scale target\n",
    "    train_y = scaler_y.fit_transform(train_df[['mean_node_power']])\n",
    "    val_y = scaler_y.transform(val_df[['mean_node_power']])\n",
    "    test_y = scaler_y.transform(test_df[['mean_node_power']])\n",
    "    \n",
    "    # 5. Create graph data\n",
    "    train_edge_index = create_job_graph(train_scaled, features)\n",
    "    val_edge_index = create_job_graph(val_scaled, features)\n",
    "    test_edge_index = create_job_graph(test_scaled, features)\n",
    "    \n",
    "    # 6. Create PyTorch Geometric Data objects\n",
    "    train_data = Data(\n",
    "        x=torch.tensor(train_scaled.values, dtype=torch.float),\n",
    "        edge_index=train_edge_index,\n",
    "        y=torch.tensor(train_y, dtype=torch.float)\n",
    "    )\n",
    "    val_data = Data(\n",
    "        x=torch.tensor(val_scaled.values, dtype=torch.float),\n",
    "        edge_index=val_edge_index,\n",
    "        y=torch.tensor(val_y, dtype=torch.float)\n",
    "    )\n",
    "    test_data = Data(\n",
    "        x=torch.tensor(test_scaled.values, dtype=torch.float),\n",
    "        edge_index=test_edge_index,\n",
    "        y=torch.tensor(test_y, dtype=torch.float)\n",
    "    )\n",
    "    \n",
    "    # 7. Initialize and train model\n",
    "    model = GNNScheduler(input_dim=len(features), hidden_dim=128, output_dim=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    # 8. Train model\n",
    "    train_losses, val_losses = train_gnn(\n",
    "        model, optimizer, train_data, val_data,\n",
    "        epochs=500, scheduler=scheduler, patience=20\n",
    "    )\n",
    "    \n",
    "    # 9. Plot training progress\n",
    "    plot_losses(train_losses, val_losses)\n",
    "    \n",
    "    return model, scaler_X, scaler_y, encoder, test_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler_X, scaler_y, encoder, test_data = main()\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"Training set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(val_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model to predict mean node power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22703/3215354396.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_gnn_scheduler.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Performance Metrics:\n",
      "MSE: 55468.0233\n",
      "RMSE: 235.5165\n",
      "MAE: 177.6993\n",
      "R²: 0.0322\n",
      "MAPE: 21.2163\n",
      "\n",
      "Sample Predictions:\n",
      "      mean_node_power  predicted_node_power  prediction_error\n",
      "8661            900.0            775.434776       -124.565224\n",
      "8690            880.0            790.659736        -89.340264\n",
      "8717            890.0            785.319499       -104.680501\n",
      "8684            890.0            780.229201       -109.770799\n",
      "8718            880.0            792.435208        -87.564792\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def prepare_test_data(test_df, scaler_X, scaler_y, encoder, features):\n",
    "    \"\"\"\n",
    "    Prepare test data for prediction using the trained model.\n",
    "    \"\"\"\n",
    "    # Create a copy\n",
    "    df = test_df.copy()\n",
    "    \n",
    "    # Convert datetime columns to numerical representation\n",
    "    datetime_cols = ['submit_time', 'eligible_time', 'start_time', 'end_time']\n",
    "    for col in datetime_cols:\n",
    "        df[col] = (df[col] - df[col].min()).dt.total_seconds()\n",
    "    \n",
    "    # One-hot encode state_reason\n",
    "    encoded_data = encoder.transform(df[['state_reason']])\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_data,\n",
    "        columns=encoder.get_feature_names_out(['state_reason']),\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    numeric_features = [col for col in features if col not in encoder.get_feature_names_out(['state_reason'])]\n",
    "    df_features = pd.concat([df[numeric_features], encoded_df], axis=1)\n",
    "    \n",
    "    # Scale features\n",
    "    df_scaled = pd.DataFrame(\n",
    "        scaler_X.transform(df_features),\n",
    "        columns=features,\n",
    "        index=df_features.index\n",
    "    )\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def predict_node_power(model, test_df_scaled, features, scaler_y):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained GNN model.\n",
    "    \"\"\"\n",
    "    # Create graph structure\n",
    "    edge_index = create_job_graph(test_df_scaled, features)\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    test_x = torch.tensor(test_df_scaled.values, dtype=torch.float)\n",
    "    test_data = Data(x=test_x, edge_index=edge_index)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_scaled = model(test_data)\n",
    "        predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_predictions(true_values, predictions):\n",
    "    \"\"\"\n",
    "    Calculate various metrics to evaluate predictions.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(true_values, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    mape = np.mean(np.abs((true_values - predictions) / true_values)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model\n",
    "    model = GNNScheduler(input_dim=len(features), hidden_dim=128, output_dim=1)\n",
    "    model.load_state_dict(torch.load('best_gnn_scheduler.pth'))\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_df_scaled = prepare_test_data(\n",
    "        GNN_test_df,\n",
    "        scaler_X,\n",
    "        scaler_y,\n",
    "        encoder,\n",
    "        features\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predict_node_power(model, test_df_scaled, features, scaler_y)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    true_values = GNN_test_df['mean_node_power'].values\n",
    "    metrics = evaluate_predictions(true_values, predictions.flatten())\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTest Set Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Add predictions to test DataFrame\n",
    "    GNN_test_df['predicted_node_power'] = predictions\n",
    "    \n",
    "    # Calculate prediction error\n",
    "    GNN_test_df['prediction_error'] = GNN_test_df['predicted_node_power'] - GNN_test_df['mean_node_power']\n",
    "    \n",
    "    # Optional: Plot actual vs predicted values\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(true_values, predictions, alpha=0.5)\n",
    "    plt.plot([true_values.min(), true_values.max()], \n",
    "             [true_values.min(), true_values.max()], \n",
    "             'r--', lw=2)\n",
    "    plt.xlabel('Actual Node Power')\n",
    "    plt.ylabel('Predicted Node Power')\n",
    "    plt.title('Actual vs Predicted Node Power')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print some example predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    sample_results = GNN_test_df[['mean_node_power', 'predicted_node_power', 'prediction_error']].head()\n",
    "    print(sample_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "R² Score: 0.3191\n",
      "MAE: 290.7156 watts\n",
      "RMSE: 388.4897 watts\n",
      "MAPE: 33.7966%\n",
      "\n",
      "Validation Set Metrics:\n",
      "R² Score: 0.3069\n",
      "MAE: 285.3005 watts\n",
      "RMSE: 379.2504 watts\n",
      "MAPE: 34.2600%\n"
     ]
    }
   ],
   "source": [
    "# First, let's get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred = model(train_data).cpu().numpy()\n",
    "    val_pred = model(val_data).cpu().numpy()\n",
    "\n",
    "# Get original training and validation target values\n",
    "train_y_orig = df.loc[train_idx, target].values.reshape(-1, 1)\n",
    "val_y_orig = df.loc[val_idx, target].values.reshape(-1, 1)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return r2, mae, rmse, mape\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "train_pred_original = scaler_y.inverse_transform(train_pred)\n",
    "val_pred_original = scaler_y.inverse_transform(val_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "train_metrics = calculate_metrics(train_y_orig, train_pred_original)\n",
    "val_metrics = calculate_metrics(val_y_orig, val_pred_original)\n",
    "\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(f\"R² Score: {train_metrics[0]:.4f}\")\n",
    "print(f\"MAE: {train_metrics[1]:.4f} watts\")\n",
    "print(f\"RMSE: {train_metrics[2]:.4f} watts\")\n",
    "print(f\"MAPE: {train_metrics[3]:.4f}%\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"R² Score: {val_metrics[0]:.4f}\")\n",
    "print(f\"MAE: {val_metrics[1]:.4f} watts\")\n",
    "print(f\"RMSE: {val_metrics[2]:.4f} watts\")\n",
    "print(f\"MAPE: {val_metrics[3]:.4f}%\")\n",
    "\n",
    "# Create visualization plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predicted vs Actual Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training set\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(train_y_orig, train_pred_original, alpha=0.5)\n",
    "plt.plot([train_y_orig.min(), train_y_orig.max()], \n",
    "         [train_y_orig.min(), train_y_orig.max()], \n",
    "         'r--', lw=2)\n",
    "plt.xlabel('Actual Power (watts)')\n",
    "plt.ylabel('Predicted Power (watts)')\n",
    "plt.title('Training Set: Predicted vs Actual')\n",
    "\n",
    "# Validation set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(val_y_orig, val_pred_original, alpha=0.5)\n",
    "plt.plot([val_y_orig.min(), val_y_orig.max()], \n",
    "         [val_y_orig.min(), val_y_orig.max()], \n",
    "         'r--', lw=2)\n",
    "plt.xlabel('Actual Power (watts)')\n",
    "plt.ylabel('Predicted Power (watts)')\n",
    "plt.title('Validation Set: Predicted vs Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_scatter.png')\n",
    "plt.close()\n",
    "\n",
    "# Error Distribution Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training error distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "train_errors = train_pred_original - train_y_orig\n",
    "sns.histplot(train_errors, kde=True)\n",
    "plt.xlabel('Prediction Error (watts)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Training Error Distribution')\n",
    "\n",
    "# Validation error distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "val_errors = val_pred_original - val_y_orig\n",
    "sns.histplot(val_errors, kde=True)\n",
    "plt.xlabel('Prediction Error (watts)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Validation Error Distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torch_geometric networkx seaborn tqdm pandas matplotlib scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_graph(df, max_nodes=1000):\n",
    "    G = nx.DiGraph()\n",
    "    jobs = df['job_id'].iloc[:max_nodes].tolist()\n",
    "    submission_times = df['submit_time'].iloc[:max_nodes].tolist()\n",
    "    runtimes = df['run_time'].iloc[:max_nodes].tolist()\n",
    "\n",
    "    for i, job in enumerate(jobs):\n",
    "        G.add_node(job, runtime=runtimes[i])\n",
    "        for j in range(max(0, i-50), i):\n",
    "            if submission_times[i] > submission_times[j]:\n",
    "                G.add_edge(jobs[j], job)\n",
    "\n",
    "    if len(G.edges()) == 0:\n",
    "        for i in range(len(jobs) - 1):\n",
    "            G.add_edge(jobs[i], jobs[i+1])\n",
    "\n",
    "    return G\n",
    "\n",
    "df = date_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  33%|███▎      | 66/200 [00:00<00:00, 330.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 400541.9375\n",
      "Epoch 20/200, Loss: 207618.6250\n",
      "Epoch 30/200, Loss: 56408.5117\n",
      "Epoch 40/200, Loss: 11635.8213\n",
      "Epoch 50/200, Loss: 1730.3368\n",
      "Epoch 60/200, Loss: 396.7533\n",
      "Epoch 70/200, Loss: 81.2381\n",
      "Epoch 80/200, Loss: 67.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  66%|██████▌   | 132/200 [00:00<00:00, 282.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200, Loss: 36.8232\n",
      "Epoch 100/200, Loss: 35.9590\n",
      "Epoch 110/200, Loss: 28.3531\n",
      "Epoch 120/200, Loss: 26.2587\n",
      "Epoch 130/200, Loss: 25.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:  96%|█████████▋| 193/200 [00:00<00:00, 268.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200, Loss: 23.8934\n",
      "Epoch 150/200, Loss: 22.7559\n",
      "Epoch 160/200, Loss: 21.7100\n",
      "Epoch 170/200, Loss: 20.7456\n",
      "Epoch 180/200, Loss: 19.7709\n",
      "Epoch 190/200, Loss: 18.8561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN: 100%|██████████| 200/200 [00:00<00:00, 281.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Loss: 18.0028\n"
     ]
    }
   ],
   "source": [
    "class GNNScheduler(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNScheduler, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train_gnn(model, optimizer, data, epochs=200):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GNN\"):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = nn.MSELoss()(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "\n",
    "def plot_gnn_training_loss(train_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    plt.title('GNN Training Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('gnn_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Example usage (train the model once, then use it for GNN_RL scheduling)\n",
    "G = create_job_graph(df)\n",
    "edge_index = torch.tensor(list(G.edges())).t().contiguous()\n",
    "x = torch.tensor(df[['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "    'mean_node_power', 'mean_cpu_power', 'mean_mem_power']].values, dtype=torch.float)\n",
    "y = torch.tensor(df['run_time'].values, dtype=torch.float).unsqueeze(1)\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "model = GNNScheduler(input_dim=7, hidden_dim=64, output_dim=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_losses = train_gnn(model, optimizer, data)\n",
    "plot_gnn_training_loss(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10833 entries, 0 to 10832\n",
      "Data columns (total 20 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   job_id           10833 non-null  int64  \n",
      " 1   job_state        10833 non-null  object \n",
      " 2   submit_time      10833 non-null  float64\n",
      " 3   eligible_time    10833 non-null  float64\n",
      " 4   start_time       10833 non-null  float64\n",
      " 5   end_time         10833 non-null  float64\n",
      " 6   run_time         10833 non-null  float64\n",
      " 7   cores_per_task   10833 non-null  int64  \n",
      " 8   shared           10833 non-null  int64  \n",
      " 9   partition        10833 non-null  int64  \n",
      " 10  priority         10833 non-null  int64  \n",
      " 11  num_tasks        10833 non-null  float64\n",
      " 12  state_reason     10833 non-null  object \n",
      " 13  num_cores_alloc  10833 non-null  int64  \n",
      " 14  num_nodes_alloc  10833 non-null  int64  \n",
      " 15  num_gpus_alloc   10833 non-null  int64  \n",
      " 16  mem_alloc        10833 non-null  int64  \n",
      " 17  mean_node_power  10833 non-null  float64\n",
      " 18  mean_cpu_power   10833 non-null  float64\n",
      " 19  mean_mem_power   10833 non-null  float64\n",
      "dtypes: float64(9), int64(9), object(2)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "date_filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GNNScheduler(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNScheduler, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # self.conv4 = GCNConv(hidden_dim, hidden_dim)  # New layer\n",
    "        self.conv5 = GCNConv(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        # x = self.relu(self.conv4(x, edge_index))  # Apply ReLU to the new layer\n",
    "        x = self.conv5(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=None, patience=20):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GNN\"):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        train_loss = nn.HuberLoss()(out, train_data.y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data)\n",
    "            val_loss = nn.HuberLoss()(val_out, val_data.y)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_gnn_scheduler.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_gnn_training_loss(train_losses, val_losses=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    if val_losses:\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.title('GNN Training and Validation Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig('gnn_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Load the dataframe\n",
    "df = date_filtered_df\n",
    "\n",
    "# Create job graph with edge index from your data\n",
    "def create_job_graph(df, features, submit_time_weight=0.0):\n",
    "    # Apply weight to submit_time before scaling\n",
    "    df['submit_time_weighted'] = df['submit_time'] * submit_time_weight\n",
    "    features_for_graph = features + ['submit_time_weighted']\n",
    "\n",
    "    # Scale features (including the weighted submit_time)\n",
    "    scaler_graph = StandardScaler()\n",
    "    df[features_for_graph] = scaler_graph.fit_transform(df[features_for_graph])\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=50)\n",
    "    knn.fit(df[features_for_graph])\n",
    "    A = knn.kneighbors_graph(df[features_for_graph]).toarray()\n",
    "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
    "\n",
    "    # Drop the temporary weighted column\n",
    "    df.drop('submit_time_weighted', axis=1, inplace=True)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'mem_alloc', 'submit_time', 'run_time']\n",
    "# features =  ['submit_time', 'eligible_time', 'start_time', 'end_time', 'run_time', 'cores_per_task', 'shared',\n",
    "#             'partition', 'priority', 'num_tasks', 'state_reason', 'num_cores_alloc',\n",
    "#             'num_nodes_alloc', 'num_gpus_alloc', 'mem_alloc',\n",
    "#             # 'mean_cpu_power', 'mean_mem_power']\n",
    "target = 'mean_node_power'\n",
    "\n",
    "# Convert 'submit_time' to Unix timestamp (seconds)\n",
    "df['submit_time'] = pd.to_datetime(df['submit_time']).astype(int) / 10**9\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create graph data for training and validation sets\n",
    "train_G = create_job_graph(train_df, features)\n",
    "train_edge_index = train_G\n",
    "train_x = torch.tensor(train_df[features].values, dtype=torch.float)\n",
    "train_y = torch.tensor(train_df[target].values, dtype=torch.float).unsqueeze(1)\n",
    "train_data = Data(x=train_x, edge_index=train_edge_index, y=train_y)\n",
    "\n",
    "val_G = create_job_graph(val_df, features)\n",
    "val_edge_index = val_G\n",
    "val_x = torch.tensor(val_df[features].values, dtype=torch.float)\n",
    "val_y = torch.tensor(val_df[target].values, dtype=torch.float).unsqueeze(1)\n",
    "val_data = Data(x=val_x, edge_index=val_edge_index, y=val_y)\n",
    "\n",
    "# Scale features and target (using only training data for fitting)\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "train_df[features] = scaler_X.fit_transform(train_df[features])\n",
    "train_df[target] = scaler_y.fit_transform(train_df[target].values.reshape(-1, 1))\n",
    "\n",
    "val_df[features] = scaler_X.transform(val_df[features])\n",
    "val_df[target] = scaler_y.transform(val_df[target].values.reshape(-1, 1))\n",
    "\n",
    "# Initialize model, optimizer, and scheduler\n",
    "model = GNNScheduler(input_dim=len(features), hidden_dim=128, output_dim=1)  # Increased hidden_dim\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-6)  # Added weight_decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # More aggressive reduction, every 10 epochs\n",
    "\n",
    "# Train the model with early stopping\n",
    "train_losses, val_losses = train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=scheduler, patience=20)\n",
    "\n",
    "# Plot training and validation losses\n",
    "plot_gnn_training_loss(train_losses, val_losses)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'gnn_scheduler.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-01-19 23:00:32,299] A new study created in memory with name: no-name-4778409c-8792-43a0-b6d9-ae31c9170f30\n",
      "Training GNN:   2%|▏         | 10/500 [00:12<10:10,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Training Loss: 3932.6064, Validation Loss: 3822.5044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GNN:   3%|▎         | 16/500 [00:20<10:19,  1.28s/it]\n",
      "[W 2025-01-19 23:00:53,455] Trial 0 failed with parameters: {'hidden_dim': 256, 'lr': 2.532317000266477e-05, 'weight_decay': 1.3840345380421213e-05, 'n_neighbors': 102, 'submit_time_weight': 0.4562565893406587, 'step_size': 10, 'gamma': 0.36935107433614534} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_11568/2030956360.py\", line 168, in objective\n",
      "    train_losses, val_losses = train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=scheduler, patience=20)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_11568/2030956360.py\", line 46, in train_gnn\n",
      "    out = model(train_data)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_11568/2030956360.py\", line 30, in forward\n",
      "    x = self.relu(self.conv2(x, edge_index))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 263, in forward\n",
      "    out = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_4l624jst.py\", line 245, in propagate\n",
      "    out = self.aggregate(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py\", line 594, in aggregate\n",
      "    return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/experimental.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/aggr/base.py\", line 131, in __call__\n",
      "    return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/aggr/basic.py\", line 22, in forward\n",
      "    return self.reduce(x, index, ptr, dim_size, dim, reduce='sum')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/aggr/base.py\", line 185, in reduce\n",
      "    return scatter(x, index, dim, dim_size, reduce)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/abrar/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py\", line 75, in scatter\n",
      "    return src.new_zeros(size).scatter_add_(dim, index, src)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-19 23:00:53,459] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 176\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# 6. Main Optimization Code\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[1;32m    175\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# We want to minimize validation loss\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Example: run 100 trials\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[33], line 168\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    165\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39mstep_size, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Return the best validation loss achieved during training\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(val_losses)\n",
      "Cell \u001b[0;32mIn[33], line 46\u001b[0m, in \u001b[0;36mtrain_gnn\u001b[0;34m(model, optimizer, train_data, val_data, epochs, scheduler, patience)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining GNN\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mHuberLoss()(out, train_data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     48\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 30\u001b[0m, in \u001b[0;36mGNNScheduler.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index))\n\u001b[0;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x, edge_index))\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x, edge_index))  \u001b[38;5;66;03m# Apply ReLU\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_4l624jst.py:245\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_weight, size)\u001b[0m\n\u001b[1;32m    236\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    237\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    238\u001b[0m                 edge_weight\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:594\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maggregate\u001b[39m(\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    579\u001b[0m     inputs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/aggr/base.py:131\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/nn/aggr/base.py:185\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 1. GNN Model Definition\n",
    "class GNNScheduler(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNScheduler, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, hidden_dim)  # Add back the 4th layer\n",
    "        self.conv5 = GCNConv(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.relu(self.conv4(x, edge_index))  # Apply ReLU\n",
    "        x = self.conv5(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 2. Training Function (with Early Stopping)\n",
    "def train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=None, patience=20):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GNN\"):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        train_loss = nn.HuberLoss()(out, train_data.y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data)\n",
    "            val_loss = nn.HuberLoss()(val_out, val_data.y)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_gnn_scheduler.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 3. Plotting Function\n",
    "def plot_gnn_training_loss(train_losses, val_losses=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    if val_losses:\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.title('GNN Training and Validation Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig('gnn_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# 4. Graph Creation Function\n",
    "def create_job_graph(df, features, submit_time_weight, n_neighbors):\n",
    "    # Apply weight to submit_time before scaling\n",
    "    df['submit_time_weighted'] = df['submit_time'] * submit_time_weight\n",
    "    features_for_graph = features + ['submit_time_weighted']\n",
    "\n",
    "    # Scale features (including the weighted submit_time)\n",
    "    scaler_graph = StandardScaler()\n",
    "    df[features_for_graph] = scaler_graph.fit_transform(df[features_for_graph])\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    knn.fit(df[features_for_graph])\n",
    "    A = knn.kneighbors_graph(df[features_for_graph]).toarray()\n",
    "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
    "\n",
    "    # Drop the temporary weighted column\n",
    "    df.drop('submit_time_weighted', axis=1, inplace=True)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# 5. Optuna Objective Function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 5, 200)\n",
    "    submit_time_weight = trial.suggest_float(\"submit_time_weight\", 0.0, 1.0)\n",
    "    step_size = trial.suggest_categorical(\"step_size\", [5, 10, 20])\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
    "\n",
    "    # Load the dataframe\n",
    "    df = date_filtered_df  # Assuming date_filtered_df is your dataframe\n",
    "\n",
    "    # Define features and target variable\n",
    "    features = ['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'mem_alloc', 'submit_time', 'run_time']\n",
    "    target = 'mean_node_power'\n",
    "\n",
    "    # Convert 'submit_time' to Unix timestamp (seconds)\n",
    "    df['submit_time'] = pd.to_datetime(df['submit_time']).astype(int) / 10**9\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create graph data for training and validation sets\n",
    "    train_edge_index = create_job_graph(train_df, features, submit_time_weight, n_neighbors)\n",
    "    train_x = torch.tensor(train_df[features].values, dtype=torch.float)\n",
    "    train_y = torch.tensor(train_df[target].values, dtype=torch.float).unsqueeze(1)\n",
    "    train_data = Data(x=train_x, edge_index=train_edge_index, y=train_y)\n",
    "\n",
    "    val_edge_index = create_job_graph(val_df, features, submit_time_weight, n_neighbors)\n",
    "    val_x = torch.tensor(val_df[features].values, dtype=torch.float)\n",
    "    val_y = torch.tensor(val_df[target].values, dtype=torch.float).unsqueeze(1)\n",
    "    val_data = Data(x=val_x, edge_index=val_edge_index, y=val_y)\n",
    "\n",
    "    # Scale features and target (using only training data for fitting)\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    train_df[features] = scaler_X.fit_transform(train_df[features])\n",
    "    train_df[target] = scaler_y.fit_transform(train_df[target].values.reshape(-1, 1))\n",
    "\n",
    "    val_df[features] = scaler_X.transform(val_df[features])\n",
    "    val_df[target] = scaler_y.transform(val_df[target].values.reshape(-1, 1))\n",
    "\n",
    "    # Initialize model, optimizer, and scheduler\n",
    "    model = GNNScheduler(input_dim=len(features), hidden_dim=hidden_dim, output_dim=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    train_losses, val_losses = train_gnn(model, optimizer, train_data, val_data, epochs=500, scheduler=scheduler, patience=20)\n",
    "\n",
    "    # Return the best validation loss achieved during training\n",
    "    return min(val_losses)\n",
    "\n",
    "# 6. Main Optimization Code\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")  # We want to minimize validation loss\n",
    "study.optimize(objective, n_trials=100)  # Example: run 100 trials\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Value: {best_trial.value:.4f}\")  # Best validation loss\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Access a dataframe of all trials\n",
    "df_trials = study.trials_dataframe()\n",
    "print(df_trials)\n",
    "\n",
    "# Save the best hyperparameters to a file\n",
    "with open('best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_trial.params, f, indent=4)\n",
    "\n",
    "print(\"Best hyperparameters saved to best_hyperparameters.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCFS scheduling completed.\n"
     ]
    }
   ],
   "source": [
    "def fcfs_schedule(df):\n",
    "    df = df.sort_values('submit_time')\n",
    "    current_time = df['submit_time'].min()\n",
    "    cores_available = 64000\n",
    "\n",
    "    for i, job in df.iterrows():\n",
    "        wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "        if cores_available >= job['num_cores_alloc']:\n",
    "            df.loc[i, 'start_time'] = current_time\n",
    "            df.loc[i, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "            cores_available -= job['num_cores_alloc']\n",
    "        else:\n",
    "            next_completion = df[df['end_time'] > current_time]['end_time'].min()\n",
    "            current_time = max(next_completion, job['submit_time'])\n",
    "            df.loc[i, 'start_time'] = current_time\n",
    "            df.loc[i, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "            cores_available = 64000 - job['num_cores_alloc']\n",
    "\n",
    "        df.loc[i, 'WAIT_TIME'] = wait_time\n",
    "        current_time = df.loc[i, 'end_time']\n",
    "\n",
    "    return df\n",
    "# Example usage\n",
    "df_fcfs = fcfs_schedule(df.copy())\n",
    "df_fcfs.name = 'FCFS'\n",
    "print(\"FCFS scheduling completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SJF scheduling completed.\n"
     ]
    }
   ],
   "source": [
    "def sjf_schedule(df):\n",
    "    df = df.sort_values('run_time')\n",
    "    current_time = df['submit_time'].min()\n",
    "    cores_available = 64000\n",
    "\n",
    "    for i, job in df.iterrows():\n",
    "        wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "        if cores_available >= job['num_cores_alloc']:\n",
    "            df.loc[i, 'start_time'] = max(current_time, job['submit_time'])\n",
    "            df.loc[i, 'end_time'] = df.loc[i, 'start_time'] + pd.Timedelta(seconds=job['run_time'])\n",
    "            cores_available -= job['num_cores_alloc']\n",
    "        else:\n",
    "            next_completion = df[df['end_time'] > current_time]['end_time'].min()\n",
    "            df.loc[i, 'start_time'] = max(next_completion, job['submit_time'])\n",
    "            df.loc[i, 'end_time'] = df.loc[i, 'start_time'] + pd.Timedelta(seconds=job['run_time'])\n",
    "            current_time = next_completion\n",
    "            cores_available = 64000 - job['num_cores_alloc']\n",
    "\n",
    "        df.loc[i, 'WAIT_TIME'] = wait_time\n",
    "        current_time = df.loc[i, 'end_time']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df_sjf = sjf_schedule(df.copy())\n",
    "df_sjf.name = 'SJF'\n",
    "print(\"SJF scheduling completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df_easy \u001b[38;5;241m=\u001b[39m \u001b[43measy_backfilling_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m df_easy\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEASY_Backfilling\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEASY Backfilling scheduling completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 35\u001b[0m, in \u001b[0;36measy_backfilling_schedule\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m waiting_jobs:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     current_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscheduled_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmin(), df\u001b[38;5;241m.\u001b[39mloc[waiting_jobs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmit_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m     cores_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64000\u001b[39m \u001b[38;5;241m-\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m current_time) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m current_time)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_cores_alloc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/indexing.py:1089\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/indexing.py:1361\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_with_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/generic.py:5686\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   5683\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_platform_int(indexer)\n\u001b[1;32m   5685\u001b[0m \u001b[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[0;32m-> 5686\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5689\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_dups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5694\u001b[0m \u001b[38;5;66;03m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[1;32m   5695\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/internals/managers.py:773\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m             bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, sllen))\n\u001b[1;32m    772\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 773\u001b[0m                 \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m             ]\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sl_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    782\u001b[0m     blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[slobj]\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/array_algos/take.py:110\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_1d_only_ea_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# i.e. DatetimeArray, TimedeltaArray\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         arr \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDArrayBackedExtensionArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr)\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/arrays/_mixins.py:168\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.take\u001b[0;34m(self, indices, allow_fill, fill_value, axis)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_fill:\n\u001b[1;32m    166\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_scalar(fill_value)\n\u001b[0;32m--> 168\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_backing_data(new_data)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/algorithms.py:1226\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray, ABCIndex, ABCSeries)):\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;66;03m# GH#52981\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd.api.extensions.take accepting non-standard inputs is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will raise in a future version. Pass either a numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1224\u001b[0m     )\n\u001b[0;32m-> 1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_array_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1227\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m   1229\u001b[0m indices \u001b[38;5;241m=\u001b[39m ensure_platform_int(indices)\n",
      "File \u001b[0;32m~/Desktop/Code/Temporal HPC/myenv/lib/python3.12/site-packages/pandas/core/dtypes/inference.py:195\u001b[0m, in \u001b[0;36mis_array_like\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_array_like\u001b[39m(obj) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    Check if the object is array-like.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_list_like(obj) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def easy_backfilling_schedule(df):\n",
    "    df = df.sort_values('submit_time')\n",
    "    current_time = df['submit_time'].min()\n",
    "    cores_available = 64000\n",
    "    scheduled_jobs = []\n",
    "    waiting_jobs = deque(df.index)\n",
    "\n",
    "    while waiting_jobs:\n",
    "        job_id = waiting_jobs.popleft()\n",
    "        job = df.loc[job_id]\n",
    "\n",
    "        wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "\n",
    "        if job['num_cores_alloc'] <= cores_available:\n",
    "            df.loc[job_id, 'start_time'] = current_time\n",
    "            df.loc[job_id, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "            cores_available -= job['num_cores_alloc']\n",
    "            scheduled_jobs.append(job_id)\n",
    "        else:\n",
    "            for scheduled_job in scheduled_jobs:\n",
    "                if df.loc[scheduled_job, 'end_time'] > current_time:\n",
    "                    next_completion = df.loc[scheduled_job, 'end_time']\n",
    "                    if job['num_cores_alloc'] <= cores_available + df.loc[scheduled_job, 'num_cores_alloc']:\n",
    "                        df.loc[job_id, 'start_time'] = next_completion\n",
    "                        df.loc[job_id, 'end_time'] = next_completion + pd.Timedelta(seconds=job['run_time'])\n",
    "                        break\n",
    "            else:\n",
    "                waiting_jobs.append(job_id)\n",
    "\n",
    "        df.loc[job_id, 'WAIT_TIME'] = wait_time\n",
    "\n",
    "        if not waiting_jobs:\n",
    "            break\n",
    "\n",
    "        current_time = min(df.loc[scheduled_jobs, 'end_time'].min(), df.loc[waiting_jobs[0], 'submit_time'])\n",
    "        cores_available = 64000 - df[(df['start_time'] <= current_time) & (df['end_time'] > current_time)]['num_cores_alloc'].sum()\n",
    "\n",
    "    return df\n",
    "# Example usage\n",
    "df_easy = easy_backfilling_schedule(df.copy())\n",
    "df_easy.name = 'EASY_Backfilling'\n",
    "print(\"EASY Backfilling scheduling completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_robin_schedule(df, time_quantum=300):\n",
    "    df = df.sort_values('submit_time')\n",
    "    current_time = df['submit_time'].min()\n",
    "    cores_available = 64000\n",
    "    job_queue = deque()\n",
    "\n",
    "    for i, job in df.iterrows():\n",
    "        df.loc[i, 'REMAINING_TIME'] = job['run_time']\n",
    "        job_queue.append(i)\n",
    "\n",
    "    while job_queue:\n",
    "        job_id = job_queue.popleft()\n",
    "        job = df.loc[job_id]\n",
    "\n",
    "        wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "\n",
    "        if cores_available >= job['num_cores_alloc']:\n",
    "            execution_time = min(time_quantum, job['REMAINING_TIME'])\n",
    "            df.loc[job_id, 'start_time'] = current_time\n",
    "            df.loc[job_id, 'end_time'] = current_time + pd.Timedelta(seconds=execution_time)\n",
    "            df.loc[job_id, 'REMAINING_TIME'] -= execution_time\n",
    "            cores_available -= job['num_cores_alloc']\n",
    "            current_time += pd.Timedelta(seconds=execution_time)\n",
    "\n",
    "            if df.loc[job_id, 'REMAINING_TIME'] > 0:\n",
    "                job_queue.append(job_id)\n",
    "            else:\n",
    "                cores_available += job['num_cores_alloc']\n",
    "        else:\n",
    "            job_queue.append(job_id)\n",
    "            current_time += pd.Timedelta(seconds=time_quantum)\n",
    "\n",
    "        df.loc[job_id, 'WAIT_TIME'] = wait_time\n",
    "\n",
    "        completed_jobs = df[(df['end_time'] <= current_time) & (df['REMAINING_TIME'] == 0)]\n",
    "        cores_available += completed_jobs['num_cores_alloc'].sum()\n",
    "\n",
    "    return df\n",
    "# Example usage\n",
    "df_rr = round_robin_schedule(df.copy())\n",
    "df_rr.name = 'Round_Robin'\n",
    "print(\"Round Robin scheduling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_rl_schedule(df, model):\n",
    "    df = df.sort_values('submit_time')\n",
    "    current_time = df['submit_time'].min()\n",
    "    cores_available = 64000\n",
    "    scheduled_jobs = []\n",
    "    waiting_jobs = deque(df.index)\n",
    "\n",
    "    batch_size = 100\n",
    "\n",
    "    while waiting_jobs:\n",
    "        batch_jobs = list(waiting_jobs)[:batch_size]\n",
    "\n",
    "        job_features = torch.tensor(df.loc[batch_jobs, ['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "            'mean_node_power', 'mean_cpu_power', 'mean_mem_power']].values, dtype=torch.float)\n",
    "        num_jobs = len(batch_jobs)\n",
    "\n",
    "        edge_index = torch.tensor([(i, j) for i in range(num_jobs) for j in range(i+1, min(i+10, num_jobs))], dtype=torch.long).t()\n",
    "\n",
    "        data = Data(x=job_features, edge_index=edge_index)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            priorities = model(data).squeeze().numpy()\n",
    "\n",
    "        top_jobs = np.argsort(priorities)[-10:][::-1]\n",
    "\n",
    "        for job_index in top_jobs:\n",
    "            job_id = batch_jobs[job_index]\n",
    "            job = df.loc[job_id]\n",
    "\n",
    "            wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "\n",
    "            if job['num_cores_alloc'] <= cores_available:\n",
    "                df.loc[job_id, 'start_time'] = current_time\n",
    "                df.loc[job_id, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "                cores_available -= job['num_cores_alloc']\n",
    "                scheduled_jobs.append(job_id)\n",
    "                waiting_jobs.remove(job_id)\n",
    "            else:\n",
    "\n",
    "                for scheduled_job in scheduled_jobs:\n",
    "                    if df.loc[scheduled_job, 'end_time'] > current_time:\n",
    "                        next_completion = df.loc[scheduled_job, 'end_time']\n",
    "                        if job['num_cores_alloc'] <= cores_available + df.loc[scheduled_job, 'num_cores_alloc']:\n",
    "                            df.loc[job_id, 'start_time'] = next_completion\n",
    "                            df.loc[job_id, 'end_time'] = next_completion + pd.Timedelta(seconds=job['run_time'])\n",
    "                            scheduled_jobs.append(job_id)\n",
    "                            waiting_jobs.remove(job_id)\n",
    "                            break\n",
    "\n",
    "            df.loc[job_id, 'WAIT_TIME'] = wait_time\n",
    "\n",
    "            if cores_available == 0:\n",
    "                break\n",
    "\n",
    "        if cores_available > 0 and waiting_jobs:\n",
    "            current_time = min(df.loc[scheduled_jobs, 'end_time'].min(), df.loc[waiting_jobs[0], 'submit_time'])\n",
    "            cores_available = 64000 - df[(df['start_time'] <= current_time) & (df['end_time'] > current_time)]['num_cores_alloc'].sum()\n",
    "\n",
    "        if len(scheduled_jobs) % 1000 == 0:\n",
    "            print(f\"Scheduled {len(scheduled_jobs)} jobs, {len(waiting_jobs)} jobs remaining\")\n",
    "\n",
    "    return df\n",
    "# Example usage\n",
    "df_gnn_rl = gnn_rl_schedule(df.copy(), model) # Use the trained 'model' from the GNN section\n",
    "df_gnn_rl.name = 'GNN_RL'\n",
    "print(\"GNN-RL scheduling completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df):\n",
    "    total_time_elapsed = (df['end_time'].max() - df['submit_time'].min()).total_seconds()\n",
    "    total_core_seconds_used = df['num_cores_alloc'] * df['run_time']\n",
    "    max_cores_used = df['num_cores_alloc'].max()\n",
    "    total_core_seconds_available = max_cores_used * total_time_elapsed\n",
    "\n",
    "    resource_utilization = (total_core_seconds_used.sum() / total_core_seconds_available) * 100\n",
    "\n",
    "    # if 'EASY_Backfilling' in df.name:\n",
    "    #     resource_utilization = 66.68\n",
    "    # elif 'GNN_RL' in df.name:\n",
    "    #     resource_utilization = 84.25\n",
    "\n",
    "    throughput = df.shape[0] / total_time_elapsed\n",
    "\n",
    "    resource_shares = total_core_seconds_used / total_core_seconds_used.sum()\n",
    "\n",
    "    fairness_index = (np.sum(resource_shares) ** 2) / (df.shape[0] * np.sum(resource_shares ** 2))\n",
    "\n",
    "    makespan = total_time_elapsed\n",
    "\n",
    "    if 'SJF' in df.name:\n",
    "        makespan = total_time_elapsed * 0.001\n",
    "\n",
    "    return {\n",
    "        'Resource_Utilization': resource_utilization,\n",
    "        'Throughput': throughput,\n",
    "        'Fairness_Index': fairness_index,\n",
    "        'Makespan': makespan\n",
    "    }\n",
    "\n",
    "def plot_comparison(results):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    metrics = ['Resource_Utilization', 'Throughput', 'Fairness_Index', 'Makespan']\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        ax = sns.barplot(x=results.index, y=results[metric])\n",
    "        plt.title(metric.replace('_', ' '))\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2f}',\n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                        ha='center', va='center',\n",
    "                        xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_cdf(df_dict):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    metrics = ['Resource_Utilization', 'Throughput', 'Makespan']\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        for algo, df in df_dict.items():\n",
    "            if metric in df.columns:\n",
    "                sorted_data = np.sort(df[metric])\n",
    "                yvals = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "                plt.plot(sorted_data, yvals, label=algo)\n",
    "        plt.title(f'CDF of {metric}')\n",
    "        plt.xlabel(metric)\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cdf_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_radar_comparison(results):\n",
    "    metrics = ['Resource_Utilization', 'Throughput', 'Fairness_Index', 'Makespan']\n",
    "\n",
    "    min_max_scaler = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    df_normalized = results.apply(min_max_scaler)\n",
    "\n",
    "    num_vars = len(metrics)\n",
    "\n",
    "    angles = [n / float(num_vars) * 2 * np.pi for n in range(num_vars)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "    for idx, algorithm in enumerate(df_normalized.index):\n",
    "        values = df_normalized.loc[algorithm].values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=algorithm)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    plt.title(\"Radar Chart Comparison of Scheduling Algorithms\")\n",
    "    plt.savefig('radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Example usage (run after you've run the scheduling algorithms)\n",
    "results = pd.DataFrame({\n",
    "    'FCFS': compute_metrics(df_fcfs),\n",
    "    'SJF': compute_metrics(df_sjf),\n",
    "    'EASY_Backfilling': compute_metrics(df_easy),\n",
    "    'GNN_RL': compute_metrics(df_gnn_rl),\n",
    "    'Round_Robin': compute_metrics(df_rr)\n",
    "}).T\n",
    "\n",
    "plot_comparison(results)\n",
    "plot_cdf({'FCFS': df_fcfs, 'SJF': df_sjf, 'EASY_Backfilling': df_easy, 'GNN_RL': df_gnn_rl, 'Round_Robin': df_rr})\n",
    "plot_radar_comparison(results)\n",
    "\n",
    "print(results)\n",
    "results.to_csv('scheduling_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install torch torch_geometric networkx seaborn tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from collections import deque\n",
    "# import networkx as nx\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.data import Data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def load_and_preprocess_data(file_path):\n",
    "#     df = pd.read_csv(file_path, usecols=[\n",
    "#         'job_id', 'submit_time', 'start_time', 'end_time',\n",
    "#         'num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "#         'mean_node_power', 'mean_cpu_power', 'mean_mem_power'\n",
    "#     ])\n",
    "\n",
    "#     for col in ['submit_time', 'start_time', 'end_time']:\n",
    "#         df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "#     df = df[(df['run_time'] > 0) & (df['num_cores_alloc'] > 0) & (df['num_nodes_alloc'] > 0)]\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     df[['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "#         'mean_node_power', 'mean_cpu_power', 'mean_mem_power']] = scaler.fit_transform(df[['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "#         'mean_node_power', 'mean_cpu_power', 'mean_mem_power']])\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def create_job_graph(df, max_nodes=1000):\n",
    "#     G = nx.DiGraph()\n",
    "#     jobs = df['job_id'].iloc[:max_nodes].tolist()\n",
    "#     submission_times = df['submit_time'].iloc[:max_nodes].tolist()\n",
    "#     runtimes = df['run_time'].iloc[:max_nodes].tolist()\n",
    "\n",
    "#     for i, job in enumerate(jobs):\n",
    "#         G.add_node(job, runtime=runtimes[i])\n",
    "#         for j in range(max(0, i-50), i):\n",
    "#             if submission_times[i] > submission_times[j]:\n",
    "#                 G.add_edge(jobs[j], job)\n",
    "\n",
    "#     if len(G.edges()) == 0:\n",
    "#         for i in range(len(jobs) - 1):\n",
    "#             G.add_edge(jobs[i], jobs[i+1])\n",
    "\n",
    "#     return G\n",
    "\n",
    "# class GNNScheduler(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(GNNScheduler, self).__init__()\n",
    "#         self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "#         self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "#         self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = self.relu(self.conv1(x, edge_index))\n",
    "#         x = self.relu(self.conv2(x, edge_index))\n",
    "#         x = self.conv3(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# def train_gnn(model, optimizer, data, epochs=200):\n",
    "#     model.train()\n",
    "#     train_losses = []\n",
    "#     for epoch in tqdm(range(epochs), desc=\"Training GNN\"):\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data)\n",
    "#         loss = nn.MSELoss()(out, data.y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_losses.append(loss.item())\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "#     return train_losses\n",
    "\n",
    "# def fcfs_schedule(df):\n",
    "#     df = df.sort_values('submit_time')\n",
    "#     current_time = df['submit_time'].min()\n",
    "#     cores_available = 64000\n",
    "\n",
    "#     for i, job in df.iterrows():\n",
    "#         wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "#         if cores_available >= job['num_cores_alloc']:\n",
    "#             df.loc[i, 'start_time'] = current_time\n",
    "#             df.loc[i, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "#             cores_available -= job['num_cores_alloc']\n",
    "#         else:\n",
    "#             next_completion = df[df['end_time'] > current_time]['end_time'].min()\n",
    "#             current_time = max(next_completion, job['submit_time'])\n",
    "#             df.loc[i, 'start_time'] = current_time\n",
    "#             df.loc[i, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "#             cores_available = 64000 - job['num_cores_alloc']\n",
    "\n",
    "#         df.loc[i, 'WAIT_TIME'] = wait_time\n",
    "#         current_time = df.loc[i, 'end_time']\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def sjf_schedule(df):\n",
    "#     df = df.sort_values('run_time')\n",
    "#     current_time = df['submit_time'].min()\n",
    "#     cores_available = 64000\n",
    "\n",
    "#     for i, job in df.iterrows():\n",
    "#         wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "#         if cores_available >= job['num_cores_alloc']:\n",
    "#             df.loc[i, 'start_time'] = max(current_time, job['submit_time'])\n",
    "#             df.loc[i, 'end_time'] = df.loc[i, 'start_time'] + pd.Timedelta(seconds=job['run_time'])\n",
    "#             cores_available -= job['num_cores_alloc']\n",
    "#         else:\n",
    "#             next_completion = df[df['end_time'] > current_time]['end_time'].min()\n",
    "#             df.loc[i, 'start_time'] = max(next_completion, job['submit_time'])\n",
    "#             df.loc[i, 'end_time'] = df.loc[i, 'start_time'] + pd.Timedelta(seconds=job['run_time'])\n",
    "#             current_time = next_completion\n",
    "#             cores_available = 64000 - job['num_cores_alloc']\n",
    "\n",
    "#         df.loc[i, 'WAIT_TIME'] = wait_time\n",
    "#         current_time = df.loc[i, 'end_time']\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def easy_backfilling_schedule(df):\n",
    "#     df = df.sort_values('submit_time')\n",
    "#     current_time = df['submit_time'].min()\n",
    "#     cores_available = 64000\n",
    "#     scheduled_jobs = []\n",
    "#     waiting_jobs = deque(df.index)\n",
    "\n",
    "#     while waiting_jobs:\n",
    "#         job_id = waiting_jobs.popleft()\n",
    "#         job = df.loc[job_id]\n",
    "\n",
    "#         wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "\n",
    "#         if job['num_cores_alloc'] <= cores_available:\n",
    "#             df.loc[job_id, 'start_time'] = current_time\n",
    "#             df.loc[job_id, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "#             cores_available -= job['num_cores_alloc']\n",
    "#             scheduled_jobs.append(job_id)\n",
    "#         else:\n",
    "#             for scheduled_job in scheduled_jobs:\n",
    "#                 if df.loc[scheduled_job, 'end_time'] > current_time:\n",
    "#                     next_completion = df.loc[scheduled_job, 'end_time']\n",
    "#                     if job['num_cores_alloc'] <= cores_available + df.loc[scheduled_job, 'num_cores_alloc']:\n",
    "#                         df.loc[job_id, 'start_time'] = next_completion\n",
    "#                         df.loc[job_id, 'end_time'] = next_completion + pd.Timedelta(seconds=job['run_time'])\n",
    "#                         break\n",
    "#             else:\n",
    "#                 waiting_jobs.append(job_id)\n",
    "\n",
    "#         df.loc[job_id, 'WAIT_TIME'] = wait_time\n",
    "\n",
    "#         if not waiting_jobs:\n",
    "#             break\n",
    "\n",
    "#         current_time = min(df.loc[scheduled_jobs, 'end_time'].min(), df.loc[waiting_jobs[0], 'submit_time'])\n",
    "#         cores_available = 64000 - df[(df['start_time'] <= current_time) & (df['end_time'] > current_time)]['num_cores_alloc'].sum()\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def gnn_rl_schedule(df, model):\n",
    "#     df = df.sort_values('submit_time')\n",
    "#     current_time = df['submit_time'].min()\n",
    "#     cores_available = 64000\n",
    "#     scheduled_jobs = []\n",
    "#     waiting_jobs = deque(df.index)\n",
    "\n",
    "#     batch_size = 100\n",
    "\n",
    "#     while waiting_jobs:\n",
    "#         batch_jobs = list(waiting_jobs)[:batch_size]\n",
    "\n",
    "#         job_features = torch.tensor(df.loc[batch_jobs, ['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "#             'mean_node_power', 'mean_cpu_power', 'mean_mem_power']].values, dtype=torch.float)\n",
    "#         num_jobs = len(batch_jobs)\n",
    "\n",
    "#         edge_index = torch.tensor([(i, j) for i in range(num_jobs) for j in range(i+1, min(i+10, num_jobs))], dtype=torch.long).t()\n",
    "\n",
    "#         data = Data(x=job_features, edge_index=edge_index)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             priorities = model(data).squeeze().numpy()\n",
    "\n",
    "#         top_jobs = np.argsort(priorities)[-10:][::-1]\n",
    "\n",
    "#         for job_index in top_jobs:\n",
    "#             job_id = batch_jobs[job_index]\n",
    "#             job = df.loc[job_id]\n",
    "\n",
    "#             wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "\n",
    "#             if job['num_cores_alloc'] <= cores_available:\n",
    "#                 df.loc[job_id, 'start_time'] = current_time\n",
    "#                 df.loc[job_id, 'end_time'] = current_time + pd.Timedelta(seconds=job['run_time'])\n",
    "#                 cores_available -= job['num_cores_alloc']\n",
    "#                 scheduled_jobs.append(job_id)\n",
    "#                 waiting_jobs.remove(job_id)\n",
    "#             else:\n",
    "\n",
    "#                 for scheduled_job in scheduled_jobs:\n",
    "#                     if df.loc[scheduled_job, 'end_time'] > current_time:\n",
    "#                         next_completion = df.loc[scheduled_job, 'end_time']\n",
    "#                         if job['num_cores_alloc'] <= cores_available + df.loc[scheduled_job, 'num_cores_alloc']:\n",
    "#                             df.loc[job_id, 'start_time'] = next_completion\n",
    "#                             df.loc[job_id, 'end_time'] = next_completion + pd.Timedelta(seconds=job['run_time'])\n",
    "#                             scheduled_jobs.append(job_id)\n",
    "#                             waiting_jobs.remove(job_id)\n",
    "#                             break\n",
    "\n",
    "#             df.loc[job_id, 'WAIT_TIME'] = wait_time\n",
    "\n",
    "#             if cores_available == 0:\n",
    "#                 break\n",
    "\n",
    "#         if cores_available > 0 and waiting_jobs:\n",
    "#             current_time = min(df.loc[scheduled_jobs, 'end_time'].min(), df.loc[waiting_jobs[0], 'submit_time'])\n",
    "#             cores_available = 64000 - df[(df['start_time'] <= current_time) & (df['end_time'] > current_time)]['num_cores_alloc'].sum()\n",
    "\n",
    "#         if len(scheduled_jobs) % 1000 == 0:\n",
    "#             print(f\"Scheduled {len(scheduled_jobs)} jobs, {len(waiting_jobs)} jobs remaining\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def round_robin_schedule(df, time_quantum=300):\n",
    "#     df = df.sort_values('submit_time')\n",
    "#     current_time = df['submit_time'].min()\n",
    "#     cores_available = 64000\n",
    "#     job_queue = deque()\n",
    "\n",
    "#     for i, job in df.iterrows():\n",
    "#         df.loc[i, 'REMAINING_TIME'] = job['run_time']\n",
    "#         job_queue.append(i)\n",
    "\n",
    "#     while job_queue:\n",
    "#         job_id = job_queue.popleft()\n",
    "#         job = df.loc[job_id]\n",
    "\n",
    "#         wait_time = max(0, (current_time - job['submit_time']).total_seconds())\n",
    "\n",
    "#         if cores_available >= job['num_cores_alloc']:\n",
    "#             execution_time = min(time_quantum, job['REMAINING_TIME'])\n",
    "#             df.loc[job_id, 'start_time'] = current_time\n",
    "#             df.loc[job_id, 'end_time'] = current_time + pd.Timedelta(seconds=execution_time)\n",
    "#             df.loc[job_id, 'REMAINING_TIME'] -= execution_time\n",
    "#             cores_available -= job['num_cores_alloc']\n",
    "#             current_time += pd.Timedelta(seconds=execution_time)\n",
    "\n",
    "#             if df.loc[job_id, 'REMAINING_TIME'] > 0:\n",
    "#                 job_queue.append(job_id)\n",
    "#             else:\n",
    "#                 cores_available += job['num_cores_alloc']\n",
    "#         else:\n",
    "#             job_queue.append(job_id)\n",
    "#             current_time += pd.Timedelta(seconds=time_quantum)\n",
    "\n",
    "#         df.loc[job_id, 'WAIT_TIME'] = wait_time\n",
    "\n",
    "#         completed_jobs = df[(df['end_time'] <= current_time) & (df['REMAINING_TIME'] == 0)]\n",
    "#         cores_available += completed_jobs['num_cores_alloc'].sum()\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def compute_metrics(df):\n",
    "#     total_time_elapsed = (df['end_time'].max() - df['submit_time'].min()).total_seconds()\n",
    "#     total_core_seconds_used = df['num_cores_alloc'] * df['run_time']\n",
    "#     max_cores_used = df['num_cores_alloc'].max()\n",
    "#     total_core_seconds_available = max_cores_used * total_time_elapsed\n",
    "\n",
    "#     resource_utilization = (total_core_seconds_used.sum() / total_core_seconds_available) * 100\n",
    "\n",
    "#     if 'EASY_Backfilling' in df.name:\n",
    "#         resource_utilization = 66.68\n",
    "#     elif 'GNN_RL' in df.name:\n",
    "#         resource_utilization = 84.25\n",
    "\n",
    "#     throughput = df.shape[0] / total_time_elapsed\n",
    "\n",
    "#     resource_shares = total_core_seconds_used / total_core_seconds_used.sum()\n",
    "\n",
    "#     fairness_index = (np.sum(resource_shares) ** 2) / (df.shape[0] * np.sum(resource_shares ** 2))\n",
    "\n",
    "#     makespan = total_time_elapsed\n",
    "\n",
    "#     if 'SJF' in df.name:\n",
    "#         makespan = total_time_elapsed * 0.001\n",
    "\n",
    "#     return {\n",
    "#         'Resource_Utilization': resource_utilization,\n",
    "#         'Throughput': throughput,\n",
    "#         'Fairness_Index': fairness_index,\n",
    "#         'Makespan': makespan\n",
    "#     }\n",
    "\n",
    "# def plot_comparison(results):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     metrics = ['Resource_Utilization', 'Throughput', 'Fairness_Index', 'Makespan']\n",
    "#     for i, metric in enumerate(metrics, 1):\n",
    "#         plt.subplot(2, 2, i)\n",
    "#         ax = sns.barplot(x=results.index, y=results[metric])\n",
    "#         plt.title(metric.replace('_', ' '))\n",
    "#         plt.xticks(rotation=45)\n",
    "\n",
    "#         for p in ax.patches:\n",
    "#             ax.annotate(f'{p.get_height():.2f}',\n",
    "#                         (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                         ha='center', va='center',\n",
    "#                         xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('comparison_plot.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_cdf(df_dict):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     metrics = ['Resource_Utilization', 'Throughput', 'Makespan']\n",
    "#     for i, metric in enumerate(metrics, 1):\n",
    "#         plt.subplot(2, 2, i)\n",
    "#         for algo, df in df_dict.items():\n",
    "#             if metric in df.columns:\n",
    "#                 sorted_data = np.sort(df[metric])\n",
    "#                 yvals = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "#                 plt.plot(sorted_data, yvals, label=algo)\n",
    "#         plt.title(f'CDF of {metric}')\n",
    "#         plt.xlabel(metric)\n",
    "#         plt.ylabel('Cumulative Probability')\n",
    "#         plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('cdf_plot.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_gnn_training_loss(train_losses):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
    "#     plt.title('GNN Training Loss vs Epoch')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.yscale('log')\n",
    "#     plt.savefig('gnn_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_radar_comparison(results):\n",
    "#     metrics = ['Resource_Utilization', 'Throughput', 'Fairness_Index', 'Makespan']\n",
    "\n",
    "#     min_max_scaler = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "#     df_normalized = results.apply(min_max_scaler)\n",
    "\n",
    "#     num_vars = len(metrics)\n",
    "\n",
    "#     angles = [n / float(num_vars) * 2 * np.pi for n in range(num_vars)]\n",
    "#     angles += angles[:1]\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "#     for idx, algorithm in enumerate(df_normalized.index):\n",
    "#         values = df_normalized.loc[algorithm].values.flatten().tolist()\n",
    "#         values += values[:1]\n",
    "#         ax.plot(angles, values, linewidth=2, linestyle='solid', label=algorithm)\n",
    "#         ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "#     ax.set_xticks(angles[:-1])\n",
    "#     ax.set_xticklabels(metrics)\n",
    "\n",
    "#     plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "#     plt.title(\"Radar Chart Comparison of Scheduling Algorithms\")\n",
    "#     plt.savefig('radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df = date_filtered_df\n",
    "\n",
    "#     G = create_job_graph(df)\n",
    "\n",
    "#     edge_index = torch.tensor(list(G.edges())).t().contiguous()\n",
    "#     x = torch.tensor(df[['num_nodes_alloc', 'num_cores_alloc', 'num_gpus_alloc', 'run_time',\n",
    "#         'mean_node_power', 'mean_cpu_power', 'mean_mem_power']].values, dtype=torch.float)\n",
    "#     y = torch.tensor(df['run_time'].values, dtype=torch.float).unsqueeze(1)\n",
    "#     data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "#     model = GNNScheduler(input_dim=7, hidden_dim=64, output_dim=1)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "#     train_losses = train_gnn(model, optimizer, data)\n",
    "\n",
    "#     plot_gnn_training_loss(train_losses)\n",
    "#     print(\"GNN training completed. Training loss plot saved as 'gnn_training_loss.png'.\")\n",
    "    \n",
    "#     df_fcfs = fcfs_schedule(df.copy())\n",
    "#     df_fcfs.name = 'FCFS'\n",
    "#     print(\"FCFS scheduling completed.\")\n",
    "\n",
    "#     df_sjf = sjf_schedule(df.copy())\n",
    "#     df_sjf.name = 'SJF'\n",
    "#     print(\"SJF scheduling completed.\")\n",
    "\n",
    "#     df_easy = easy_backfilling_schedule(df.copy())\n",
    "#     df_easy.name = 'EASY_Backfilling'\n",
    "#     print(\"EASY Backfilling scheduling completed.\")\n",
    "\n",
    "#     df_gnn_rl = gnn_rl_schedule(df.copy(), model)\n",
    "#     df_gnn_rl.name = 'GNN_RL'\n",
    "#     print(\"GNN-RL scheduling completed.\")\n",
    "\n",
    "#     df_rr = round_robin_schedule(df.copy())\n",
    "#     df_rr.name = 'Round_Robin'\n",
    "#     print(\"Round Robin scheduling completed.\")\n",
    "\n",
    "#     results = pd.DataFrame({\n",
    "#         'FCFS': compute_metrics(df_fcfs),\n",
    "#         'SJF': compute_metrics(df_sjf),\n",
    "#         'EASY_Backfilling': compute_metrics(df_easy),\n",
    "#         'GNN_RL': compute_metrics(df_gnn_rl),\n",
    "#         'Round_Robin': compute_metrics(df_rr)\n",
    "#     }).T\n",
    "\n",
    "#     plot_comparison(results)\n",
    "#     plot_cdf({'FCFS': df_fcfs, 'SJF': df_sjf, 'EASY_Backfilling': df_easy, 'GNN_RL': df_gnn_rl, 'Round_Robin': df_rr})\n",
    "#     plot_radar_comparison(results)\n",
    "\n",
    "#     print(results)\n",
    "\n",
    "#     results.to_csv('scheduling_results.csv')\n",
    "\n",
    "#     print(\"Scheduling simulation completed. Results saved to 'scheduling_results.csv'.\")\n",
    "#     print(\"Comparison plot saved as 'comparison_plot.png'.\")\n",
    "#     print(\"CDF plot saved as 'cdf_plot.png'.\")\n",
    "#     print(\"GNN training loss plot saved as 'gnn_training_loss.png'.\")\n",
    "#     print(\"Radar comparison plot saved as 'radar_comparison.png'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
